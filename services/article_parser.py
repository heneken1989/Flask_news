"""
Parser ƒë·ªÉ extract article data t·ª´ HTML c·ªßa sermitsiaq.ag
"""
from bs4 import BeautifulSoup
from datetime import datetime
import re
from urllib.parse import urljoin, urlparse


def parse_title_with_highlights(title_elem):
    """
    Parse title element ƒë·ªÉ extract text v√† c√°c ph·∫ßn ƒë∆∞·ª£c highlight v·ªõi m√†u s·∫Øc
    
    Args:
        title_elem: BeautifulSoup element c·ªßa h2.headline
    
    Returns:
        dict: {
            'full_text': str,  # Full title text (plain text)
            'parts': [  # List of title parts v·ªõi highlight info
                {'text': str, 'color_class': str or None},
                ...
            ]
        }
    """
    if not title_elem:
        return None
    
    try:
        # L·∫•y full text (plain text, kh√¥ng c√≥ HTML)
        full_text = title_elem.get_text(strip=False)  # Gi·ªØ newlines
        
        # Parse c√°c parts v·ªõi highlight
        parts = []
        
        # Duy·ªát qua t·∫•t c·∫£ children c·ªßa title_elem
        for child in title_elem.children:
            if hasattr(child, 'name'):
                # N·∫øu l√† tag (span, etc.)
                if child.name == 'span':
                    # Extract color classes t·ª´ span
                    span_classes = child.get('class', [])
                    color_class = ' '.join(span_classes) if span_classes else None
                    text = child.get_text(strip=False)  # Gi·ªØ newlines
                    parts.append({
                        'text': text,
                        'color_class': color_class
                    })
                else:
                    # Tag kh√°c, l·∫•y text
                    text = child.get_text(strip=False)
                    if text.strip():
                        parts.append({
                            'text': text,
                            'color_class': None
                        })
            else:
                # Text node
                text = str(child).strip()
                if text:
                    parts.append({
                        'text': text,
                        'color_class': None
                    })
        
        # N·∫øu kh√¥ng c√≥ parts (kh√¥ng c√≥ span), t·∫°o 1 part v·ªõi full text
        if not parts:
            parts.append({
                'text': full_text,
                'color_class': None
            })
        
        return {
            'full_text': full_text.strip(),
            'parts': parts
        }
    except Exception as e:
        print(f"‚ö†Ô∏è  Error parsing title with highlights: {e}")
        # Fallback: return plain text
        return {
            'full_text': title_elem.get_text(strip=True),
            'parts': [{'text': title_elem.get_text(strip=True), 'color_class': None}]
        }


def extract_grid_size_from_classes(article_classes):
    """
    Extract grid size t·ª´ article classes (large-5, large-6, large-7, etc.)
    
    Args:
        article_classes: List of class strings ho·∫∑c string
    
    Returns:
        int: Grid size (5, 6, 7, 8, 4, 12, etc.) ho·∫∑c None
    """
    try:
        if isinstance(article_classes, str):
            class_str = article_classes
        else:
            class_str = ' '.join(article_classes) if article_classes else ''
        
        # T√¨m pattern large-X trong class string
        import re
        match = re.search(r'large-(\d+)', class_str)
        if match:
            return int(match.group(1))
        return None
    except:
        return None


def parse_article_element(article_element, base_url='https://www.sermitsiaq.ag'):
    """
    Parse m·ªôt article element t·ª´ HTML ƒë·ªÉ extract data
    
    Args:
        article_element: BeautifulSoup element c·ªßa article
        base_url: Base URL ƒë·ªÉ resolve relative URLs
    
    Returns:
        dict: Article data ho·∫∑c None n·∫øu kh√¥ng parse ƒë∆∞·ª£c
    """
    try:
        # L·∫•y element_guid t·ª´ data-element-guid attribute
        element_guid = article_element.get('data-element-guid', '')
        if not element_guid:
            return None
        
        # L·∫•y instance v√† site_alias
        instance = article_element.get('data-instance', '')
        site_alias = article_element.get('data-site-alias', 'sermitsiaq')
        section = article_element.get('data-section', '')
        
        # T√¨m link ch√≠nh c·ªßa article
        link_elem = article_element.find('a', itemprop='url')
        if not link_elem:
            return None
        
        # URL v√† k5a_url
        url = link_elem.get('href', '')
        k5a_url = link_elem.get('data-k5a-url', url)
        
        # Resolve relative URLs
        if url and not url.startswith('http'):
            url = urljoin(base_url, url)
        if k5a_url and not k5a_url.startswith('http'):
            k5a_url = urljoin(base_url, k5a_url)
        
        # Title v·ªõi highlights
        title_elem = article_element.find('h2', class_='headline')
        title_data = parse_title_with_highlights(title_elem) if title_elem else None
        if not title_data or not title_data['full_text']:
            return None
        
        title = title_data['full_text']  # Plain text cho backward compatibility
        title_parts = title_data['parts']  # Parts v·ªõi highlight info
        
        # Published date
        time_elem = article_element.find('time', itemprop='datePublished')
        published_date = None
        if time_elem:
            datetime_str = time_elem.get('datetime', '')
            if datetime_str:
                try:
                    # Parse ISO format: 2026-01-15T20:29:57+01:00
                    published_date = datetime.fromisoformat(datetime_str.replace('Z', '+00:00'))
                except:
                    pass
        
        # Paywall check
        paywall_elem = article_element.find('div', class_='paywallLabel')
        is_paywall = paywall_elem is not None
        paywall_class = 'paywall' if is_paywall else ''
        
        # Extract kicker floating (text m√†u xanh tr√™n h√¨nh ·∫£nh)
        kicker_floating = None
        floating_text_elem = article_element.find('div', class_='floatingText')
        if floating_text_elem:
            kicker_elem = floating_text_elem.find('div', class_=lambda x: x and 'kicker' in x and 'floating' in x)
            if kicker_elem:
                kicker_floating = kicker_elem.get_text(strip=True)
        
        # Extract kicker below (text n·∫±m gi·ªØa media v√† headline, v√≠ d·ª• "OPDATERET")
        kicker_below = None
        kicker_below_classes = None
        # T√¨m div v·ªõi class "kicker below" trong article element
        kicker_below_elem = article_element.find('div', class_=lambda x: x and 'kicker' in x and 'below' in x)
        if kicker_below_elem:
            kicker_below = kicker_below_elem.get_text(strip=True)
            # L·∫•y classes c·ªßa kicker below ƒë·ªÉ gi·ªØ nguy√™n styling
            kicker_below_classes = ' '.join(kicker_below_elem.get('class', []))
        
        # Image data
        image_data = parse_article_image(article_element, base_url)
        
        # Extract slug t·ª´ URL
        slug = extract_slug_from_url(url)
        
        # Extract article ID t·ª´ URL (n·∫øu c√≥)
        article_id = extract_article_id_from_url(url)
        
        # Extract grid size t·ª´ classes
        article_classes = article_element.get('class', [])
        grid_size = extract_grid_size_from_classes(article_classes)
        
        return {
            'element_guid': element_guid,
            'title': title,
            'title_parts': title_parts,  # Parts v·ªõi highlight info
            'slug': slug,
            'url': url,
            'k5a_url': k5a_url,
            'section': section,
            'site_alias': site_alias,
            'instance': instance,
            'published_date': published_date,
            'is_paywall': is_paywall,
            'paywall_class': paywall_class,
            'kicker_floating': kicker_floating,
            'kicker_below': kicker_below,  # Kicker below (v√≠ d·ª• "OPDATERET")
            'kicker_below_classes': kicker_below_classes,  # Classes c·ªßa kicker below
            'image_data': image_data,
            'article_id': article_id,
            'grid_size': grid_size,  # L∆∞u grid size t·ª´ HTML
        }
    except Exception as e:
        print(f"‚ö†Ô∏è  Error parsing article element: {e}")
        return None


def parse_article_image(article_element, base_url='https://www.sermitsiaq.ag'):
    """
    Parse image data t·ª´ article element
    
    Args:
        article_element: BeautifulSoup element c·ªßa article
        base_url: Base URL ƒë·ªÉ resolve relative URLs
    
    Returns:
        dict: Image data
    """
    image_data = {}
    
    try:
        # T√¨m figure element
        figure = article_element.find('figure')
        if not figure:
            return image_data
        
        # L·∫•y element_guid c·ªßa image
        image_element_guid = figure.get('data-element-guid', '')
        if image_element_guid:
            image_data['element_guid'] = image_element_guid
        
        # T√¨m picture element
        picture = figure.find('picture')
        if not picture:
            # Fallback: t√¨m img tr·ª±c ti·∫øp
            img = figure.find('img')
            if img:
                src = img.get('src', '')
                if src and not src.startswith('http'):
                    src = urljoin(base_url, src)
                image_data['fallback'] = src
                image_data['desktop_webp'] = src
                image_data['desktop_jpeg'] = src
                image_data['mobile_webp'] = src
                image_data['mobile_jpeg'] = src
                image_data['alt'] = img.get('alt', '')
                image_data['title'] = img.get('title', '')
                image_data['desktop_width'] = img.get('width', '524')
                image_data['desktop_height'] = img.get('height', '341')
                image_data['mobile_width'] = img.get('width', '480')
                image_data['mobile_height'] = img.get('height', '312')
            return image_data
        
        # Parse source elements
        sources = picture.find_all('source')
        for source in sources:
            srcset = source.get('srcset', '')
            media = source.get('media', '')
            img_type = source.get('type', '')
            
            if not srcset:
                continue
            
            # Resolve relative URLs
            if not srcset.startswith('http'):
                srcset = urljoin(base_url, srcset)
            
            # Determine image type
            if 'webp' in img_type:
                if 'min-width: 768px' in media or 'min-width: 768' in media:
                    image_data['desktop_webp'] = srcset
                elif 'max-width: 767px' in media or 'max-width: 767' in media:
                    image_data['mobile_webp'] = srcset
            elif 'jpeg' in img_type or 'jpg' in img_type:
                if 'min-width: 768px' in media or 'min-width: 768' in media:
                    image_data['desktop_jpeg'] = srcset
                elif 'max-width: 767px' in media or 'max-width: 767' in media:
                    image_data['mobile_jpeg'] = srcset
        
        # Fallback img
        img = picture.find('img')
        if img:
            src = img.get('src', '')
            if src and not src.startswith('http'):
                src = urljoin(base_url, src)
            image_data['fallback'] = src
            image_data['alt'] = img.get('alt', '')
            image_data['title'] = img.get('title', '')
            image_data['desktop_width'] = img.get('width', '524')
            image_data['desktop_height'] = img.get('height', '341')
            image_data['mobile_width'] = img.get('width', '480')
            image_data['mobile_height'] = img.get('height', '312')
        
    except Exception as e:
        print(f"‚ö†Ô∏è  Error parsing image: {e}")
    
    return image_data


def extract_slug_from_url(url):
    """
    Extract slug t·ª´ URL
    V√≠ d·ª•: https://www.sermitsiaq.ag/erhverv/article-slug/2329217 -> article-slug
    """
    if not url:
        return ''
    
    try:
        # Parse URL
        parsed = urlparse(url)
        path = parsed.path.strip('/')
        
        # Split path
        parts = path.split('/')
        if len(parts) >= 2:
            # L·∫•y ph·∫ßn tr∆∞·ªõc s·ªë ID
            slug = parts[-2] if parts[-1].isdigit() else parts[-1]
            return slug
        return ''
    except:
        return ''


def extract_article_id_from_url(url):
    """
    Extract article ID t·ª´ URL
    V√≠ d·ª•: https://www.sermitsiaq.ag/erhverv/article-slug/2329217 -> 2329217
    """
    if not url:
        return None
    
    try:
        parsed = urlparse(url)
        path = parsed.path.strip('/')
        parts = path.split('/')
        
        # L·∫•y s·ªë cu·ªëi c√πng trong path
        for part in reversed(parts):
            if part.isdigit():
                return int(part)
        return None
    except:
        return None


def detect_layout_type_from_element(article_elem, row_elem=None):
    """
    Detect layout_type t·ª´ article element v√† row structure
    
    Args:
        article_elem: BeautifulSoup element c·ªßa article
        row_elem: BeautifulSoup element c·ªßa row ch·ª©a article (optional)
    
    Returns:
        str: Layout type ('1_full', '2_articles', '3_articles', '1_special_bg', etc.)
    """
    try:
        # Check article classes ƒë·ªÉ x√°c ƒë·ªãnh grid size
        article_classes = article_elem.get('class', [])
        article_class_str = ' '.join(article_classes) if article_classes else ''
        
        # Check special background (nh∆∞ng kh√¥ng return ngay, v√¨ c√≥ th·ªÉ l√† 2_articles v·ªõi bg-black)
        content_div = article_elem.find('div', class_='content')
        has_special_bg = False
        if content_div:
            content_classes = content_div.get('class', [])
            content_class_str = ' '.join(content_classes) if content_classes else ''
            if 'bg-black' in content_class_str:
                has_special_bg = True
        
        # N·∫øu c√≥ row_elem, check s·ªë l∆∞·ª£ng articles trong row ƒë·ªÉ x√°c ƒë·ªãnh layout
        if row_elem:
            # L·ªçc ch·ªâ l·∫•y elements c√≥ name (b·ªè qua text nodes, comments, etc.)
            row_children = [child for child in row_elem.children 
                           if hasattr(child, 'name') and child.name is not None]
            
            # ƒê·∫øm s·ªë articles trong row
            articles_in_row = [child for child in row_children if child.name == 'article']
            
            # Check xem c√≥ list b√™n c·∫°nh kh√¥ng
            list_elem = row_elem.find('div', class_='articlesByTag')
            if not list_elem:
                list_elem = row_elem.find('div', class_='toplist')
            
            if list_elem:
                # C√≥ list trong row - check v·ªã tr√≠
                article_index = None
                list_index = None
                
                for idx, child in enumerate(row_children):
                    if child.name == 'article':
                        if child == article_elem or child.get('data-element-guid') == article_elem.get('data-element-guid'):
                            article_index = idx
                    elif child.name == 'div':
                        child_classes = child.get('class', [])
                        if 'articlesByTag' in child_classes or 'toplist' in child_classes:
                            list_index = idx
                
                if article_index is not None and list_index is not None:
                    if list_index < article_index:
                        return '1_with_list_left'
                    else:
                        return '1_with_list_right'
            
            # N·∫øu c√≥ 2 articles trong row, ƒë√≥ l√† 2_articles (b·∫•t k·ªÉ grid size)
            if len(articles_in_row) == 2:
                return '2_articles'
            # N·∫øu c√≥ 3 articles trong row, ƒë√≥ l√† 3_articles
            elif len(articles_in_row) == 3:
                return '3_articles'
        
        # Check grid size t·ª´ classes (fallback n·∫øu kh√¥ng c√≥ row_elem)
        if 'large-12' in article_class_str:
            # Full width - c√≥ th·ªÉ l√† 1_full
            return '1_full'
        elif 'large-6' in article_class_str or 'large-5' in article_class_str or 'large-7' in article_class_str or 'large-8' in article_class_str:
            # 2 per row (v·ªõi c√°c t·ª∑ l·ªá kh√°c nhau: 6+6, 5+7, 8+4, etc.)
            return '2_articles'
        elif 'large-4' in article_class_str:
            # 3 per row
            return '3_articles'
        
        # N·∫øu c√≥ special bg v√† kh√¥ng ph·∫£i 2_articles, return 1_special_bg
        if has_special_bg:
            return '1_special_bg'
        
        # Default
        return '1_full'
    except:
        return '1_full'  # Default fallback


def parse_articles_from_html(html_content, base_url='https://www.sermitsiaq.ag', is_home=False):
    """
    Parse t·∫•t c·∫£ articles t·ª´ HTML content
    
    Args:
        html_content: HTML content string
        base_url: Base URL ƒë·ªÉ resolve relative URLs
        is_home: N·∫øu True, s·∫Ω detect layout_type t·ª´ HTML structure
    
    Returns:
        list: List of article dictionaries
    """
    articles = []
    
    try:
        soup = BeautifulSoup(html_content, 'lxml')
        
        # T√¨m t·∫•t c·∫£ article elements
        # Selector: article[data-element-guid] trong .page-content
        page_content = soup.find('div', class_='page-content')
        if not page_content:
            # Fallback: t√¨m t·∫•t c·∫£ articles
            article_elements = soup.find_all('article', attrs={'data-element-guid': True})
            rows = []
        else:
            article_elements = page_content.find_all('article', attrs={'data-element-guid': True})
            # T√¨m rows n·∫øu l√† home page
            if is_home:
                rows = page_content.find_all('div', class_='row')
            else:
                rows = []
        
        print(f"üì∞ Found {len(article_elements)} article elements")
        
        for article_elem in article_elements:
            article_data = parse_article_element(article_elem, base_url)
            if article_data:
                # N·∫øu l√† home page, detect layout_type
                if is_home:
                    # T√¨m row ch·ª©a article n√†y
                    row_elem = None
                    for row in rows:
                        if article_elem in row.find_all('article'):
                            row_elem = row
                            break
                    
                    layout_type = detect_layout_type_from_element(article_elem, row_elem)
                    article_data['layout_type'] = layout_type
                    
                    # Detect layout_data n·∫øu c√≥
                    layout_data = {}
                    
                    # Th√™m kicker_floating v√†o layout_data n·∫øu c√≥ (cho t·∫•t c·∫£ layout types)
                    if article_data.get('kicker_floating'):
                        layout_data['kicker_floating'] = article_data['kicker_floating']
                    
                    # Th√™m kicker_below v√†o layout_data n·∫øu c√≥ (cho t·∫•t c·∫£ layout types)
                    if article_data.get('kicker_below'):
                        layout_data['kicker_below'] = article_data['kicker_below']
                        layout_data['kicker_below_classes'] = article_data.get('kicker_below_classes', 'kicker below primary color_mobile_primary')
                    
                    # Th√™m title_parts v√†o layout_data n·∫øu c√≥ highlights (cho t·∫•t c·∫£ layout types)
                    if article_data.get('title_parts'):
                        layout_data['title_parts'] = article_data['title_parts']
                    
                    # Check v√† l∆∞u has_bg_black n·∫øu article c√≥ bg-black (cho t·∫•t c·∫£ layout types)
                    content_div = article_elem.find('div', class_='content')
                    if content_div:
                        content_classes = content_div.get('class', [])
                        content_class_str = ' '.join(content_classes) if content_classes else ''
                        if 'bg-black' in content_class_str:
                            layout_data['has_bg_black'] = True
                            # L∆∞u t·∫•t c·∫£ classes c·ªßa content div ƒë·ªÉ gi·ªØ nguy√™n styling
                            layout_data['content_classes'] = content_class_str
                    
                    if layout_type == '1_special_bg':
                        # Check kicker
                        kicker_elem = article_elem.find('div', class_='kicker')
                        if kicker_elem:
                            layout_data['kicker'] = kicker_elem.get_text(strip=True)
                    elif layout_type in ['1_with_list_left', '1_with_list_right']:
                        # Parse list items t·ª´ row
                        if row_elem:
                            # T√¨m list element (c√≥ th·ªÉ l√† articlesByTag ho·∫∑c toplist)
                            list_elem = row_elem.find('div', class_='articlesByTag')
                            if not list_elem:
                                list_elem = row_elem.find('div', class_='toplist')
                            
                            if list_elem:
                                # Extract list title - c√≥ th·ªÉ l√† h3 v·ªõi class headline ho·∫∑c kh√¥ng
                                list_title_elem = list_elem.find('h3')
                                if list_title_elem:
                                    layout_data['list_title'] = list_title_elem.get_text(strip=True)
                                
                                # Extract list items
                                list_items = []
                                # T√¨m trong ul.toplist-results ho·∫∑c ul th√¥ng th∆∞·ªùng
                                ul_elem = list_elem.find('ul', class_='toplist-results')
                                if not ul_elem:
                                    ul_elem = list_elem.find('ul')
                                
                                if ul_elem:
                                    for li in ul_elem.find_all('li'):
                                        link = li.find('a')
                                        if link:
                                            # T√¨m title - c√≥ th·ªÉ l√† h4 v·ªõi class abt-title ho·∫∑c h4 th√¥ng th∆∞·ªùng
                                            # Title c√≥ th·ªÉ n·∫±m trong link ho·∫∑c trong li
                                            title_elem = link.find('h4', class_='abt-title')
                                            if not title_elem:
                                                title_elem = link.find('h4')
                                            if not title_elem:
                                                # Fallback: t√¨m trong li
                                                title_elem = li.find('h4', class_='abt-title')
                                            if not title_elem:
                                                title_elem = li.find('h4')
                                            
                                            if title_elem:
                                                title = title_elem.get_text(strip=True)
                                                url = link.get('href', '')
                                                if title and url:
                                                    list_items.append({
                                                        'title': title,
                                                        'url': url
                                                    })
                                
                                if list_items:
                                    layout_data['list_items'] = list_items
                    
                    if layout_data:
                        article_data['layout_data'] = layout_data
                
                articles.append(article_data)
        
        print(f"‚úÖ Successfully parsed {len(articles)} articles")
        
    except Exception as e:
        print(f"‚ùå Error parsing HTML: {e}")
        import traceback
        traceback.print_exc()
    
    return articles

