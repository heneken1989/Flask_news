"""
Parser ƒë·ªÉ extract article data t·ª´ HTML c·ªßa sermitsiaq.ag
"""
from bs4 import BeautifulSoup
from datetime import datetime
import re
from urllib.parse import urljoin, urlparse


def parse_title_with_highlights(title_elem):
    """
    Parse title element ƒë·ªÉ extract text v√† c√°c ph·∫ßn ƒë∆∞·ª£c highlight v·ªõi m√†u s·∫Øc
    
    Args:
        title_elem: BeautifulSoup element c·ªßa h2.headline
    
    Returns:
        dict: {
            'full_text': str,  # Full title text (plain text)
            'parts': [  # List of title parts v·ªõi highlight info
                {'text': str, 'color_class': str or None},
                ...
            ]
        }
    """
    if not title_elem:
        return None
    
    try:
        # L·∫•y full text (plain text, kh√¥ng c√≥ HTML)
        full_text = title_elem.get_text(strip=False)  # Gi·ªØ newlines
        
        # Parse c√°c parts v·ªõi highlight
        parts = []
        
        # Duy·ªát qua t·∫•t c·∫£ children c·ªßa title_elem
        for child in title_elem.children:
            if hasattr(child, 'name'):
                # N·∫øu l√† tag (span, etc.)
                if child.name == 'span':
                    # Extract color classes t·ª´ span
                    span_classes = child.get('class', [])
                    color_class = ' '.join(span_classes) if span_classes else None
                    text = child.get_text(strip=False)  # Gi·ªØ newlines
                    parts.append({
                        'text': text,
                        'color_class': color_class
                    })
                else:
                    # Tag kh√°c, l·∫•y text
                    text = child.get_text(strip=False)
                    if text.strip():
                        parts.append({
                            'text': text,
                            'color_class': None
                        })
            else:
                # Text node
                text = str(child).strip()
                if text:
                    parts.append({
                        'text': text,
                        'color_class': None
                    })
        
        # N·∫øu kh√¥ng c√≥ parts (kh√¥ng c√≥ span), t·∫°o 1 part v·ªõi full text
        if not parts:
            parts.append({
                'text': full_text,
                'color_class': None
            })
        
        return {
            'full_text': full_text.strip(),
            'parts': parts
        }
    except Exception as e:
        print(f"‚ö†Ô∏è  Error parsing title with highlights: {e}")
        # Fallback: return plain text
        return {
            'full_text': title_elem.get_text(strip=True),
            'parts': [{'text': title_elem.get_text(strip=True), 'color_class': None}]
        }


def extract_grid_size_from_classes(article_classes):
    """
    Extract grid size t·ª´ article classes (large-5, large-6, large-7, etc.)
    
    Args:
        article_classes: List of class strings ho·∫∑c string
    
    Returns:
        int: Grid size (5, 6, 7, 8, 4, 12, etc.) ho·∫∑c None
    """
    try:
        if isinstance(article_classes, str):
            class_str = article_classes
        else:
            class_str = ' '.join(article_classes) if article_classes else ''
        
        # T√¨m pattern large-X trong class string
        import re
        match = re.search(r'large-(\d+)', class_str)
        if match:
            return int(match.group(1))
        return None
    except:
        return None


def parse_article_element(article_element, base_url='https://www.sermitsiaq.ag'):
    """
    Parse m·ªôt article element t·ª´ HTML ƒë·ªÉ extract data
    
    Args:
        article_element: BeautifulSoup element c·ªßa article
        base_url: Base URL ƒë·ªÉ resolve relative URLs
    
    Returns:
        dict: Article data ho·∫∑c None n·∫øu kh√¥ng parse ƒë∆∞·ª£c
    """
    try:
        # L·∫•y element_guid t·ª´ data-element-guid attribute
        element_guid = article_element.get('data-element-guid', '')
        if not element_guid:
            return None
        
        # L·∫•y instance v√† site_alias
        instance = article_element.get('data-instance', '')
        site_alias = article_element.get('data-site-alias', 'sermitsiaq')
        section = article_element.get('data-section', '')
        
        # T√¨m link ch√≠nh c·ªßa article
        link_elem = article_element.find('a', itemprop='url')
        if not link_elem:
            return None
        
        # URL v√† k5a_url
        url = link_elem.get('href', '')
        k5a_url = link_elem.get('data-k5a-url', url)
        
        # Resolve relative URLs
        if url and not url.startswith('http'):
            url = urljoin(base_url, url)
        if k5a_url and not k5a_url.startswith('http'):
            k5a_url = urljoin(base_url, k5a_url)
        
        # Title v·ªõi highlights
        title_elem = article_element.find('h2', class_='headline')
        title_data = parse_title_with_highlights(title_elem) if title_elem else None
        if not title_data or not title_data['full_text']:
            return None
        
        title = title_data['full_text']  # Plain text cho backward compatibility
        title_parts = title_data['parts']  # Parts v·ªõi highlight info
        
        # Published date
        time_elem = article_element.find('time', itemprop='datePublished')
        published_date = None
        if time_elem:
            datetime_str = time_elem.get('datetime', '')
            if datetime_str:
                try:
                    # Parse ISO format: 2026-01-15T20:29:57+01:00
                    published_date = datetime.fromisoformat(datetime_str.replace('Z', '+00:00'))
                except:
                    pass
        
        # Paywall check
        paywall_elem = article_element.find('div', class_='paywallLabel')
        is_paywall = paywall_elem is not None
        paywall_class = 'paywall' if is_paywall else ''
        
        # Extract kicker floating (text m√†u xanh tr√™n h√¨nh ·∫£nh)
        kicker_floating = None
        floating_text_elem = article_element.find('div', class_='floatingText')
        if floating_text_elem:
            kicker_elem = floating_text_elem.find('div', class_=lambda x: x and 'kicker' in x and 'floating' in x)
            if kicker_elem:
                kicker_floating = kicker_elem.get_text(strip=True)
        
        # Extract kicker below (text n·∫±m gi·ªØa media v√† headline, v√≠ d·ª• "OPDATERET")
        kicker_below = None
        kicker_below_classes = None
        # T√¨m div v·ªõi class "kicker below" trong article element
        kicker_below_elem = article_element.find('div', class_=lambda x: x and 'kicker' in x and 'below' in x)
        if kicker_below_elem:
            kicker_below = kicker_below_elem.get_text(strip=True)
            # L·∫•y classes c·ªßa kicker below ƒë·ªÉ gi·ªØ nguy√™n styling
            kicker_below_classes = ' '.join(kicker_below_elem.get('class', []))
        
        # Image data
        image_data = parse_article_image(article_element, base_url)
        
        # Extract slug t·ª´ URL
        slug = extract_slug_from_url(url)
        
        # Extract article ID t·ª´ URL (n·∫øu c√≥)
        article_id = extract_article_id_from_url(url)
        
        # Extract grid size t·ª´ classes
        article_classes = article_element.get('class', [])
        grid_size = extract_grid_size_from_classes(article_classes)
        
        return {
            'element_guid': element_guid,
            'title': title,
            'title_parts': title_parts,  # Parts v·ªõi highlight info
            'slug': slug,
            'url': url,
            'k5a_url': k5a_url,
            'section': section,
            'site_alias': site_alias,
            'instance': instance,
            'published_date': published_date,
            'is_paywall': is_paywall,
            'paywall_class': paywall_class,
            'kicker_floating': kicker_floating,
            'kicker_below': kicker_below,  # Kicker below (v√≠ d·ª• "OPDATERET")
            'kicker_below_classes': kicker_below_classes,  # Classes c·ªßa kicker below
            'image_data': image_data,
            'article_id': article_id,
            'grid_size': grid_size,  # L∆∞u grid size t·ª´ HTML
        }
    except Exception as e:
        print(f"‚ö†Ô∏è  Error parsing article element: {e}")
        return None


def parse_article_image(article_element, base_url='https://www.sermitsiaq.ag'):
    """
    Parse image data t·ª´ article element
    
    Args:
        article_element: BeautifulSoup element c·ªßa article
        base_url: Base URL ƒë·ªÉ resolve relative URLs
    
    Returns:
        dict: Image data
    """
    image_data = {}
    
    try:
        # T√¨m figure element
        figure = article_element.find('figure')
        if not figure:
            return image_data
        
        # L·∫•y element_guid c·ªßa image
        image_element_guid = figure.get('data-element-guid', '')
        if image_element_guid:
            image_data['element_guid'] = image_element_guid
        
        # T√¨m picture element
        picture = figure.find('picture')
        if not picture:
            # Fallback: t√¨m img tr·ª±c ti·∫øp
            img = figure.find('img')
            if img:
                src = img.get('src', '')
                if src and not src.startswith('http'):
                    src = urljoin(base_url, src)
                image_data['fallback'] = src
                image_data['desktop_webp'] = src
                image_data['desktop_jpeg'] = src
                image_data['mobile_webp'] = src
                image_data['mobile_jpeg'] = src
                image_data['alt'] = img.get('alt', '')
                image_data['title'] = img.get('title', '')
                image_data['desktop_width'] = img.get('width', '524')
                image_data['desktop_height'] = img.get('height', '341')
                image_data['mobile_width'] = img.get('width', '480')
                image_data['mobile_height'] = img.get('height', '312')
            return image_data
        
        # Parse source elements
        sources = picture.find_all('source')
        for source in sources:
            srcset = source.get('srcset', '')
            media = source.get('media', '')
            img_type = source.get('type', '')
            
            if not srcset:
                continue
            
            # Resolve relative URLs
            if not srcset.startswith('http'):
                srcset = urljoin(base_url, srcset)
            
            # Determine image type
            if 'webp' in img_type:
                if 'min-width: 768px' in media or 'min-width: 768' in media:
                    image_data['desktop_webp'] = srcset
                elif 'max-width: 767px' in media or 'max-width: 767' in media:
                    image_data['mobile_webp'] = srcset
            elif 'jpeg' in img_type or 'jpg' in img_type:
                if 'min-width: 768px' in media or 'min-width: 768' in media:
                    image_data['desktop_jpeg'] = srcset
                elif 'max-width: 767px' in media or 'max-width: 767' in media:
                    image_data['mobile_jpeg'] = srcset
        
        # Fallback img
        img = picture.find('img')
        if img:
            src = img.get('src', '')
            if src and not src.startswith('http'):
                src = urljoin(base_url, src)
            image_data['fallback'] = src
            image_data['alt'] = img.get('alt', '')
            image_data['title'] = img.get('title', '')
            image_data['desktop_width'] = img.get('width', '524')
            image_data['desktop_height'] = img.get('height', '341')
            image_data['mobile_width'] = img.get('width', '480')
            image_data['mobile_height'] = img.get('height', '312')
        
    except Exception as e:
        print(f"‚ö†Ô∏è  Error parsing image: {e}")
    
    return image_data


def extract_slug_from_url(url):
    """
    Extract slug t·ª´ URL
    V√≠ d·ª•: https://www.sermitsiaq.ag/erhverv/article-slug/2329217 -> article-slug
    """
    if not url:
        return ''
    
    try:
        # Parse URL
        parsed = urlparse(url)
        path = parsed.path.strip('/')
        
        # Split path
        parts = path.split('/')
        if len(parts) >= 2:
            # L·∫•y ph·∫ßn tr∆∞·ªõc s·ªë ID
            slug = parts[-2] if parts[-1].isdigit() else parts[-1]
            return slug
        return ''
    except:
        return ''


def extract_article_id_from_url(url):
    """
    Extract article ID t·ª´ URL
    V√≠ d·ª•: https://www.sermitsiaq.ag/erhverv/article-slug/2329217 -> 2329217
    """
    if not url:
        return None
    
    try:
        parsed = urlparse(url)
        path = parsed.path.strip('/')
        parts = path.split('/')
        
        # L·∫•y s·ªë cu·ªëi c√πng trong path
        for part in reversed(parts):
            if part.isdigit():
                return int(part)
        return None
    except:
        return None


def detect_layout_type_from_element(article_elem, row_elem=None):
    """
    Detect layout_type t·ª´ article element v√† row structure
    
    Args:
        article_elem: BeautifulSoup element c·ªßa article
        row_elem: BeautifulSoup element c·ªßa row ch·ª©a article (optional)
    
    Returns:
        str: Layout type ('1_full', '2_articles', '3_articles', '1_special_bg', etc.)
    """
    try:
        # Check article classes ƒë·ªÉ x√°c ƒë·ªãnh grid size
        article_classes = article_elem.get('class', [])
        article_class_str = ' '.join(article_classes) if article_classes else ''
        
        # Check special background (nh∆∞ng kh√¥ng return ngay, v√¨ c√≥ th·ªÉ l√† 2_articles v·ªõi bg-black)
        content_div = article_elem.find('div', class_='content')
        has_special_bg = False
        if content_div:
            content_classes = content_div.get('class', [])
            content_class_str = ' '.join(content_classes) if content_classes else ''
            if 'bg-black' in content_class_str:
                has_special_bg = True
        
        # N·∫øu c√≥ row_elem, check s·ªë l∆∞·ª£ng articles trong row ƒë·ªÉ x√°c ƒë·ªãnh layout
        if row_elem:
            # L·ªçc ch·ªâ l·∫•y elements c√≥ name (b·ªè qua text nodes, comments, etc.)
            row_children = [child for child in row_elem.children 
                           if hasattr(child, 'name') and child.name is not None]
            
            # ƒê·∫øm s·ªë articles trong row
            articles_in_row = [child for child in row_children if child.name == 'article']
            
            # Check xem c√≥ list b√™n c·∫°nh kh√¥ng
            list_elem = row_elem.find('div', class_='articlesByTag')
            if not list_elem:
                list_elem = row_elem.find('div', class_='toplist')
            
            if list_elem:
                # C√≥ list trong row - check v·ªã tr√≠
                article_index = None
                list_index = None
                
                for idx, child in enumerate(row_children):
                    if child.name == 'article':
                        if child == article_elem or child.get('data-element-guid') == article_elem.get('data-element-guid'):
                            article_index = idx
                    elif child.name == 'div':
                        child_classes = child.get('class', [])
                        if 'articlesByTag' in child_classes or 'toplist' in child_classes:
                            list_index = idx
                
                if article_index is not None and list_index is not None:
                    if list_index < article_index:
                        return '1_with_list_left'
                    else:
                        return '1_with_list_right'
            
            # N·∫øu c√≥ 2 articles trong row, ƒë√≥ l√† 2_articles (b·∫•t k·ªÉ grid size)
            if len(articles_in_row) == 2:
                return '2_articles'
            # N·∫øu c√≥ 3 articles trong row, ƒë√≥ l√† 3_articles
            elif len(articles_in_row) == 3:
                return '3_articles'
        
        # Check grid size t·ª´ classes (fallback n·∫øu kh√¥ng c√≥ row_elem)
        if 'large-12' in article_class_str:
            # Full width - c√≥ th·ªÉ l√† 1_full
            return '1_full'
        elif 'large-6' in article_class_str or 'large-5' in article_class_str or 'large-7' in article_class_str or 'large-8' in article_class_str:
            # 2 per row (v·ªõi c√°c t·ª∑ l·ªá kh√°c nhau: 6+6, 5+7, 8+4, etc.)
            return '2_articles'
        elif 'large-4' in article_class_str:
            # 3 per row
            return '3_articles'
        
        # N·∫øu c√≥ special bg v√† kh√¥ng ph·∫£i 2_articles, return 1_special_bg
        if has_special_bg:
            return '1_special_bg'
        
        # Default
        return '1_full'
    except:
        return '1_full'  # Default fallback


def parse_slider_item(slider_item_elem, base_url='https://www.sermitsiaq.ag'):
    """
    Parse m·ªôt article item trong slider
    
    Args:
        slider_item_elem: BeautifulSoup element c·ªßa li.scroll-item
        base_url: Base URL ƒë·ªÉ resolve relative URLs
    
    Returns:
        dict: Article data trong slider
    """
    try:
        link = slider_item_elem.find('a')
        if not link:
            print(f"    ‚ö†Ô∏è  Slider item has no <a> tag")
            return None
        
        url = link.get('href', '')
        if not url:
            print(f"    ‚ö†Ô∏è  Slider item has no href")
            return None
        
        if url and not url.startswith('http'):
            url = urljoin(base_url, url)
        
        # Parse image - figure n·∫±m trong <a>
        image_data = {}
        figure = link.find('figure')  # T√¨m figure trong <a>, kh√¥ng ph·∫£i trong <li>
        if figure:
            img = figure.find('img')
            if img:
                img_src = img.get('src', '')
                if img_src and not img_src.startswith('http'):
                    img_src = urljoin(base_url, img_src)
                image_data['src'] = img_src
                image_data['alt'] = img.get('alt', '')
                image_data['width'] = img.get('width', '265')
                image_data['height'] = img.get('height', '159')
        
        # Parse paywall - c√≥ th·ªÉ n·∫±m trong figure (trong <a>) ho·∫∑c ngo√†i <li>
        is_paywall = False
        paywall_in_figure = figure.find('div', class_='paywallLabel') if figure else None
        paywall_in_link = link.find('div', class_='paywallLabel')  # C√≥ th·ªÉ n·∫±m trong <a> nh∆∞ng ngo√†i figure
        paywall_outside = slider_item_elem.find('div', class_='paywallLabel')  # Ho·∫∑c ngo√†i <a>
        is_paywall = paywall_in_figure is not None or paywall_in_link is not None or paywall_outside is not None
        
        # Parse title v√† kicker - text-container n·∫±m trong <a>
        text_container = link.find('div', class_='text-container')  # T√¨m trong <a>, kh√¥ng ph·∫£i trong <li>
        title = ''
        kicker = ''
        
        if text_container:
            # Title l√† h3
            h3 = text_container.find('h3')
            if h3:
                title = h3.get_text(strip=True)
            
            # Kicker l√† h4 (c√≥ th·ªÉ kh√¥ng c√≥)
            h4 = text_container.find('h4')
            if h4:
                kicker = h4.get_text(strip=True)
        
        # Parse section
        section = slider_item_elem.get('data-section', '')
        
        # Lu√¥n tr·∫£ v·ªÅ k·∫øt qu·∫£, ngay c·∫£ khi thi·∫øu title (c√≥ th·ªÉ c√≥ tr∆∞·ªùng h·ª£p ƒë·∫∑c bi·ªát)
        result = {
            'url': url,
            'title': title,
            'kicker': kicker,
            'image': image_data,
            'is_paywall': is_paywall,
            'section': section
        }
        
        return result
    except Exception as e:
        print(f"‚ö†Ô∏è  Error parsing slider item: {e}")
        import traceback
        traceback.print_exc()
        return None


def parse_slider(slider_elem, base_url='https://www.sermitsiaq.ag'):
    """
    Parse m·ªôt slider (articlescroller) element
    
    Args:
        slider_elem: BeautifulSoup element c·ªßa div.articlescroller
        base_url: Base URL ƒë·ªÉ resolve relative URLs
    
    Returns:
        dict: Slider data v·ªõi layout_type='slider' v√† layout_data ch·ª©a slider info
    """
    try:
        element_guid = slider_elem.get('data-element-guid', '')
        slider_id = slider_elem.get('id', '')
        
        # Parse slider title v√† header link (cho JOB slider)
        title_elem = slider_elem.find('h2', class_='articlescroller-header')
        slider_title = ''
        header_link = None
        if title_elem:
            # Check xem c√≥ link trong header kh√¥ng (JOB slider)
            link_elem = title_elem.find('a')
            if link_elem:
                header_link = {
                    'url': link_elem.get('href', ''),
                    'text': link_elem.get_text(strip=True)
                }
                slider_title = header_link['text']
            else:
                # Title c√≥ th·ªÉ c√≥ span b√™n trong
                span = title_elem.find('span')
                if span:
                    slider_title = span.get_text(strip=True)
                else:
                    slider_title = title_elem.get_text(strip=True)
        
        # Parse c√°c articles trong slider
        # T√¨m ul.scroll-container - c√≥ th·ªÉ n·∫±m trong .inner.content
        inner_content = slider_elem.find('div', class_='inner')
        if not inner_content:
            inner_content = slider_elem
        
        scroll_container = inner_content.find('ul', class_=lambda x: x and 'scroll-container' in x)
        if not scroll_container:
            # Fallback: t√¨m b·∫•t k·ª≥ ul n√†o c√≥ class ch·ª©a 'scroll'
            scroll_container = inner_content.find('ul', class_=lambda x: x and 'scroll' in str(x).lower())
        
        # Check xem c√≥ nav buttons kh√¥ng
        has_nav = inner_content.find('nav') is not None
        
        # Check xem c√≥ ph·∫£i slider NUUK kh√¥ng (source_nuuk class ho·∫∑c count_5)
        slider_classes = slider_elem.get('class', [])
        is_nuuk_slider = 'source_nuuk' in slider_classes
        
        # Check xem c√≥ ph·∫£i JOB slider kh√¥ng (source_job-dk, source_feed_random_kl_jobs, ho·∫∑c source_job)
        is_job_slider = (
            'source_job-dk' in slider_classes or 
            'source_feed_random_kl_jobs' in slider_classes or
            'source_job' in str(slider_classes)
        )
        
        # Parse c√°c class ƒë·∫∑c bi·ªát cho JOB slider
        extra_classes = []
        if 'bg-custom-2' in slider_classes:
            extra_classes.append('bg-custom-2')
        if 'color_mobile_bg-custom-2' in slider_classes:
            extra_classes.append('color_mobile_bg-custom-2')
        if 'hasContentPadding' in slider_classes:
            extra_classes.append('hasContentPadding')
        if 'mobile-hasContentPadding' in slider_classes:
            extra_classes.append('mobile-hasContentPadding')
        if 'layout-align-centered' in slider_classes:
            extra_classes.append('layout-align-centered')
        
        # Parse header classes cho JOB (l∆∞u t·∫•t c·∫£ classes t·ª´ header)
        header_classes = []
        if title_elem:
            title_classes = title_elem.get('class', [])
            # L∆∞u t·∫•t c·∫£ classes t·ª´ header ƒë·ªÉ gi·ªØ nguy√™n styling
            header_classes = title_classes.copy() if title_classes else []
        
        # Check count class t·ª´ scroll-container
        scroll_container_classes = scroll_container.get('class', []) if scroll_container else []
        count_class = None
        for cls in scroll_container_classes:
            if cls.startswith('count_'):
                count_class = cls
                count_num = cls.replace('count_', '')
                if count_num == '5':
                    is_nuuk_slider = True
                break
        
        slider_articles = []
        
        if scroll_container:
            # T√¨m t·∫•t c·∫£ li.scroll-item - c√≥ th·ªÉ c√≥ nhi·ªÅu class, n√™n d√πng lambda
            items = scroll_container.find_all('li', class_=lambda x: x and 'scroll-item' in x)
            print(f"  üé† Found {len(items)} items in slider '{slider_title}'")
            
            for idx, item in enumerate(items):
                article_data = parse_slider_item(item, base_url)
                if article_data:
                    article_data['position'] = idx  # Th·ª© t·ª± trong slider
                    slider_articles.append(article_data)
                else:
                    print(f"    ‚ö†Ô∏è  Failed to parse slider item {idx}")
            
            print(f"  ‚úÖ Successfully parsed {len(slider_articles)}/{len(items)} slider items")
        else:
            print(f"  ‚ö†Ô∏è  No scroll-container found in slider")
            # Debug: in ra c·∫•u tr√∫c HTML ƒë·ªÉ ki·ªÉm tra
            print(f"  üîç Slider HTML structure:")
            print(f"     - Has inner div: {inner_content is not None}")
            if inner_content:
                all_uls = inner_content.find_all('ul')
                print(f"     - Found {len(all_uls)} ul elements")
                for ul_idx, ul in enumerate(all_uls):
                    ul_classes = ul.get('class', [])
                    print(f"       UL {ul_idx}: classes = {ul_classes}")
        
        # X√°c ƒë·ªãnh items_per_view v√† source class
        if is_nuuk_slider:
            items_per_view = 5
            source_class = 'source_nuuk'
        elif is_job_slider:
            items_per_view = 4
            # X√°c ƒë·ªãnh source_class d·ª±a tr√™n class th·ª±c t·∫ø
            if 'source_feed_random_kl_jobs' in slider_classes:
                source_class = 'source_feed_random_kl_jobs'
            elif 'source_job-dk' in slider_classes:
                source_class = 'source_job-dk'
            else:
                source_class = 'source_job'  # Fallback
        else:
            items_per_view = 4
            source_class = 'source_nyheder'
        
        # X√°c ƒë·ªãnh layout_type
        if is_job_slider:
            layout_type = 'job_slider'
        else:
            layout_type = 'slider'
        
        return {
            'element_guid': element_guid,
            'slider_id': slider_id,
            'layout_type': layout_type,
            'title': slider_title or 'Slider',  # Fallback title
            'url': '',  # Slider kh√¥ng c√≥ URL ri√™ng
            'section': 'home',
            'layout_data': {
                'slider_title': slider_title,
                'slider_articles': slider_articles,
                'slider_id': slider_id,
                'has_nav': has_nav,
                'items_per_view': items_per_view,
                'source_class': source_class,
                'header_link': header_link,  # Link trong header (cho JOB)
                'extra_classes': extra_classes,  # C√°c class ƒë·∫∑c bi·ªát (bg-custom-2, etc.)
                'header_classes': header_classes  # C√°c class cho header (underline, t22, etc.)
            },
            'is_home': True
        }
    except Exception as e:
        print(f"‚ö†Ô∏è  Error parsing slider: {e}")
        import traceback
        traceback.print_exc()
        return None


def parse_nuuk_articles(nuuk_elem, base_url='https://www.sermitsiaq.ag', row_index=0):
    """
    Parse NUUK articlescroller th√†nh 5 articles ri√™ng l·∫ª v·ªõi layout_type='5_articles'
    
    Args:
        nuuk_elem: BeautifulSoup element c·ªßa div.articlescroller.source_nuuk
        base_url: Base URL ƒë·ªÉ resolve relative URLs
        row_index: Index c·ªßa row ƒë·ªÉ t√≠nh display_order
    
    Returns:
        list: List of 5 article dictionaries v·ªõi layout_type='5_articles'
    """
    try:
        # T√¨m scroll-container v√† c√°c items
        inner_content = nuuk_elem.find('div', class_='inner')
        if not inner_content:
            inner_content = nuuk_elem
        
        scroll_container = inner_content.find('ul', class_=lambda x: x and 'scroll-container' in x)
        if not scroll_container:
            print(f"  ‚ö†Ô∏è  NUUK: No scroll-container found")
            return []
        
        items = scroll_container.find_all('li', class_=lambda x: x and 'scroll-item' in x)
        print(f"  üèôÔ∏è  NUUK: Found {len(items)} items")
        
        nuuk_articles = []
        for idx, item in enumerate(items):
            # Parse m·ªói item nh∆∞ m·ªôt article
            article_data = parse_slider_item(item, base_url)
            if article_data:
                # Convert 'image' th√†nh 'image_data' ƒë·ªÉ l∆∞u v√†o database
                if 'image' in article_data and article_data['image']:
                    article_data['image_data'] = article_data.pop('image')
                
                # Set layout_type v√† display_order
                article_data['layout_type'] = '5_articles'
                article_data['display_order'] = row_index * 1000 + idx
                article_data['section'] = 'home'
                article_data['is_home'] = True
                nuuk_articles.append(article_data)
        
        print(f"  ‚úÖ NUUK: Successfully parsed {len(nuuk_articles)} articles")
        return nuuk_articles
        
    except Exception as e:
        print(f"  ‚ùå Error parsing NUUK articles: {e}")
        import traceback
        traceback.print_exc()
        return []


def parse_articles_from_html(html_content, base_url='https://www.sermitsiaq.ag', is_home=False):
    """
    Parse t·∫•t c·∫£ articles t·ª´ HTML content
    
    Args:
        html_content: HTML content string
        base_url: Base URL ƒë·ªÉ resolve relative URLs
        is_home: N·∫øu True, s·∫Ω detect layout_type t·ª´ HTML structure v√† parse sliders
    
    Returns:
        list: List of article dictionaries
    """
    articles = []
    
    try:
        soup = BeautifulSoup(html_content, 'lxml')
        
        # T√¨m t·∫•t c·∫£ article elements
        # Selector: article[data-element-guid] trong .page-content
        page_content = soup.find('div', class_='page-content')
        if not page_content:
            # Fallback: t√¨m t·∫•t c·∫£ articles
            article_elements = soup.find_all('article', attrs={'data-element-guid': True})
            rows = []
        else:
            article_elements = page_content.find_all('article', attrs={'data-element-guid': True})
            # T√¨m rows n·∫øu l√† home page
            if is_home:
                rows = page_content.find_all('div', class_='row')
            else:
                rows = []
        
        print(f"üì∞ Found {len(article_elements)} article elements")
        
        # N·∫øu l√† home page, parse theo th·ª© t·ª± rows ƒë·ªÉ gi·ªØ ƒë√∫ng th·ª© t·ª±
        if is_home and page_content:
            total_rows = len(rows)
            print(f"üìê Home page structure: {total_rows} rows found")
            
            # Parse theo th·ª© t·ª± rows ƒë·ªÉ gi·ªØ ƒë√∫ng th·ª© t·ª±
            for row_idx, row in enumerate(rows):
                print(f"   üìê Processing row {row_idx + 1}/{total_rows}")
                # Check xem row c√≥ ch·ª©a slider kh√¥ng - t√¨m div c√≥ class ch·ª©a 'articlescroller'
                slider_elem = row.find('div', class_=lambda x: x and 'articlescroller' in x)
                if slider_elem:
                    # Check xem c√≥ ph·∫£i NUUK kh√¥ng (source_nuuk class)
                    slider_classes = slider_elem.get('class', [])
                    is_nuuk = 'source_nuuk' in slider_classes
                    
                    if is_nuuk:
                        # Parse NUUK nh∆∞ 5 articles ri√™ng l·∫ª
                        nuuk_articles = parse_nuuk_articles(slider_elem, base_url, row_idx)
                        if nuuk_articles:
                            articles.extend(nuuk_articles)
                            print(f"üèôÔ∏è  Parsed NUUK: {len(nuuk_articles)} articles")
                    else:
                        # Parse nh∆∞ slider th√¥ng th∆∞·ªùng
                        slider_data = parse_slider(slider_elem, base_url)
                        if slider_data:
                            slider_data['display_order'] = row_idx * 1000  # ƒê·∫∑t display_order d·ª±a tr√™n v·ªã tr√≠ row
                            slider_data['row_index'] = row_idx  # L∆∞u row_index
                            slider_data['total_rows'] = total_rows  # L∆∞u t·ªïng s·ªë rows
                            articles.append(slider_data)
                            slider_title = slider_data.get('layout_data', {}).get('slider_title', 'Untitled')
                            slider_articles_count = len(slider_data.get('layout_data', {}).get('slider_articles', []))
                            print(f"üé† Parsed slider '{slider_title}': {slider_articles_count} articles")
                
                # Parse articles trong row n√†y
                row_articles = row.find_all('article', attrs={'data-element-guid': True})
                total_articles_in_row = len(row_articles)
                
                print(f"      üìê Row {row_idx + 1} has {total_articles_in_row} articles")
                
                for article_idx, article_elem in enumerate(row_articles):
                    # Ch·ªâ parse n·∫øu article n√†y ch∆∞a ƒë∆∞·ª£c parse (tr√°nh duplicate)
                    if article_elem in article_elements:
                        article_data = parse_article_element(article_elem, base_url)
                        if article_data:
                            # L∆∞u th√¥ng tin chi ti·∫øt v·ªÅ row
                            article_data['display_order'] = row_idx * 1000 + article_idx  # ƒê·∫∑t display_order
                            article_data['row_index'] = row_idx  # L∆∞u row_index ƒë·ªÉ bi·∫øt article thu·ªôc h√†ng n√†o
                            article_data['article_index_in_row'] = article_idx  # L∆∞u v·ªã tr√≠ trong row
                            article_data['total_rows'] = total_rows  # L∆∞u t·ªïng s·ªë rows
                            
                            # Detect layout_type t·ª´ "d·∫°ng" th·ª±c s·ª± c·ªßa article (CSS classes v√† row structure)
                            # detect_layout_type_from_element s·∫Ω check:
                            # - CSS classes (large-4 = 3_articles, large-6 = 2_articles, large-12 = 1_full)
                            # - S·ªë l∆∞·ª£ng articles trong row (n·∫øu c√≥ row_elem)
                            # - C√≥ list b√™n c·∫°nh kh√¥ng (1_with_list_left/right)
                            layout_type = detect_layout_type_from_element(article_elem, row)
                            article_data['layout_type'] = layout_type
                            
                            # Log th√¥ng tin chi ti·∫øt v·ªÅ "d·∫°ng" v√† h√†ng
                            article_classes = article_elem.get('class', [])
                            class_str = ' '.join(article_classes) if article_classes else 'no-classes'
                            # Extract grid size classes ƒë·ªÉ log
                            grid_classes = [c for c in article_classes if 'large-' in c]
                            grid_str = ', '.join(grid_classes) if grid_classes else 'no-grid'
                            print(f"      üì∞ Article {article_idx + 1}/{total_articles_in_row} in row {row_idx + 1}: display_order={article_data['display_order']}, layout_type={layout_type}, grid={grid_str}, title={article_data.get('title', 'N/A')[:40]}")
                            
                            # Detect layout_data n·∫øu c√≥
                            layout_data = {}
                            
                            # Th√™m kicker_floating v√†o layout_data n·∫øu c√≥ (cho t·∫•t c·∫£ layout types)
                            if article_data.get('kicker_floating'):
                                layout_data['kicker_floating'] = article_data['kicker_floating']
                            
                            # Th√™m kicker_below v√†o layout_data n·∫øu c√≥ (cho t·∫•t c·∫£ layout types)
                            if article_data.get('kicker_below'):
                                layout_data['kicker_below'] = article_data['kicker_below']
                                layout_data['kicker_below_classes'] = article_data.get('kicker_below_classes', 'kicker below primary color_mobile_primary')
                            
                            # Th√™m title_parts v√†o layout_data n·∫øu c√≥ highlights (cho t·∫•t c·∫£ layout types)
                            if article_data.get('title_parts'):
                                layout_data['title_parts'] = article_data['title_parts']
                            
                            # Check v√† l∆∞u background colors n·∫øu article c√≥ bg-* (cho t·∫•t c·∫£ layout types)
                            content_div = article_elem.find('div', class_='content')
                            if content_div:
                                content_classes = content_div.get('class', [])
                                content_class_str = ' '.join(content_classes) if content_classes else ''
                                # Check b·∫•t k·ª≥ background color n√†o (bg-black, bg-secondary, bg-primary, etc.)
                                has_bg_color = any(cls.startswith('bg-') for cls in content_classes)
                                if has_bg_color:
                                    layout_data['has_bg_color'] = True
                                    # Gi·ªØ l·∫°i has_bg_black cho backward compatibility
                                    if 'bg-black' in content_class_str:
                                        layout_data['has_bg_black'] = True
                                    # L∆∞u t·∫•t c·∫£ classes c·ªßa content div ƒë·ªÉ gi·ªØ nguy√™n styling
                                    layout_data['content_classes'] = content_class_str
                            
                            if layout_type == '1_special_bg':
                                # Check kicker
                                kicker_elem = article_elem.find('div', class_='kicker')
                                if kicker_elem:
                                    layout_data['kicker'] = kicker_elem.get_text(strip=True)
                            elif layout_type in ['1_with_list_left', '1_with_list_right']:
                                # Parse list items t·ª´ row
                                # T√¨m list element (c√≥ th·ªÉ l√† articlesByTag ho·∫∑c toplist)
                                list_elem = row.find('div', class_='articlesByTag')
                                if not list_elem:
                                    list_elem = row.find('div', class_='toplist')
                                
                                if list_elem:
                                    # Extract list title - c√≥ th·ªÉ l√† h3 v·ªõi class headline ho·∫∑c kh√¥ng
                                    list_title_elem = list_elem.find('h3')
                                    if list_title_elem:
                                        layout_data['list_title'] = list_title_elem.get_text(strip=True)
                                    
                                    # Extract list items
                                    list_items = []
                                    # T√¨m trong ul.toplist-results ho·∫∑c ul th√¥ng th∆∞·ªùng
                                    ul_elem = list_elem.find('ul', class_='toplist-results')
                                    if not ul_elem:
                                        ul_elem = list_elem.find('ul')
                                    
                                    if ul_elem:
                                        for li in ul_elem.find_all('li'):
                                            link = li.find('a')
                                            if link:
                                                # T√¨m title - c√≥ th·ªÉ l√† h4 v·ªõi class abt-title ho·∫∑c h4 th√¥ng th∆∞·ªùng
                                                # Title c√≥ th·ªÉ n·∫±m trong link ho·∫∑c trong li
                                                title_elem = link.find('h4', class_='abt-title')
                                                if not title_elem:
                                                    title_elem = link.find('h4')
                                                if not title_elem:
                                                    # Fallback: t√¨m trong li
                                                    title_elem = li.find('h4', class_='abt-title')
                                                if not title_elem:
                                                    title_elem = li.find('h4')
                                                
                                                if title_elem:
                                                    title = title_elem.get_text(strip=True)
                                                    url = link.get('href', '')
                                                    if title and url:
                                                        list_items.append({
                                                            'title': title,
                                                            'url': url
                                                        })
                                    
                                    if list_items:
                                        layout_data['list_items'] = list_items
                            
                            if layout_data:
                                article_data['layout_data'] = layout_data
                            
                            articles.append(article_data)
        else:
            # Kh√¥ng ph·∫£i home page, parse articles nh∆∞ b√¨nh th∆∞·ªùng (kh√¥ng c√≥ layout detection)
            for article_elem in article_elements:
                article_data = parse_article_element(article_elem, base_url)
                if article_data:
                    articles.append(article_data)
        
        print(f"‚úÖ Successfully parsed {len(articles)} articles")
        
    except Exception as e:
        print(f"‚ùå Error parsing HTML: {e}")
        import traceback
        traceback.print_exc()
    
    return articles

