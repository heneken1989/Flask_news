"""
Parser ƒë·ªÉ extract article data t·ª´ HTML c·ªßa sermitsiaq.ag
"""
from bs4 import BeautifulSoup
from datetime import datetime
import re
from urllib.parse import urljoin, urlparse


def parse_article_element(article_element, base_url='https://www.sermitsiaq.ag'):
    """
    Parse m·ªôt article element t·ª´ HTML ƒë·ªÉ extract data
    
    Args:
        article_element: BeautifulSoup element c·ªßa article
        base_url: Base URL ƒë·ªÉ resolve relative URLs
    
    Returns:
        dict: Article data ho·∫∑c None n·∫øu kh√¥ng parse ƒë∆∞·ª£c
    """
    try:
        # L·∫•y element_guid t·ª´ data-element-guid attribute
        element_guid = article_element.get('data-element-guid', '')
        if not element_guid:
            return None
        
        # L·∫•y instance v√† site_alias
        instance = article_element.get('data-instance', '')
        site_alias = article_element.get('data-site-alias', 'sermitsiaq')
        section = article_element.get('data-section', '')
        
        # T√¨m link ch√≠nh c·ªßa article
        link_elem = article_element.find('a', itemprop='url')
        if not link_elem:
            return None
        
        # URL v√† k5a_url
        url = link_elem.get('href', '')
        k5a_url = link_elem.get('data-k5a-url', url)
        
        # Resolve relative URLs
        if url and not url.startswith('http'):
            url = urljoin(base_url, url)
        if k5a_url and not k5a_url.startswith('http'):
            k5a_url = urljoin(base_url, k5a_url)
        
        # Title
        title_elem = article_element.find('h2', class_='headline')
        title = title_elem.get_text(strip=True) if title_elem else ''
        if not title:
            return None
        
        # Published date
        time_elem = article_element.find('time', itemprop='datePublished')
        published_date = None
        if time_elem:
            datetime_str = time_elem.get('datetime', '')
            if datetime_str:
                try:
                    # Parse ISO format: 2026-01-15T20:29:57+01:00
                    published_date = datetime.fromisoformat(datetime_str.replace('Z', '+00:00'))
                except:
                    pass
        
        # Paywall check
        paywall_elem = article_element.find('div', class_='paywallLabel')
        is_paywall = paywall_elem is not None
        paywall_class = 'paywall' if is_paywall else ''
        
        # Image data
        image_data = parse_article_image(article_element, base_url)
        
        # Extract slug t·ª´ URL
        slug = extract_slug_from_url(url)
        
        # Extract article ID t·ª´ URL (n·∫øu c√≥)
        article_id = extract_article_id_from_url(url)
        
        return {
            'element_guid': element_guid,
            'title': title,
            'slug': slug,
            'url': url,
            'k5a_url': k5a_url,
            'section': section,
            'site_alias': site_alias,
            'instance': instance,
            'published_date': published_date,
            'is_paywall': is_paywall,
            'paywall_class': paywall_class,
            'image_data': image_data,
            'article_id': article_id,
        }
    except Exception as e:
        print(f"‚ö†Ô∏è  Error parsing article element: {e}")
        return None


def parse_article_image(article_element, base_url='https://www.sermitsiaq.ag'):
    """
    Parse image data t·ª´ article element
    
    Args:
        article_element: BeautifulSoup element c·ªßa article
        base_url: Base URL ƒë·ªÉ resolve relative URLs
    
    Returns:
        dict: Image data
    """
    image_data = {}
    
    try:
        # T√¨m figure element
        figure = article_element.find('figure')
        if not figure:
            return image_data
        
        # L·∫•y element_guid c·ªßa image
        image_element_guid = figure.get('data-element-guid', '')
        if image_element_guid:
            image_data['element_guid'] = image_element_guid
        
        # T√¨m picture element
        picture = figure.find('picture')
        if not picture:
            # Fallback: t√¨m img tr·ª±c ti·∫øp
            img = figure.find('img')
            if img:
                src = img.get('src', '')
                if src and not src.startswith('http'):
                    src = urljoin(base_url, src)
                image_data['fallback'] = src
                image_data['desktop_webp'] = src
                image_data['desktop_jpeg'] = src
                image_data['mobile_webp'] = src
                image_data['mobile_jpeg'] = src
                image_data['alt'] = img.get('alt', '')
                image_data['title'] = img.get('title', '')
                image_data['desktop_width'] = img.get('width', '524')
                image_data['desktop_height'] = img.get('height', '341')
                image_data['mobile_width'] = img.get('width', '480')
                image_data['mobile_height'] = img.get('height', '312')
            return image_data
        
        # Parse source elements
        sources = picture.find_all('source')
        for source in sources:
            srcset = source.get('srcset', '')
            media = source.get('media', '')
            img_type = source.get('type', '')
            
            if not srcset:
                continue
            
            # Resolve relative URLs
            if not srcset.startswith('http'):
                srcset = urljoin(base_url, srcset)
            
            # Determine image type
            if 'webp' in img_type:
                if 'min-width: 768px' in media or 'min-width: 768' in media:
                    image_data['desktop_webp'] = srcset
                elif 'max-width: 767px' in media or 'max-width: 767' in media:
                    image_data['mobile_webp'] = srcset
            elif 'jpeg' in img_type or 'jpg' in img_type:
                if 'min-width: 768px' in media or 'min-width: 768' in media:
                    image_data['desktop_jpeg'] = srcset
                elif 'max-width: 767px' in media or 'max-width: 767' in media:
                    image_data['mobile_jpeg'] = srcset
        
        # Fallback img
        img = picture.find('img')
        if img:
            src = img.get('src', '')
            if src and not src.startswith('http'):
                src = urljoin(base_url, src)
            image_data['fallback'] = src
            image_data['alt'] = img.get('alt', '')
            image_data['title'] = img.get('title', '')
            image_data['desktop_width'] = img.get('width', '524')
            image_data['desktop_height'] = img.get('height', '341')
            image_data['mobile_width'] = img.get('width', '480')
            image_data['mobile_height'] = img.get('height', '312')
        
    except Exception as e:
        print(f"‚ö†Ô∏è  Error parsing image: {e}")
    
    return image_data


def extract_slug_from_url(url):
    """
    Extract slug t·ª´ URL
    V√≠ d·ª•: https://www.sermitsiaq.ag/erhverv/article-slug/2329217 -> article-slug
    """
    if not url:
        return ''
    
    try:
        # Parse URL
        parsed = urlparse(url)
        path = parsed.path.strip('/')
        
        # Split path
        parts = path.split('/')
        if len(parts) >= 2:
            # L·∫•y ph·∫ßn tr∆∞·ªõc s·ªë ID
            slug = parts[-2] if parts[-1].isdigit() else parts[-1]
            return slug
        return ''
    except:
        return ''


def extract_article_id_from_url(url):
    """
    Extract article ID t·ª´ URL
    V√≠ d·ª•: https://www.sermitsiaq.ag/erhverv/article-slug/2329217 -> 2329217
    """
    if not url:
        return None
    
    try:
        parsed = urlparse(url)
        path = parsed.path.strip('/')
        parts = path.split('/')
        
        # L·∫•y s·ªë cu·ªëi c√πng trong path
        for part in reversed(parts):
            if part.isdigit():
                return int(part)
        return None
    except:
        return None


def detect_layout_type_from_element(article_elem, row_elem=None):
    """
    Detect layout_type t·ª´ article element v√† row structure
    
    Args:
        article_elem: BeautifulSoup element c·ªßa article
        row_elem: BeautifulSoup element c·ªßa row ch·ª©a article (optional)
    
    Returns:
        str: Layout type ('1_full', '2_articles', '3_articles', '1_special_bg', etc.)
    """
    try:
        # Check article classes ƒë·ªÉ x√°c ƒë·ªãnh grid size
        article_classes = article_elem.get('class', [])
        article_class_str = ' '.join(article_classes) if article_classes else ''
        
        # Check special background
        content_div = article_elem.find('div', class_='content')
        if content_div:
            content_classes = content_div.get('class', [])
            content_class_str = ' '.join(content_classes) if content_classes else ''
            if 'bg-black' in content_class_str:
                return '1_special_bg'
        
        # Check grid size t·ª´ classes
        if 'large-12' in article_class_str:
            # Full width - c√≥ th·ªÉ l√† 1_full
            return '1_full'
        elif 'large-6' in article_class_str:
            # 2 per row
            # Check xem c√≥ list b√™n c·∫°nh kh√¥ng
            if row_elem:
                # Check xem c√≥ articlesByTag ho·∫∑c toplist trong row kh√¥ng
                list_elem = row_elem.find('div', class_='articlesByTag')
                if not list_elem:
                    # Th·ª≠ t√¨m toplist
                    list_elem = row_elem.find('div', class_='toplist')
                
                if list_elem:
                    # Check v·ªã tr√≠ c·ªßa list (left or right)
                    # L·ªçc ch·ªâ l·∫•y elements c√≥ name (b·ªè qua text nodes, comments, etc.)
                    row_children = [child for child in row_elem.children 
                                   if hasattr(child, 'name') and child.name is not None]
                    
                    article_index = None
                    list_index = None
                    
                    for idx, child in enumerate(row_children):
                        if child.name == 'article':
                            # So s√°nh b·∫±ng c√°ch check element_guid ho·∫∑c so s√°nh tr·ª±c ti·∫øp
                            if child == article_elem or child.get('data-element-guid') == article_elem.get('data-element-guid'):
                                article_index = idx
                        elif child.name == 'div':
                            child_classes = child.get('class', [])
                            if 'articlesByTag' in child_classes or 'toplist' in child_classes:
                                list_index = idx
                    
                    if article_index is not None and list_index is not None:
                        if list_index < article_index:
                            return '1_with_list_left'
                        else:
                            return '1_with_list_right'
            return '2_articles'
        elif 'large-4' in article_class_str:
            # 3 per row
            return '3_articles'
        
        # Default
        return '1_full'
    except:
        return '1_full'  # Default fallback


def parse_articles_from_html(html_content, base_url='https://www.sermitsiaq.ag', is_home=False):
    """
    Parse t·∫•t c·∫£ articles t·ª´ HTML content
    
    Args:
        html_content: HTML content string
        base_url: Base URL ƒë·ªÉ resolve relative URLs
        is_home: N·∫øu True, s·∫Ω detect layout_type t·ª´ HTML structure
    
    Returns:
        list: List of article dictionaries
    """
    articles = []
    
    try:
        soup = BeautifulSoup(html_content, 'lxml')
        
        # T√¨m t·∫•t c·∫£ article elements
        # Selector: article[data-element-guid] trong .page-content
        page_content = soup.find('div', class_='page-content')
        if not page_content:
            # Fallback: t√¨m t·∫•t c·∫£ articles
            article_elements = soup.find_all('article', attrs={'data-element-guid': True})
            rows = []
        else:
            article_elements = page_content.find_all('article', attrs={'data-element-guid': True})
            # T√¨m rows n·∫øu l√† home page
            if is_home:
                rows = page_content.find_all('div', class_='row')
            else:
                rows = []
        
        print(f"üì∞ Found {len(article_elements)} article elements")
        
        for article_elem in article_elements:
            article_data = parse_article_element(article_elem, base_url)
            if article_data:
                # N·∫øu l√† home page, detect layout_type
                if is_home:
                    # T√¨m row ch·ª©a article n√†y
                    row_elem = None
                    for row in rows:
                        if article_elem in row.find_all('article'):
                            row_elem = row
                            break
                    
                    layout_type = detect_layout_type_from_element(article_elem, row_elem)
                    article_data['layout_type'] = layout_type
                    
                    # Detect layout_data n·∫øu c√≥
                    layout_data = {}
                    if layout_type == '1_special_bg':
                        # Check kicker
                        kicker_elem = article_elem.find('div', class_='kicker')
                        if kicker_elem:
                            layout_data['kicker'] = kicker_elem.get_text(strip=True)
                    elif layout_type in ['1_with_list_left', '1_with_list_right']:
                        # Parse list items t·ª´ row
                        if row_elem:
                            # T√¨m list element (c√≥ th·ªÉ l√† articlesByTag ho·∫∑c toplist)
                            list_elem = row_elem.find('div', class_='articlesByTag')
                            if not list_elem:
                                list_elem = row_elem.find('div', class_='toplist')
                            
                            if list_elem:
                                # Extract list title - c√≥ th·ªÉ l√† h3 v·ªõi class headline ho·∫∑c kh√¥ng
                                list_title_elem = list_elem.find('h3')
                                if list_title_elem:
                                    layout_data['list_title'] = list_title_elem.get_text(strip=True)
                                
                                # Extract list items
                                list_items = []
                                # T√¨m trong ul.toplist-results ho·∫∑c ul th√¥ng th∆∞·ªùng
                                ul_elem = list_elem.find('ul', class_='toplist-results')
                                if not ul_elem:
                                    ul_elem = list_elem.find('ul')
                                
                                if ul_elem:
                                    for li in ul_elem.find_all('li'):
                                        link = li.find('a')
                                        if link:
                                            # T√¨m title - c√≥ th·ªÉ l√† h4 v·ªõi class abt-title ho·∫∑c h4 th√¥ng th∆∞·ªùng
                                            # Title c√≥ th·ªÉ n·∫±m trong link ho·∫∑c trong li
                                            title_elem = link.find('h4', class_='abt-title')
                                            if not title_elem:
                                                title_elem = link.find('h4')
                                            if not title_elem:
                                                # Fallback: t√¨m trong li
                                                title_elem = li.find('h4', class_='abt-title')
                                            if not title_elem:
                                                title_elem = li.find('h4')
                                            
                                            if title_elem:
                                                title = title_elem.get_text(strip=True)
                                                url = link.get('href', '')
                                                if title and url:
                                                    list_items.append({
                                                        'title': title,
                                                        'url': url
                                                    })
                                
                                if list_items:
                                    layout_data['list_items'] = list_items
                    
                    if layout_data:
                        article_data['layout_data'] = layout_data
                
                articles.append(article_data)
        
        print(f"‚úÖ Successfully parsed {len(articles)} articles")
        
    except Exception as e:
        print(f"‚ùå Error parsing HTML: {e}")
        import traceback
        traceback.print_exc()
    
    return articles

