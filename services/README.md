# ğŸ•·ï¸ Crawl Service

Service Ä‘á»ƒ crawl articles tá»« sermitsiaq.ag sá»­ dá»¥ng SeleniumBase.

## ğŸ“‹ Cáº¥u trÃºc

```
services/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ crawl_service.py      # Main crawl service vá»›i SeleniumBase
â”œâ”€â”€ article_parser.py     # Parser Ä‘á»ƒ extract data tá»« HTML
â””â”€â”€ README.md
```

## ğŸš€ Sá»­ dá»¥ng

### 1. Crawl má»™t section

```bash
# Crawl section erhverv (máº·c Ä‘á»‹nh)
python3 scripts/crawl_articles.py erhverv

# Crawl vá»›i sá»‘ lÆ°á»£ng articles tÃ¹y chá»‰nh
python3 scripts/crawl_articles.py erhverv --max-articles 100

# Crawl vá»›i browser visible (Ä‘á»ƒ debug)
python3 scripts/crawl_articles.py erhverv --no-headless

# Crawl section khÃ¡c
python3 scripts/crawl_articles.py samfund
python3 scripts/crawl_articles.py kultur
python3 scripts/crawl_articles.py sport
python3 scripts/crawl_articles.py job

# Crawl custom URL
python3 scripts/crawl_articles.py erhverv --url "https://www.sermitsiaq.ag/tag/erhverv"
```

### 2. Sá»­ dá»¥ng trong code

```python
from app import app
from services.crawl_service import SermitsiaqCrawler

with app.app_context():
    crawler = SermitsiaqCrawler()
    
    try:
        crawler.start_browser(headless=True)
        result = crawler.crawl_section(
            section_url='https://www.sermitsiaq.ag/tag/erhverv',
            section_name='erhverv',
            max_articles=50
        )
        print(result)
    finally:
        crawler.close_browser()
```

## ğŸ“Š Dá»¯ liá»‡u Ä‘Æ°á»£c crawl

Má»—i article sáº½ Ä‘Æ°á»£c lÆ°u vá»›i cÃ¡c thÃ´ng tin:

- **element_guid**: GUID duy nháº¥t tá»« website gá»‘c
- **title**: TiÃªu Ä‘á» bÃ i viáº¿t
- **slug**: URL slug
- **url**: URL Ä‘áº§y Ä‘á»§
- **k5a_url**: URL cho K5A
- **section**: Section (erhverv, samfund, kultur, sport, job)
- **published_date**: NgÃ y publish
- **is_paywall**: CÃ³ pháº£i paywall khÃ´ng
- **image_data**: ThÃ´ng tin hÃ¬nh áº£nh (JSON)
  - `desktop_webp`, `desktop_jpeg`
  - `mobile_webp`, `mobile_jpeg`
  - `fallback`
  - `width`, `height`, `alt`, `title`
- **display_order**: Thá»© tá»± hiá»ƒn thá»‹ (0, 1, 2, ...)

## ğŸ–¼ï¸ HÃ¬nh áº£nh

**KhÃ´ng download hÃ¬nh áº£nh** - chá»‰ lÆ°u link tá»« website gá»‘c:
- Tiáº¿t kiá»‡m storage
- ÄÆ¡n giáº£n hÆ¡n
- LuÃ´n cÃ³ hÃ¬nh áº£nh má»›i nháº¥t tá»« CDN cá»§a há»

## ğŸ”„ Logic crawl

1. Má»Ÿ browser vá»›i SeleniumBase
2. Navigate Ä‘áº¿n section URL
3. Scroll Ä‘á»ƒ load thÃªm articles (lazy loading)
4. Parse HTML Ä‘á»ƒ extract article data
5. LÆ°u vÃ o database:
   - Náº¿u article Ä‘Ã£ tá»“n táº¡i (theo `element_guid`) â†’ Update
   - Náº¿u chÆ°a tá»“n táº¡i â†’ Create má»›i
6. Set `display_order` Ä‘á»ƒ match pattern 2-3-2-3-2-3...

## ğŸ“ LÆ°u Ã½

1. **Cáº§n database connection**: Äáº£m báº£o `.env` cÃ³ `DATABASE_URL` Ä‘Ãºng
2. **SeleniumBase**: Cáº§n cÃ i Ä‘áº·t Chrome/Chromium
3. **Rate limiting**: TrÃ¡nh crawl quÃ¡ nhanh Ä‘á»ƒ khÃ´ng bá»‹ block
4. **Headless mode**: Máº·c Ä‘á»‹nh cháº¡y headless (khÃ´ng hiá»ƒn thá»‹ browser)

## ğŸ› Troubleshooting

### Browser khÃ´ng khá»Ÿi Ä‘á»™ng
```bash
# CÃ i Ä‘áº·t Chrome/Chromium
# Ubuntu/Debian:
sudo apt-get install chromium-browser

# macOS:
brew install --cask google-chrome
```

### Lá»—i import
```bash
# CÃ i Ä‘áº·t dependencies
pip install -r requirements.txt
```

### Database connection error
```bash
# Kiá»ƒm tra .env file
cat .env | grep DATABASE_URL
```

