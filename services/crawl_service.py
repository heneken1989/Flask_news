"""
Crawl service s·ª≠ d·ª•ng SeleniumBase ƒë·ªÉ crawl articles t·ª´ sermitsiaq.ag
"""
import sys
from pathlib import Path

# Add parent directory to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from seleniumbase import SB
from services.article_parser import parse_articles_from_html
from services.image_downloader import download_and_update_image_data
from database import Article, CrawlLog, db
from datetime import datetime
import time


class SermitsiaqCrawler:
    """Crawler cho sermitsiaq.ag"""
    
    def __init__(self, base_url='https://www.sermitsiaq.ag', language='da'):
        """
        Initialize crawler
        
        Args:
            base_url: Base URL c·ªßa website (default: https://www.sermitsiaq.ag)
            language: Language code ('da' for Danish, 'kl' for Greenlandic)
        """
        self.base_url = base_url
        self.language = language
    
    def crawl_section(self, section_url, section_name='erhverv', max_articles=50, scroll_pause=2, headless=True, language=None):
        """
        Crawl articles t·ª´ m·ªôt section
        
        Args:
            section_url: URL c·ªßa section (v√≠ d·ª•: https://www.sermitsiaq.ag/tag/erhverv)
            section_name: T√™n section (erhverv, samfund, kultur, sport, podcasti)
            max_articles: S·ªë l∆∞·ª£ng articles t·ªëi ƒëa c·∫ßn crawl
            scroll_pause: Th·ªùi gian ch·ªù gi·ªØa c√°c l·∫ßn scroll (gi√¢y)
        
        Returns:
            dict: K·∫øt qu·∫£ crawl
        """
        crawl_log = CrawlLog(
            crawl_type='section',
            section=section_name,
            status='running',
            started_at=datetime.utcnow()
        )
        db.session.add(crawl_log)
        db.session.commit()
        
        articles_crawled = 0
        articles_created = 0
        errors = []
        
        try:
            print(f"üì∞ Crawling section: {section_name}")
            print(f"üîó URL: {section_url}")
            
            # S·ª≠ d·ª•ng SeleniumBase v·ªõi context manager
            with SB(uc=True, headless=headless) as sb:
                # Navigate to section page
                sb.open(section_url)
                time.sleep(3)  # Wait for page load
                
                # Scroll ƒë·ªÉ load th√™m articles (lazy loading)
                print("üìú Scrolling to load articles...")
                scroll_count = 0
                max_scrolls = 3  # T·ªëi ƒëa scroll 3 l·∫ßn
                previous_count = 0
                no_new_articles_count = 0
                
                while scroll_count < max_scrolls and (max_articles == 0 or articles_crawled < max_articles):
                    # Scroll down
                    sb.scroll_to_bottom()
                    time.sleep(scroll_pause)
                    
                    # Get current page HTML
                    html_content = sb.get_page_source()
                    articles = parse_articles_from_html(html_content, self.base_url, is_home=False)
                    current_count = len(articles)
                    
                    # Ki·ªÉm tra xem c√≥ articles m·ªõi kh√¥ng
                    if current_count == previous_count:
                        no_new_articles_count += 1
                        # N·∫øu 3 l·∫ßn scroll li√™n ti·∫øp kh√¥ng c√≥ articles m·ªõi, d·ª´ng l·∫°i
                        if no_new_articles_count >= 3:
                            print(f"  ‚èπÔ∏è  No new articles found after {no_new_articles_count} scrolls. Stopping.")
                            break
                    else:
                        no_new_articles_count = 0
                    
                    previous_count = current_count
                    scroll_count += 1
                    print(f"  Scroll {scroll_count}: Found {current_count} articles")
                    
                    if max_articles > 0 and current_count >= max_articles:
                        break
                
                # Get final HTML
                html_content = sb.get_page_source()
                articles = parse_articles_from_html(html_content, self.base_url, is_home=False)
                print(f"üîç After parsing: {len(articles)} articles")
            
            # Limit to max_articles (0 = no limit, crawl all)
            if max_articles > 0:
                articles = articles[:max_articles]
                print(f"üîç After limiting to {max_articles}: {len(articles)} articles")
            articles_crawled = len(articles)
            
            print(f"‚úÖ Crawled {articles_crawled} articles")
            
            # Determine language from base_url or parameter
            article_language = language or self.language
            
            # Check existing articles to avoid duplicates
            # ‚ö†Ô∏è QUAN TR·ªåNG: V·ªõi section crawl, CH·ªà check trong section ƒë√≥
            # (kh√°c v·ªõi home crawl - home c·∫ßn check ALL v√¨ c√≥ articles t·ª´ nhi·ªÅu sections)
            print(f"üîç Checking for existing {article_language} articles in section '{section_name}'...")
            existing_urls = {}  # Dict: {published_url: Article object}
            
            # ‚ö†Ô∏è CRITICAL: Refresh database session ƒë·ªÉ tr√°nh l·∫•y cached data c≈©
            db.session.expire_all()
            
            # CH·ªà check articles trong section n√†y
            existing_articles = Article.query.filter_by(
                section=section_name,
                language=article_language
            ).all()
            for art in existing_articles:
                if art.published_url:
                    existing_urls[art.published_url] = art
            print(f"   Found {len(existing_urls)} existing articles in section '{section_name}'")
            
            # Save new articles to database (only if not exists)
            print("üíæ Saving new articles to database...")
            articles_skipped = 0
            for idx, article_data in enumerate(articles):
                try:
                    # QUAN TR·ªåNG: Override section t·ª´ article_data v·ªõi section_name ƒëang crawl
                    # V√¨ parser c√≥ th·ªÉ l·∫•y section t·ª´ HTML (c√≥ th·ªÉ kh√¥ng ƒë√∫ng)
                    article_data['section'] = section_name
                    
                    # Determine language from base_url or parameter
                    article_language = language or self.language
                    
                    # Check if article already exists (by published_url)
                    article_url = article_data.get('url', '')
                    if article_url in existing_urls:
                        articles_skipped += 1
                        if articles_skipped % 10 == 0:
                            print(f"  ‚è≠Ô∏è  Skipped {articles_skipped} existing articles...")
                        continue
                    
                    # Download v√† c·∫≠p nh·∫≠t image_data n·∫øu c√≥
                    image_data = article_data.get('image_data', {})
                    if image_data:
                        try:
                            print(f"  üì• Downloading header image for article: {article_data.get('title', '')[:50]}...")
                            image_data = download_and_update_image_data(
                                image_data,
                                base_url='https://www.sermitsiaq.com',
                                download_all_formats=False  # Ch·ªâ download desktop_webp v√† fallback
                            )
                        except Exception as e:
                            print(f"  ‚ö†Ô∏è  Error downloading image: {e}")
                            # Gi·ªØ nguy√™n image_data g·ªëc n·∫øu l·ªói
                    
                    # T·∫°o article m·ªõi
                    new_article = Article(
                        element_guid=article_data.get('element_guid'),  # C√≥ th·ªÉ None, kh√¥ng unique
                        title=article_data['title'],
                        slug=article_data['slug'],
                        published_url=article_data['url'],
                        k5a_url=article_data['k5a_url'],
                        section=section_name,
                        site_alias=article_data.get('site_alias', 'sermitsiaq'),
                        instance=article_data.get('instance', ''),
                        published_date=article_data.get('published_date'),
                        is_paywall=article_data['is_paywall'],
                        paywall_class=article_data['paywall_class'],
                        image_data=image_data,  # ƒê√£ ƒë∆∞·ª£c download v√† c·∫≠p nh·∫≠t
                        display_order=idx,  # Set display_order ƒë·ªÉ match pattern
                        language=article_language,  # Set language
                        original_language=article_language,  # Set original_language
                    )
                    db.session.add(new_article)
                    
                    # ‚ö†Ô∏è CRITICAL: Wrap commit trong try-except ƒë·ªÉ catch IntegrityError (race condition)
                    try:
                        # Commit m·ªói 10 articles ƒë·ªÉ tr√°nh timeout
                        if (articles_created + 1) % 10 == 0:
                            db.session.commit()
                            print(f"  üíæ Saved {articles_created + 1} new articles, skipped {articles_skipped} existing...")
                        
                        articles_created += 1
                        if article_url:
                            existing_urls[article_url] = new_article  # Add to dict to avoid duplicates in same batch
                    except Exception as commit_error:
                        # IntegrityError ho·∫∑c unique constraint violation (race condition)
                        db.session.rollback()
                        error_msg_str = str(commit_error)
                        if 'unique' in error_msg_str.lower() or 'duplicate' in error_msg_str.lower():
                            print(f"  ‚è≠Ô∏è  Article already exists (duplicate detected during commit), skipping...")
                            articles_skipped += 1
                            if article_url:
                                existing_urls[article_url] = None  # Mark as processed
                        else:
                            # Re-raise n·∫øu kh√¥ng ph·∫£i duplicate error
                            raise
                
                except Exception as e:
                    error_msg = f"Error saving article {article_data.get('element_guid', 'unknown')}: {str(e)}"
                    errors.append(error_msg)
                    print(f"  ‚ö†Ô∏è  {error_msg}")
                    db.session.rollback()
                    continue
            
            # Final commit
            db.session.commit()
            print(f"‚úÖ Successfully saved {articles_created} new articles, skipped {articles_skipped} existing articles")
            
            # Update crawl log
            crawl_log.status = 'success' if not errors else 'partial'
            crawl_log.articles_crawled = articles_crawled
            crawl_log.articles_created = articles_created
            crawl_log.articles_updated = 0  # Kh√¥ng c√≤n update, ch·ªâ create m·ªõi v·ªõi ID m·ªõi
            crawl_log.completed_at = datetime.utcnow()
            if errors:
                crawl_log.errors = '\n'.join(errors[:10])  # Limit to first 10 errors
            db.session.commit()
            
            print(f"‚úÖ Crawl completed!")
            print(f"   üìä Articles crawled: {articles_crawled}")
            print(f"   ‚ûï Articles created: {articles_created} (with new IDs)")
            if errors:
                print(f"   ‚ö†Ô∏è  Errors: {len(errors)}")
            
            return {
                'success': True,
                'articles_crawled': articles_crawled,
                'articles_created': articles_created,
                'articles_updated': 0,
                'errors': errors
            }
        
        except Exception as e:
            error_msg = f"Crawl failed: {str(e)}"
            print(f"‚ùå {error_msg}")
            errors.append(error_msg)
            
            # Update crawl log
            crawl_log.status = 'failed'
            crawl_log.errors = '\n'.join(errors)
            crawl_log.completed_at = datetime.utcnow()
            db.session.commit()
            
            return {
                'success': False,
                'articles_crawled': articles_crawled,
                'articles_created': articles_created,
                'articles_updated': 0,
                'errors': errors
            }
    
    def crawl_home(self, home_url=None, max_articles=100, scroll_pause=2, headless=True, language=None):
        """
        Crawl articles t·ª´ trang home
        
        Args:
            home_url: URL c·ªßa trang home
            max_articles: S·ªë l∆∞·ª£ng articles t·ªëi ƒëa c·∫ßn crawl
            scroll_pause: Th·ªùi gian ch·ªù gi·ªØa c√°c l·∫ßn scroll (gi√¢y)
            headless: Ch·∫°y browser ·ªü ch·∫ø ƒë·ªô headless
        
        Returns:
            dict: K·∫øt qu·∫£ crawl
        """
        crawl_log = CrawlLog(
            crawl_type='home',
            section='home',
            status='running',
            started_at=datetime.utcnow()
        )
        db.session.add(crawl_log)
        db.session.commit()
        
        articles_crawled = 0
        articles_created = 0
        articles_updated = 0
        errors = []
        
        try:
            print(f"üè† Crawling home page: {home_url}")
            
            # S·ª≠ d·ª•ng SeleniumBase v·ªõi context manager
            with SB(uc=True, headless=headless) as sb:
                # Navigate to home page
                sb.open(home_url)
                time.sleep(3)  # Wait for page load
                
                # Scroll ƒë·ªÉ load th√™m articles (lazy loading)
                print("üìú Scrolling to load articles...")
                scroll_count = 0
                # TƒÉng max_scrolls n·∫øu crawl all (max_articles=0)
                max_scrolls = 100 if max_articles == 0 else 50  # TƒÉng s·ªë l·∫ßn scroll ƒë·ªÉ load t·∫•t c·∫£ articles
                previous_count = 0
                no_new_articles_count = 0
                
                # Crawl t·∫•t c·∫£ articles n·∫øu max_articles = 0 ho·∫∑c r·∫•t l·ªõn
                crawl_all = max_articles == 0 or max_articles >= 1000
                
                while scroll_count < max_scrolls:
                    # Scroll down
                    sb.scroll_to_bottom()
                    time.sleep(scroll_pause)
                    
                    # Get current page HTML
                    html_content = sb.get_page_source()
                    articles = parse_articles_from_html(html_content, self.base_url, is_home=True)
                    current_count = len(articles)
                    
                    # Ki·ªÉm tra xem c√≥ articles m·ªõi kh√¥ng
                    if current_count == previous_count:
                        no_new_articles_count += 1
                        # N·∫øu 3 l·∫ßn scroll li√™n ti·∫øp kh√¥ng c√≥ articles m·ªõi, d·ª´ng l·∫°i
                        if no_new_articles_count >= 3:
                            print(f"  ‚èπÔ∏è  No new articles found after {no_new_articles_count} scrolls. Stopping.")
                            break
                    else:
                        no_new_articles_count = 0
                    
                    previous_count = current_count
                    scroll_count += 1
                    print(f"  Scroll {scroll_count}: Found {current_count} articles")
                    
                    # N·∫øu kh√¥ng crawl all v√† ƒë√£ ƒë·ªß s·ªë l∆∞·ª£ng, d·ª´ng l·∫°i
                    if not crawl_all and current_count >= max_articles:
                        print(f"  ‚úÖ Reached max articles limit ({max_articles})")
                        break
                
                # Get final HTML
                html_content = sb.get_page_source()
                articles = parse_articles_from_html(html_content, self.base_url, is_home=True)
            
            # Limit to max_articles n·∫øu kh√¥ng crawl all
            if not crawl_all and max_articles > 0:
                articles = articles[:max_articles]
            articles_crawled = len(articles)
            
            print(f"‚úÖ Crawled {articles_crawled} articles from home page")
            
            # Log th√¥ng tin v·ªÅ rows n·∫øu c√≥
            if articles:
                row_info = {}
                for article_data in articles:
                    row_idx = article_data.get('row_index', -1)
                    if row_idx >= 0:
                        if row_idx not in row_info:
                            row_info[row_idx] = []
                        row_info[row_idx].append({
                            'title': article_data.get('title', 'N/A')[:40],
                            'layout_type': article_data.get('layout_type', 'N/A'),
                            'display_order': article_data.get('display_order', 0)
                        })
                
                print(f"üìê Home page structure summary:")
                print(f"   Total rows: {articles[0].get('total_rows', 'N/A') if articles else 'N/A'}")
                for row_idx in sorted(row_info.keys()):
                    articles_in_row = row_info[row_idx]
                    print(f"   Row {row_idx + 1}: {len(articles_in_row)} items - {[a['layout_type'] for a in articles_in_row]}")
            
            # Determine language from base_url or parameter
            article_language = language or self.language
            
            # Check existing articles tr∆∞·ªõc khi crawl ƒë·ªÉ bi·∫øt articles n√†o ƒë√£ t·ªìn t·∫°i
            print(f"üîç Checking for existing {article_language} articles...")
            
            # ‚ö†Ô∏è CRITICAL: Refresh database session ƒë·ªÉ tr√°nh l·∫•y cached data c≈©
            db.session.expire_all()
            
            existing_articles_map = {}  # Dict: {published_url: Article} ho·∫∑c {(layout_type, display_order): Article} cho sliders
            
            # ‚ö†Ô∏è CRITICAL: Check T·∫§T C·∫¢ articles theo published_url + language
            # KH√îNG filter theo section v√¨ articles t·ª´ c√°c sections kh√°c c√≥ th·ªÉ xu·∫•t hi·ªán tr√™n home
            # (v√≠ d·ª•: erhverv articles tr√™n home page)
            existing_articles = Article.query.filter_by(
                language=article_language
            ).all()
            
            for art in existing_articles:
                if art.published_url:
                    existing_articles_map[art.published_url] = art
                elif art.layout_type in ['slider', 'job_slider']:
                    # Slider containers: key b·∫±ng (layout_type, display_order)
                    # Ch·ªâ l∆∞u sliders c√≥ section='home'
                    if art.section == 'home':
                        key = (art.layout_type, art.display_order)
                        existing_articles_map[key] = art
            
            print(f"   Found {len(existing_articles_map)} existing {article_language} articles (all sections)")
            
            # Save new articles to database (update s·∫Ω l√†m sau)
            print("üíæ Saving new home articles...")
            articles_skipped = 0
            articles_updated = 0
            articles_not_found_in_home = 0  # Track articles kh√¥ng t√¨m th·∫•y trong home
            updated_article_ids = set()  # Track IDs ƒë√£ ƒë∆∞·ª£c update ƒë·ªÉ tr√°nh ƒë·∫øm tr√πng
            skipped_articles_info = []  # Track th√¥ng tin articles b·ªã skip ƒë·ªÉ debug
            articles_to_update = []  # Track articles c·∫ßn update sau khi save xong
            existing_urls = {}  # Dict: {published_url: Article} - Track URLs trong batch n√†y ƒë·ªÉ tr√°nh duplicate
            for idx, article_data in enumerate(articles):
                try:
                    # ‚ö†Ô∏è KH√îNG set section='home' hardcoded ·ªü ƒë√¢y
                    # Section s·∫Ω ƒë∆∞·ª£c detect t·ª´ URL cho c√°c layout types: 1_full, 1_article, 2_articles, 3_articles, 1_special_bg
                    # (xem logic ·ªü d√≤ng 690-711)
                    
                    # S·ª≠ d·ª•ng display_order t·ª´ parser n·∫øu c√≥, n·∫øu kh√¥ng th√¨ d√πng idx
                    display_order = article_data.get('display_order', idx)
                    
                    # Check if article already exists (by published_url, section='home', is_home=True, language)
                    article_url = article_data.get('url', '')
                    layout_type = article_data.get('layout_type', '')
                    
                    # Slider containers (slider, job_slider, 5_articles) kh√¥ng c√≥ URL nh∆∞ng v·∫´n c·∫ßn ƒë∆∞·ª£c l∆∞u ƒë·ªÉ gi·ªØ c·∫•u tr√∫c home page
                    # S·ª≠ d·ª•ng element_guid ho·∫∑c display_order ƒë·ªÉ identify
                    # ‚ö†Ô∏è QUAN TR·ªåNG: 5_articles gi·ªù l√† container (kh√¥ng t·∫°o individual articles), gi·ªëng slider v√† job_slider
                    is_slider_container = layout_type in ['slider', 'job_slider', '5_articles'] and not article_url
                    
                    if not article_url and not is_slider_container:
                        # Kh√¥ng c√≥ URL v√† kh√¥ng ph·∫£i slider container, skip
                        skip_info = {
                            'layout_type': layout_type,
                            'display_order': display_order,
                            'title': article_data.get('title', 'N/A')[:50],
                            'url': article_url or '(no URL)',
                            'reason': 'no_url_not_slider'
                        }
                        skipped_articles_info.append(skip_info)
                        print(f"  ‚ö†Ô∏è  Skipping article without URL (not a slider): layout_type={layout_type}, display_order={display_order}, title={skip_info['title']}, url={skip_info['url']}")
                        articles_skipped += 1
                        continue
                    
                    # V·ªõi slider containers, s·ª≠ d·ª•ng element_guid ho·∫∑c display_order l√†m identifier
                    if is_slider_container:
                        # T·∫†M B·ªé QUA: Update logic - lu√¥n t·∫°o m·ªõi slider container
                        # T·∫°o m·ªôt identifier duy nh·∫•t cho slider container
                        element_guid = article_data.get('element_guid', '')
                        slider_id = article_data.get('layout_data', {}).get('slider_id', '')
                        slider_title = article_data.get('layout_data', {}).get('slider_title', 'Untitled')
                        # S·ª≠ d·ª•ng element_guid ho·∫∑c slider_id l√†m identifier
                        article_identifier = element_guid or slider_id or f"slider_{display_order}"
                        # T·∫†M B·ªé QUA: Check existing slider container - lu√¥n t·∫°o m·ªõi
                        # existing_slider = Article.query.filter_by(
                        #     section='home',
                        #     is_home=True,
                        #     language=article_language,
                        #     layout_type=layout_type,
                        #     display_order=display_order
                        # ).first()
                        # 
                        # if existing_slider:
                        #     # Update existing slider container
                        #     existing_slider.display_order = display_order
                        #     existing_slider.layout_type = layout_type
                        #     layout_data = article_data.get('layout_data', {})
                        #     layout_data['row_index'] = article_data.get('row_index', -1)
                        #     layout_data['article_index_in_row'] = article_data.get('article_index_in_row', -1)
                        #     layout_data['total_rows'] = article_data.get('total_rows', 0)
                        #     existing_slider.layout_data = layout_data
                        #     existing_slider.grid_size = article_data.get('grid_size', 6)
                        #     existing_slider.is_home = True
                        #     existing_slider.section = 'home'
                        #     
                        #     if existing_slider.id not in updated_article_ids:
                        #         updated_article_ids.add(existing_slider.id)
                        #         articles_updated += 1
                        #         print(f"  üîÑ Updated slider container: {layout_type} '{slider_title}' (display_order={display_order})")
                        #     
                        #     # Kh√¥ng ƒë·∫øm v√†o articles_skipped v√¨ ƒë√£ ƒë∆∞·ª£c update
                        #     continue
                        # N·∫øu kh√¥ng t√¨m th·∫•y, s·∫Ω t·∫°o m·ªõi ·ªü d∆∞·ªõi (v·ªõi published_url='')
                        print(f"  ‚ûï Will create new slider container: {layout_type} '{slider_title}' (display_order={display_order})")
                        article_url = ''  # Gi·ªØ empty ƒë·ªÉ kh√¥ng match v·ªõi existing_urls
                        # Note: Slider containers s·∫Ω ƒë∆∞·ª£c t·∫°o m·ªõi ·ªü ph·∫ßn t·∫°o article b√™n d∆∞·ªõi
                    
                    # T·∫†M B·ªé QUA: T·∫•t c·∫£ logic update - lu√¥n t·∫°o m·ªõi article (ch·ªâ cho home)
                    # if article_url in existing_urls:
                    # B·ªè qua t·∫•t c·∫£ check existing article, lu√¥n t·∫°o m·ªõi
                    # if article_url:  # Ch·ªâ check n·∫øu c√≥ URL
                    #     # QUAN TR·ªåNG: Ch·ªâ check duplicate trong ph·∫°m vi home page (section='home', is_home=True)
                    #     existing_article = Article.query.filter_by(
                    #         published_url=article_url,
                    #         language=article_language,
                    #         section='home',
                    #         is_home=True  # QUAN TR·ªåNG: Ch·ªâ check v·ªõi is_home=True
                    #     ).first()
                    #     
                    #     if existing_article:
                    #         # Verify ƒëi·ªÅu ki·ªán tr∆∞·ªõc khi update
                    #         if existing_article.section != 'home' or not existing_article.is_home:
                    #             skip_info = {
                    #                 'layout_type': layout_type,
                    #                 'display_order': display_order,
                    #                 'title': article_data.get('title', 'N/A')[:50],
                    #                 'url': article_url,
                    #                 'reason': f'section_mismatch (section={existing_article.section}, is_home={existing_article.is_home})'
                    #             }
                    #             skipped_articles_info.append(skip_info)
                    #             print(f"  ‚ö†Ô∏è  WARNING: Found article ID {existing_article.id} but section={existing_article.section}, is_home={existing_article.is_home}. Skipping update. URL: {article_url}")
                    #             articles_skipped += 1
                    #             continue
                    #         
                    #         # Article ƒë√£ t·ªìn t·∫°i trong home: update display_order, layout_type, layout_data ƒë·ªÉ gi·ªØ ƒë√∫ng th·ª© t·ª±
                    #         existing_article.display_order = display_order
                    #         existing_article.layout_type = article_data.get('layout_type')
                    #         
                    #         # Merge layout_data v·ªõi th√¥ng tin row
                    #         layout_data = article_data.get('layout_data', {})
                    #         layout_data['row_index'] = article_data.get('row_index', -1)
                    #         layout_data['article_index_in_row'] = article_data.get('article_index_in_row', -1)
                    #         layout_data['total_rows'] = article_data.get('total_rows', 0)
                    #         existing_article.layout_data = layout_data
                    #         
                    #         existing_article.grid_size = article_data.get('grid_size', 6)
                    #         # ƒê·∫£m b·∫£o is_home=True v√† section='home'
                    #         existing_article.is_home = True
                    #         existing_article.section = 'home'
                    #         
                    #         # Ch·ªâ ƒë·∫øm n·∫øu ch∆∞a ƒë∆∞·ª£c update tr∆∞·ªõc ƒë√≥
                    #         if existing_article.id not in updated_article_ids:
                    #             updated_article_ids.add(existing_article.id)
                    #             articles_updated += 1
                    #         
                    #         # Kh√¥ng ƒë·∫øm v√†o articles_skipped v√¨ ƒë√£ ƒë∆∞·ª£c update
                    #         if articles_updated % 10 == 0:
                    #             print(f"  üîÑ Updated display_order for {articles_updated} existing home articles...")
                    #         continue
                    #     else:
                    #         # C√≥ trong existing_urls nh∆∞ng kh√¥ng t√¨m th·∫•y v·ªõi ƒëi·ªÅu ki·ªán ƒë·∫ßy ƒë·ªß
                    #         # C√≥ th·ªÉ l√† article t·ª´ section page, kh√¥ng ph·∫£i home
                    #         # Ho·∫∑c c√≥ th·ªÉ c√≥ v·∫•n ƒë·ªÅ v·ªõi URL format
                    #         # T√¨m article ·ªü section kh√°c v√† update ƒë·ªÉ th√™m v√†o home
                    #         all_articles_with_url = Article.query.filter_by(
                    #             published_url=article_url,
                    #             language=article_language
                    #         ).all()
                    #         
                    #         if all_articles_with_url:
                    #             # T√¨m article ƒë·∫ßu ti√™n (c√≥ th·ªÉ c√≥ nhi·ªÅu b·∫£n copy)
                    #             article_to_update = all_articles_with_url[0]
                    #             
                    #             # Update article n√†y ƒë·ªÉ th√™m v√†o home page
                    #             if article_to_update.id not in updated_article_ids:
                    #                 article_to_update.display_order = display_order
                    #                 article_to_update.layout_type = article_data.get('layout_type')
                    #                 layout_data = article_data.get('layout_data', {})
                    #                 layout_data['row_index'] = article_data.get('row_index', -1)
                    #                 layout_data['article_index_in_row'] = article_data.get('article_index_in_row', -1)
                    #                 layout_data['total_rows'] = article_data.get('total_rows', 0)
                    #                 article_to_update.layout_data = layout_data
                    #                 article_to_update.grid_size = article_data.get('grid_size', 6)
                    #                 # ƒê·∫£m b·∫£o is_home=True v√† section='home'
                    #                 article_to_update.is_home = True
                    #                 article_to_update.section = 'home'
                    #                 
                    #                 updated_article_ids.add(article_to_update.id)
                    #                 articles_updated += 1
                    #                 
                    #                 if articles_updated % 10 == 0:
                    #                     print(f"  üîÑ Updated display_order for {articles_updated} existing home articles...")
                    #             # else: article ƒë√£ ƒë∆∞·ª£c update tr∆∞·ªõc ƒë√≥, kh√¥ng c·∫ßn ƒë·∫øm l·∫°i
                    #         else:
                    #             # Kh√¥ng t√¨m th·∫•y article n√†o, s·∫Ω t·∫°o m·ªõi ·ªü d∆∞·ªõi
                    #             articles_not_found_in_home += 1
                    #             print(f"  ‚ö†Ô∏è  WARNING: URL '{article_url[:60]}...' not found in database. Will create new article.")
                    
                    # Determine language from base_url or parameter (c·∫ßn x√°c ƒë·ªãnh tr∆∞·ªõc khi check skip)
                    article_language = language or self.language
                    
                    # Check xem article ƒë√£ t·ªìn t·∫°i ch∆∞a (ch·ªâ check, kh√¥ng update ngay)
                    if is_slider_container:
                        # Slider containers: check b·∫±ng (layout_type, display_order)
                        key = (layout_type, display_order)
                        if key in existing_articles_map:
                            # ƒê√£ t·ªìn t·∫°i, s·∫Ω update sau khi save xong t·∫•t c·∫£ articles m·ªõi
                            articles_to_update.append({
                                'type': 'slider',
                                'key': key,
                                'article': existing_articles_map[key],
                                'article_data': article_data,
                                'display_order': display_order
                            })
                            continue
                    elif article_url:
                        # Articles c√≥ URL: check b·∫±ng published_url
                        # ‚ö†Ô∏è QUAN TR·ªåNG: V·ªõi 1_with_list_left/right, ch·ªâ check trong section='home'
                        # V√¨ ch√∫ng ch·ªâ xu·∫•t hi·ªán ·ªü home, kh√¥ng n√™n check trong c√°c sections kh√°c
                        if layout_type in ['1_with_list_left', '1_with_list_right']:
                            # Check ri√™ng trong section='home'
                            existing_article = Article.query.filter_by(
                                published_url=article_url,
                                language=article_language,
                                section='home'
                            ).first()
                            
                            if existing_article:
                                # ƒê√£ t·ªìn t·∫°i trong section='home', s·∫Ω update sau
                                articles_to_update.append({
                                    'type': 'article',
                                    'key': article_url,
                                    'article': existing_article,
                                    'article_data': article_data,
                                    'display_order': display_order
                                })
                                continue
                        else:
                            # Articles kh√°c: check trong t·∫•t c·∫£ sections (nh∆∞ hi·ªán t·∫°i)
                            if article_url in existing_articles_map:
                                # ƒê√£ t·ªìn t·∫°i, s·∫Ω update sau khi save xong t·∫•t c·∫£ articles m·ªõi
                                articles_to_update.append({
                                    'type': 'article',
                                    'key': article_url,
                                    'article': existing_articles_map[article_url],
                                    'article_data': article_data,
                                    'display_order': display_order
                                })
                                continue
                    
                    # Lu√¥n t·∫°o m·ªõi article (n·∫øu ch∆∞a t·ªìn t·∫°i)
                    print(f"  ‚ûï Will create new article: {article_data.get('title', 'Untitled')[:50]}... (URL: {article_url[:60] if article_url else 'no URL'}...)")
                    
                    # Download v√† c·∫≠p nh·∫≠t image_data n·∫øu c√≥
                    image_data = article_data.get('image_data', {})
                    if image_data:
                        try:
                            print(f"  üì• Downloading header image for article: {article_data.get('title', 'Untitled')[:50]}...")
                            image_data = download_and_update_image_data(
                                image_data,
                                base_url='https://www.sermitsiaq.com',
                                download_all_formats=False  # Ch·ªâ download desktop_webp v√† fallback
                            )
                        except Exception as e:
                            print(f"  ‚ö†Ô∏è  Error downloading image: {e}")
                            # Gi·ªØ nguy√™n image_data g·ªëc n·∫øu l·ªói
                    
                    # Merge layout_data v·ªõi th√¥ng tin row
                    layout_data = article_data.get('layout_data', {})
                    layout_data['row_index'] = article_data.get('row_index', -1)
                    layout_data['article_index_in_row'] = article_data.get('article_index_in_row', -1)
                    layout_data['total_rows'] = article_data.get('total_rows', 0)
                    
                    # T·∫†M B·ªé QUA: Ki·ªÉm tra l·∫°i m·ªôt l·∫ßn n·ªØa tr∆∞·ªõc khi t·∫°o - lu√¥n t·∫°o m·ªõi (ch·ªâ cho home)
                    # # Ki·ªÉm tra l·∫°i m·ªôt l·∫ßn n·ªØa tr∆∞·ªõc khi t·∫°o
                    # # V·ªõi slider containers (kh√¥ng c√≥ URL), check b·∫±ng display_order + layout_type
                    # # V·ªõi articles c√≥ URL, check b·∫±ng published_url
                    # if is_slider_container:
                    #     # Slider containers: check b·∫±ng display_order + layout_type
                    #     final_check = Article.query.filter_by(
                    #         section='home',
                    #         is_home=True,
                    #         language=article_language,
                    #         layout_type=layout_type,
                    #         display_order=display_order
                    #     ).first()
                    #     
                    #     if final_check:
                    #         # Update existing slider container
                    #         final_check.display_order = display_order
                    #         final_check.layout_type = layout_type
                    #         layout_data = article_data.get('layout_data', {})
                    #         layout_data['row_index'] = article_data.get('row_index', -1)
                    #         layout_data['article_index_in_row'] = article_data.get('article_index_in_row', -1)
                    #         layout_data['total_rows'] = article_data.get('total_rows', 0)
                    #         final_check.layout_data = layout_data
                    #         final_check.grid_size = article_data.get('grid_size', 6)
                    #         final_check.is_home = True
                    #         final_check.section = 'home'
                    #         
                    #         if final_check.id not in updated_article_ids:
                    #             updated_article_ids.add(final_check.id)
                    #             articles_updated += 1
                    #         
                    #         # Kh√¥ng ƒë·∫øm v√†o articles_skipped v√¨ ƒë√£ ƒë∆∞·ª£c update
                    #         continue
                    #     # N·∫øu kh√¥ng t√¨m th·∫•y, s·∫Ω t·∫°o m·ªõi ·ªü d∆∞·ªõi
                    # T·∫†M B·ªé QUA: Final check - lu√¥n t·∫°o m·ªõi (ch·ªâ cho home)
                    # elif article_url and article_url not in existing_urls:
                    #     # Articles c√≥ URL: check b·∫±ng published_url
                    #     final_check = Article.query.filter_by(
                    #         published_url=article_url,
                    #         language=article_language,
                    #         section='home',
                    #         is_home=True
                    #     ).first()
                    #     
                    #     if final_check:
                    #         # Verify ƒëi·ªÅu ki·ªán tr∆∞·ªõc khi update
                    #         if final_check.section != 'home' or not final_check.is_home:
                    #             skip_info = {
                    #                 'layout_type': layout_type,
                    #                 'display_order': display_order,
                    #                 'title': article_data.get('title', 'N/A')[:50],
                    #                 'url': article_url,
                    #                 'reason': f'final_check_section_mismatch (section={final_check.section}, is_home={final_check.is_home})'
                    #             }
                    #             skipped_articles_info.append(skip_info)
                    #             print(f"  ‚ö†Ô∏è  WARNING: Found article ID {final_check.id} in final_check but section={final_check.section}, is_home={final_check.is_home}. Skipping update. URL: {article_url}")
                    #             existing_urls.add(article_url)  # Add ƒë·ªÉ tr√°nh check l·∫°i
                    #             articles_skipped += 1
                    #             continue
                    #         
                    #         # ƒê√£ t·ªìn t·∫°i, skip v√† update (ch·ªâ update n·∫øu ch∆∞a ƒë∆∞·ª£c update ·ªü tr√™n)
                    #         final_check.display_order = display_order
                    #         final_check.layout_type = article_data.get('layout_type')
                    #         layout_data = article_data.get('layout_data', {})
                    #         layout_data['row_index'] = article_data.get('row_index', -1)
                    #         layout_data['article_index_in_row'] = article_data.get('article_index_in_row', -1)
                    #         layout_data['total_rows'] = article_data.get('total_rows', 0)
                    #         final_check.layout_data = layout_data
                    #         final_check.grid_size = article_data.get('grid_size', 6)
                    #         # ƒê·∫£m b·∫£o is_home=True v√† section='home'
                    #         final_check.is_home = True
                    #         final_check.section = 'home'
                    #         
                    #         # Ch·ªâ ƒë·∫øm n·∫øu ch∆∞a ƒë∆∞·ª£c update tr∆∞·ªõc ƒë√≥
                    #         if final_check.id not in updated_article_ids:
                    #             updated_article_ids.add(final_check.id)
                    #             articles_updated += 1
                    #         
                    #         # Kh√¥ng ƒë·∫øm v√†o articles_skipped v√¨ ƒë√£ ƒë∆∞·ª£c update
                    #         existing_urls.add(article_url)  # Add ƒë·ªÉ tr√°nh duplicate trong c√πng batch
                    #         if articles_updated % 10 == 0:
                    #             print(f"  ‚è≠Ô∏è  Updated {articles_updated} existing home articles (final check)...")
                    #         continue
                    
                    # Add v√†o existing_urls ƒë·ªÉ tr√°nh duplicate trong c√πng batch (n·∫øu ch∆∞a c√≥)
                    if article_url and article_url not in existing_urls:
                        existing_urls[article_url] = None  # Mark as will be created
                    
                    # ‚ö†Ô∏è QUAN TR·ªåNG: M·ªçi article t·∫°o ra t·ª´ crawl home ph·∫£i c√≥ section='home'
                    # Kh√¥ng detect section t·ª´ URL n·ªØa, t·∫•t c·∫£ ƒë·ªÅu l√† section='home'
                    article_section = 'home'
                    
                    # T·∫°o article m·ªõi cho home page
                    new_article = Article(
                        element_guid=article_data.get('element_guid'),
                        title=article_data.get('title', 'Untitled'),  # Slider c√≥ th·ªÉ kh√¥ng c√≥ title
                        slug=article_data.get('slug', ''),
                        published_url=article_data.get('url', ''),
                        k5a_url=article_data.get('k5a_url', ''),
                        language=article_language,  # Set language
                        original_language=article_language,  # Set original_language
                        section=article_section,  # ‚ö†Ô∏è Detect section t·ª´ URL cho 1_article, 2_articles, 3_articles
                        site_alias=article_data.get('site_alias', 'sermitsiaq'),
                        instance=article_data.get('instance', ''),
                        published_date=article_data.get('published_date'),
                        is_paywall=article_data.get('is_paywall', False),
                        paywall_class=article_data.get('paywall_class', ''),
                        image_data=image_data,  # ƒê√£ ƒë∆∞·ª£c download v√† c·∫≠p nh·∫≠t
                        display_order=display_order,  # S·ª≠ d·ª•ng display_order t·ª´ parser
                        is_home=True,  # QUAN TR·ªåNG: ƒê√°nh d·∫•u thu·ªôc home
                        layout_type=layout_type,  # Layout type t·ª´ parser
                        layout_data=layout_data,  # Layout data v·ªõi th√¥ng tin row
                        grid_size=article_data.get('grid_size', 6),  # Grid size t·ª´ HTML (5, 6, 7, 8, etc.)
                    )
                    db.session.add(new_article)
                    
                    # Debug: Log slider info
                    if article_data.get('layout_type') == 'slider':
                        layout_data = article_data.get('layout_data', {})
                        slider_articles = layout_data.get('slider_articles', [])
                        slider_title = layout_data.get('slider_title', 'Untitled')
                        print(f"  üé† Saving slider '{slider_title}': {len(slider_articles)} articles")
                        if len(slider_articles) < 4:
                            print(f"     ‚ö†Ô∏è  WARNING: Slider has only {len(slider_articles)} articles")
                    
                    # ‚ö†Ô∏è CRITICAL: Wrap commit trong try-except ƒë·ªÉ catch IntegrityError (race condition)
                    try:
                        # Commit m·ªói 10 articles ƒë·ªÉ tr√°nh timeout
                        if (articles_created + articles_updated + 1) % 10 == 0:
                            db.session.commit()
                            print(f"  üíæ Saved {articles_created + 1} new articles, updated {articles_updated} existing...")
                        
                        articles_created += 1
                        if article_url:
                            existing_urls[article_url] = new_article  # Add to dict to avoid duplicates in same batch
                    except Exception as commit_error:
                        # IntegrityError ho·∫∑c unique constraint violation (race condition)
                        db.session.rollback()
                        error_msg_str = str(commit_error)
                        if 'unique' in error_msg_str.lower() or 'duplicate' in error_msg_str.lower():
                            print(f"  ‚è≠Ô∏è  Article already exists (duplicate detected during commit), skipping...")
                            articles_skipped += 1
                            if article_url:
                                existing_urls[article_url] = None  # Mark as processed
                        else:
                            # Re-raise n·∫øu kh√¥ng ph·∫£i duplicate error
                            raise
                
                except Exception as e:
                    error_msg = f"Error saving article {article_data.get('element_guid', 'unknown')}: {str(e)}"
                    errors.append(error_msg)
                    print(f"  ‚ö†Ô∏è  {error_msg}")
                    db.session.rollback()
                    continue
            
            # Final commit cho articles m·ªõi
            db.session.commit()
            print(f"‚úÖ Successfully saved {articles_created} new home articles")
            
            # Sau khi save ƒë·∫ßy ƒë·ªß articles m·ªõi, m·ªõi ch·∫°y logic update cho articles ƒë√£ t·ªìn t·∫°i
            print(f"\nüîÑ Updating existing articles...")
            for update_info in articles_to_update:
                try:
                    existing_article = update_info['article']
                    article_data = update_info['article_data']
                    display_order = update_info['display_order']
                    layout_type = article_data.get('layout_type', '')
                    
                    if update_info['type'] == 'slider':
                        # Update slider container
                        existing_article.display_order = display_order
                        existing_article.layout_type = layout_type
                        layout_data = article_data.get('layout_data', {})
                        layout_data['row_index'] = article_data.get('row_index', -1)
                        layout_data['article_index_in_row'] = article_data.get('article_index_in_row', -1)
                        layout_data['total_rows'] = article_data.get('total_rows', 0)
                        existing_article.layout_data = layout_data
                        existing_article.grid_size = article_data.get('grid_size', 6)
                        existing_article.is_home = True
                        existing_article.section = 'home'
                        
                        if existing_article.id not in updated_article_ids:
                            updated_article_ids.add(existing_article.id)
                            articles_updated += 1
                            slider_title = article_data.get('layout_data', {}).get('slider_title', 'Untitled')
                            print(f"  üîÑ Updated slider container: {layout_type} '{slider_title}' (display_order={display_order})")
                    else:
                        # Update article c√≥ URL
                        existing_article.display_order = display_order
                        existing_article.layout_type = layout_type
                        
                        # Merge layout_data v·ªõi th√¥ng tin row
                        layout_data = article_data.get('layout_data', {})
                        layout_data['row_index'] = article_data.get('row_index', -1)
                        layout_data['article_index_in_row'] = article_data.get('article_index_in_row', -1)
                        layout_data['total_rows'] = article_data.get('total_rows', 0)
                        existing_article.layout_data = layout_data
                        
                        existing_article.grid_size = article_data.get('grid_size', 6)
                        existing_article.is_home = True
                        # ‚ö†Ô∏è KH√îNG ƒë·ªïi section g·ªëc c·ªßa existing articles - gi·ªØ nguy√™n section ban ƒë·∫ßu
                        
                        if existing_article.id not in updated_article_ids:
                            updated_article_ids.add(existing_article.id)
                            articles_updated += 1
                            if articles_updated % 10 == 0:
                                print(f"  üîÑ Updated display_order for {articles_updated} existing home articles...")
                except Exception as e:
                    error_msg = f"Error updating article {update_info.get('key', 'unknown')}: {str(e)}"
                    errors.append(error_msg)
                    print(f"  ‚ö†Ô∏è  {error_msg}")
                    continue
            
            # Commit t·∫•t c·∫£ updates
            if articles_to_update:
                db.session.commit()
                print(f"‚úÖ Updated {articles_updated} existing articles")
            
            print(f"‚úÖ Successfully saved {articles_created} new home articles, updated {articles_updated} existing articles (display_order)")
            if articles_not_found_in_home > 0:
                print(f"   ‚ö†Ô∏è  {articles_not_found_in_home} articles not found in database (should have been created)")
            print(f"   üìä Summary: {articles_crawled} crawled, {articles_created} created, {articles_updated} updated, {articles_skipped} skipped")
            
            # Debug: Show skipped articles info
            if skipped_articles_info:
                print(f"   üìã Skipped articles details ({len(skipped_articles_info)}):")
                for skip_info in skipped_articles_info:
                    print(f"      - {skip_info['reason']}: layout_type={skip_info['layout_type']}, display_order={skip_info['display_order']}, title={skip_info['title'][:50]}, url={skip_info.get('url', 'N/A')}")
            
            if articles_crawled != (articles_created + articles_updated + articles_skipped):
                missing = articles_crawled - (articles_created + articles_updated + articles_skipped)
                print(f"   ‚ö†Ô∏è  WARNING: {missing} articles were not processed (crawled={articles_crawled}, processed={articles_created + articles_updated + articles_skipped})")
                print(f"   üîç This might indicate articles that were crawled but not saved/updated/skipped properly")
            
            # Update crawl log
            crawl_log.status = 'success' if not errors else 'partial'
            crawl_log.articles_crawled = articles_crawled
            crawl_log.articles_created = articles_created
            crawl_log.articles_updated = articles_updated
            crawl_log.completed_at = datetime.utcnow()
            if errors:
                crawl_log.errors = '\n'.join(errors[:10])
            db.session.commit()
            
            print(f"‚úÖ Home crawl completed!")
            print(f"   üìä Articles crawled: {articles_crawled}")
            print(f"   ‚ûï Articles created: {articles_created}")
            print(f"   üîÑ Articles updated (display_order): {articles_updated}")
            if errors:
                print(f"   ‚ö†Ô∏è  Errors: {len(errors)}")
            
            return {
                'success': True,
                'articles_crawled': articles_crawled,
                'articles_created': articles_created,
                'articles_updated': articles_updated,
                'errors': errors
            }
        
        except Exception as e:
            error_msg = f"Home crawl failed: {str(e)}"
            print(f"‚ùå {error_msg}")
            errors.append(error_msg)
            
            # Update crawl log
            crawl_log.status = 'failed'
            crawl_log.errors = '\n'.join(errors)
            crawl_log.completed_at = datetime.utcnow()
            db.session.commit()
            
            return {
                'success': False,
                'articles_crawled': articles_crawled,
                'articles_created': articles_created,
                'articles_updated': 0,
                'errors': errors
            }


def crawl_erhverv_section(headless=True, max_articles=50):
    """
    Helper function ƒë·ªÉ crawl section erhverv
    
    Args:
        headless: Ch·∫°y browser ·ªü ch·∫ø ƒë·ªô headless
        max_articles: S·ªë l∆∞·ª£ng articles t·ªëi ƒëa
    
    Returns:
        dict: K·∫øt qu·∫£ crawl
    """
    crawler = SermitsiaqCrawler()
    result = crawler.crawl_section(
        section_url='https://www.sermitsiaq.ag/tag/erhverv',
        section_name='erhverv',
        max_articles=max_articles,
        headless=headless
    )
    return result


if __name__ == '__main__':
    # Test crawl
    from app import app
    
    with app.app_context():
        result = crawl_erhverv_section(headless=True, max_articles=50)
        print("\nüìä Final Result:")
        print(result)

