"""
Crawl service s·ª≠ d·ª•ng SeleniumBase ƒë·ªÉ crawl articles t·ª´ sermitsiaq.ag
"""
import sys
from pathlib import Path

# Add parent directory to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from seleniumbase import SB
from services.article_parser import parse_articles_from_html
from database import Article, CrawlLog, db
from datetime import datetime
import time


class SermitsiaqCrawler:
    """Crawler cho sermitsiaq.ag"""
    
    def __init__(self, base_url='https://www.sermitsiaq.ag', language='da'):
        """
        Initialize crawler
        
        Args:
            base_url: Base URL c·ªßa website (default: https://www.sermitsiaq.ag)
            language: Language code ('da' for Danish, 'kl' for Greenlandic)
        """
        self.base_url = base_url
        self.language = language
    
    def crawl_section(self, section_url, section_name='erhverv', max_articles=50, scroll_pause=2, headless=True, language=None):
        """
        Crawl articles t·ª´ m·ªôt section
        
        Args:
            section_url: URL c·ªßa section (v√≠ d·ª•: https://www.sermitsiaq.ag/tag/erhverv)
            section_name: T√™n section (erhverv, samfund, kultur, sport, job)
            max_articles: S·ªë l∆∞·ª£ng articles t·ªëi ƒëa c·∫ßn crawl
            scroll_pause: Th·ªùi gian ch·ªù gi·ªØa c√°c l·∫ßn scroll (gi√¢y)
        
        Returns:
            dict: K·∫øt qu·∫£ crawl
        """
        crawl_log = CrawlLog(
            crawl_type='section',
            section=section_name,
            status='running',
            started_at=datetime.utcnow()
        )
        db.session.add(crawl_log)
        db.session.commit()
        
        articles_crawled = 0
        articles_created = 0
        errors = []
        
        try:
            print(f"üì∞ Crawling section: {section_name}")
            print(f"üîó URL: {section_url}")
            
            # S·ª≠ d·ª•ng SeleniumBase v·ªõi context manager
            with SB(uc=True, headless=headless) as sb:
                # Navigate to section page
                sb.open(section_url)
                time.sleep(3)  # Wait for page load
                
                # Scroll ƒë·ªÉ load th√™m articles (lazy loading)
                print("üìú Scrolling to load articles...")
                scroll_count = 0
                max_scrolls = 10  # T·ªëi ƒëa scroll 10 l·∫ßn
                
                while scroll_count < max_scrolls and articles_crawled < max_articles:
                    # Scroll down
                    sb.scroll_to_bottom()
                    time.sleep(scroll_pause)
                    
                    # Get current page HTML
                    html_content = sb.get_page_source()
                    articles = parse_articles_from_html(html_content, self.base_url, is_home=False)
                    
                    if len(articles) >= max_articles:
                        break
                    
                    scroll_count += 1
                    print(f"  Scroll {scroll_count}: Found {len(articles)} articles")
                
                # Get final HTML
                html_content = sb.get_page_source()
                articles = parse_articles_from_html(html_content, self.base_url, is_home=False)
            
            # Limit to max_articles
            articles = articles[:max_articles]
            articles_crawled = len(articles)
            
            print(f"‚úÖ Crawled {articles_crawled} articles")
            
            # QUAN TR·ªåNG: X√≥a articles c≈© c·ªßa section C√ôNG LANGUAGE tr∆∞·ªõc khi l∆∞u articles m·ªõi
            # Determine language from base_url or parameter
            article_language = language or self.language
            print(f"üóëÔ∏è  Removing old {article_language} articles from section '{section_name}'...")
            old_articles_count = Article.query.filter_by(
                section=section_name,
                language=article_language
            ).count()
            if old_articles_count > 0:
                deleted_count = Article.query.filter_by(
                    section=section_name,
                    language=article_language
                ).delete()
                db.session.commit()
                print(f"   ‚úÖ Deleted {deleted_count} old {article_language} articles")
            else:
                print(f"   ‚ÑπÔ∏è  No old {article_language} articles to delete")
            
            # Save new articles to database
            print("üíæ Saving new articles to database...")
            for idx, article_data in enumerate(articles):
                try:
                    # QUAN TR·ªåNG: Override section t·ª´ article_data v·ªõi section_name ƒëang crawl
                    # V√¨ parser c√≥ th·ªÉ l·∫•y section t·ª´ HTML (c√≥ th·ªÉ kh√¥ng ƒë√∫ng)
                    article_data['section'] = section_name
                    
                    # Determine language from base_url or parameter
                    article_language = language or self.language
                    
                    # T·∫°o article m·ªõi v·ªõi ID m·ªõi
                    new_article = Article(
                        element_guid=article_data.get('element_guid'),  # C√≥ th·ªÉ None, kh√¥ng unique
                        title=article_data['title'],
                        slug=article_data['slug'],
                        published_url=article_data['url'],
                        k5a_url=article_data['k5a_url'],
                        section=section_name,
                        site_alias=article_data.get('site_alias', 'sermitsiaq'),
                        instance=article_data.get('instance', ''),
                        published_date=article_data.get('published_date'),
                        is_paywall=article_data['is_paywall'],
                        paywall_class=article_data['paywall_class'],
                        image_data=article_data.get('image_data', {}),
                        display_order=idx,  # Set display_order ƒë·ªÉ match pattern
                        language=article_language,  # Set language
                        original_language=article_language,  # Set original_language
                    )
                    db.session.add(new_article)
                    articles_created += 1
                    
                    # Commit m·ªói 10 articles ƒë·ªÉ tr√°nh timeout
                    if articles_created % 10 == 0:
                        db.session.commit()
                        print(f"  üíæ Saved {articles_created}/{articles_crawled} articles...")
                
                except Exception as e:
                    error_msg = f"Error saving article {article_data.get('element_guid', 'unknown')}: {str(e)}"
                    errors.append(error_msg)
                    print(f"  ‚ö†Ô∏è  {error_msg}")
                    continue
            
            # Final commit
            db.session.commit()
            print(f"‚úÖ Successfully saved {articles_created} new articles (replaced {old_articles_count} old articles)")
            
            # Update crawl log
            crawl_log.status = 'success' if not errors else 'partial'
            crawl_log.articles_crawled = articles_crawled
            crawl_log.articles_created = articles_created
            crawl_log.articles_updated = 0  # Kh√¥ng c√≤n update, ch·ªâ create m·ªõi v·ªõi ID m·ªõi
            crawl_log.completed_at = datetime.utcnow()
            if errors:
                crawl_log.errors = '\n'.join(errors[:10])  # Limit to first 10 errors
            db.session.commit()
            
            print(f"‚úÖ Crawl completed!")
            print(f"   üìä Articles crawled: {articles_crawled}")
            print(f"   ‚ûï Articles created: {articles_created} (with new IDs)")
            if errors:
                print(f"   ‚ö†Ô∏è  Errors: {len(errors)}")
            
            return {
                'success': True,
                'articles_crawled': articles_crawled,
                'articles_created': articles_created,
                'articles_updated': 0,
                'errors': errors
            }
        
        except Exception as e:
            error_msg = f"Crawl failed: {str(e)}"
            print(f"‚ùå {error_msg}")
            errors.append(error_msg)
            
            # Update crawl log
            crawl_log.status = 'failed'
            crawl_log.errors = '\n'.join(errors)
            crawl_log.completed_at = datetime.utcnow()
            db.session.commit()
            
            return {
                'success': False,
                'articles_crawled': articles_crawled,
                'articles_created': articles_created,
                'articles_updated': 0,
                'errors': errors
            }
    
    def crawl_home(self, home_url=None, max_articles=100, scroll_pause=2, headless=True, language=None):
        """
        Crawl articles t·ª´ trang home
        
        Args:
            home_url: URL c·ªßa trang home
            max_articles: S·ªë l∆∞·ª£ng articles t·ªëi ƒëa c·∫ßn crawl
            scroll_pause: Th·ªùi gian ch·ªù gi·ªØa c√°c l·∫ßn scroll (gi√¢y)
            headless: Ch·∫°y browser ·ªü ch·∫ø ƒë·ªô headless
        
        Returns:
            dict: K·∫øt qu·∫£ crawl
        """
        crawl_log = CrawlLog(
            crawl_type='home',
            section='home',
            status='running',
            started_at=datetime.utcnow()
        )
        db.session.add(crawl_log)
        db.session.commit()
        
        articles_crawled = 0
        articles_created = 0
        errors = []
        
        try:
            print(f"üè† Crawling home page: {home_url}")
            
            # S·ª≠ d·ª•ng SeleniumBase v·ªõi context manager
            with SB(uc=True, headless=headless) as sb:
                # Navigate to home page
                sb.open(home_url)
                time.sleep(3)  # Wait for page load
                
                # Scroll ƒë·ªÉ load th√™m articles (lazy loading)
                print("üìú Scrolling to load articles...")
                scroll_count = 0
                # TƒÉng max_scrolls n·∫øu crawl all (max_articles=0)
                max_scrolls = 100 if max_articles == 0 else 50  # TƒÉng s·ªë l·∫ßn scroll ƒë·ªÉ load t·∫•t c·∫£ articles
                previous_count = 0
                no_new_articles_count = 0
                
                # Crawl t·∫•t c·∫£ articles n·∫øu max_articles = 0 ho·∫∑c r·∫•t l·ªõn
                crawl_all = max_articles == 0 or max_articles >= 1000
                
                while scroll_count < max_scrolls:
                    # Scroll down
                    sb.scroll_to_bottom()
                    time.sleep(scroll_pause)
                    
                    # Get current page HTML
                    html_content = sb.get_page_source()
                    articles = parse_articles_from_html(html_content, self.base_url, is_home=True)
                    current_count = len(articles)
                    
                    # Ki·ªÉm tra xem c√≥ articles m·ªõi kh√¥ng
                    if current_count == previous_count:
                        no_new_articles_count += 1
                        # N·∫øu 3 l·∫ßn scroll li√™n ti·∫øp kh√¥ng c√≥ articles m·ªõi, d·ª´ng l·∫°i
                        if no_new_articles_count >= 3:
                            print(f"  ‚èπÔ∏è  No new articles found after {no_new_articles_count} scrolls. Stopping.")
                            break
                    else:
                        no_new_articles_count = 0
                    
                    previous_count = current_count
                    scroll_count += 1
                    print(f"  Scroll {scroll_count}: Found {current_count} articles")
                    
                    # N·∫øu kh√¥ng crawl all v√† ƒë√£ ƒë·ªß s·ªë l∆∞·ª£ng, d·ª´ng l·∫°i
                    if not crawl_all and current_count >= max_articles:
                        print(f"  ‚úÖ Reached max articles limit ({max_articles})")
                        break
                
                # Get final HTML
                html_content = sb.get_page_source()
                articles = parse_articles_from_html(html_content, self.base_url, is_home=True)
            
            # Limit to max_articles n·∫øu kh√¥ng crawl all
            if not crawl_all and max_articles > 0:
                articles = articles[:max_articles]
            articles_crawled = len(articles)
            
            print(f"‚úÖ Crawled {articles_crawled} articles from home page")
            
            # QUAN TR·ªåNG: X√≥a articles c≈© c·ªßa home C√ôNG LANGUAGE tr∆∞·ªõc khi l∆∞u articles m·ªõi
            # Determine language from base_url or parameter
            article_language = language or self.language
            print(f"üóëÔ∏è  Removing old {article_language} articles from home...")
            old_articles_count = Article.query.filter_by(
                section='home', 
                is_home=True,
                language=article_language
            ).count()
            if old_articles_count > 0:
                deleted_count = Article.query.filter_by(
                    section='home', 
                    is_home=True,
                    language=article_language
                ).delete()
                db.session.commit()
                print(f"   ‚úÖ Deleted {deleted_count} old {article_language} home articles")
            else:
                print(f"   ‚ÑπÔ∏è  No old {article_language} home articles to delete")
            
            # Save new articles to database
            print("üíæ Saving new home articles to database...")
            for idx, article_data in enumerate(articles):
                try:
                    # Set section='home' v√† is_home=True
                    article_data['section'] = 'home'
                    
                    # S·ª≠ d·ª•ng display_order t·ª´ parser n·∫øu c√≥, n·∫øu kh√¥ng th√¨ d√πng idx
                    display_order = article_data.get('display_order', idx)
                    
                    # Determine language from base_url or parameter
                    article_language = language or self.language
                    
                    # T·∫°o article m·ªõi v·ªõi ID m·ªõi
                    new_article = Article(
                        element_guid=article_data.get('element_guid'),
                        title=article_data.get('title', 'Untitled'),  # Slider c√≥ th·ªÉ kh√¥ng c√≥ title
                        slug=article_data.get('slug', ''),
                        published_url=article_data.get('url', ''),
                        k5a_url=article_data.get('k5a_url', ''),
                        language=article_language,  # Set language
                        original_language=article_language,  # Set original_language
                        section='home',  # Section = 'home'
                        site_alias=article_data.get('site_alias', 'sermitsiaq'),
                        instance=article_data.get('instance', ''),
                        published_date=article_data.get('published_date'),
                        is_paywall=article_data.get('is_paywall', False),
                        paywall_class=article_data.get('paywall_class', ''),
                        image_data=article_data.get('image_data', {}),
                        display_order=display_order,  # S·ª≠ d·ª•ng display_order t·ª´ parser
                        is_home=True,  # ƒê√°nh d·∫•u thu·ªôc home
                        layout_type=article_data.get('layout_type'),  # Layout type t·ª´ parser
                        layout_data=article_data.get('layout_data', {}),  # Layout data n·∫øu c√≥
                        grid_size=article_data.get('grid_size', 6),  # Grid size t·ª´ HTML (5, 6, 7, 8, etc.)
                    )
                    db.session.add(new_article)
                    articles_created += 1
                    
                    # Debug: Log slider info
                    if article_data.get('layout_type') == 'slider':
                        layout_data = article_data.get('layout_data', {})
                        slider_articles = layout_data.get('slider_articles', [])
                        slider_title = layout_data.get('slider_title', 'Untitled')
                        print(f"  üé† Saving slider '{slider_title}': {len(slider_articles)} articles")
                        if len(slider_articles) < 4:
                            print(f"     ‚ö†Ô∏è  WARNING: Slider has only {len(slider_articles)} articles")
                    
                    # Commit m·ªói 10 articles ƒë·ªÉ tr√°nh timeout
                    if articles_created % 10 == 0:
                        db.session.commit()
                        print(f"  üíæ Saved {articles_created}/{articles_crawled} articles...")
                
                except Exception as e:
                    error_msg = f"Error saving article {article_data.get('element_guid', 'unknown')}: {str(e)}"
                    errors.append(error_msg)
                    print(f"  ‚ö†Ô∏è  {error_msg}")
                    continue
            
            # Final commit
            db.session.commit()
            print(f"‚úÖ Successfully saved {articles_created} new home articles (replaced {old_articles_count} old articles)")
            
            # Update crawl log
            crawl_log.status = 'success' if not errors else 'partial'
            crawl_log.articles_crawled = articles_crawled
            crawl_log.articles_created = articles_created
            crawl_log.articles_updated = 0
            crawl_log.completed_at = datetime.utcnow()
            if errors:
                crawl_log.errors = '\n'.join(errors[:10])
            db.session.commit()
            
            print(f"‚úÖ Home crawl completed!")
            print(f"   üìä Articles crawled: {articles_crawled}")
            print(f"   ‚ûï Articles created: {articles_created}")
            if errors:
                print(f"   ‚ö†Ô∏è  Errors: {len(errors)}")
            
            return {
                'success': True,
                'articles_crawled': articles_crawled,
                'articles_created': articles_created,
                'articles_updated': 0,
                'errors': errors
            }
        
        except Exception as e:
            error_msg = f"Home crawl failed: {str(e)}"
            print(f"‚ùå {error_msg}")
            errors.append(error_msg)
            
            # Update crawl log
            crawl_log.status = 'failed'
            crawl_log.errors = '\n'.join(errors)
            crawl_log.completed_at = datetime.utcnow()
            db.session.commit()
            
            return {
                'success': False,
                'articles_crawled': articles_crawled,
                'articles_created': articles_created,
                'articles_updated': 0,
                'errors': errors
            }


def crawl_erhverv_section(headless=True, max_articles=50):
    """
    Helper function ƒë·ªÉ crawl section erhverv
    
    Args:
        headless: Ch·∫°y browser ·ªü ch·∫ø ƒë·ªô headless
        max_articles: S·ªë l∆∞·ª£ng articles t·ªëi ƒëa
    
    Returns:
        dict: K·∫øt qu·∫£ crawl
    """
    crawler = SermitsiaqCrawler()
    result = crawler.crawl_section(
        section_url='https://www.sermitsiaq.ag/tag/erhverv',
        section_name='erhverv',
        max_articles=max_articles,
        headless=headless
    )
    return result


if __name__ == '__main__':
    # Test crawl
    from app import app
    
    with app.app_context():
        result = crawl_erhverv_section(headless=True, max_articles=50)
        print("\nüìä Final Result:")
        print(result)

