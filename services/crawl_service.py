"""
Crawl service s·ª≠ d·ª•ng SeleniumBase ƒë·ªÉ crawl articles t·ª´ sermitsiaq.ag
"""
import sys
from pathlib import Path

# Add parent directory to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from seleniumbase import SB
from services.article_parser import parse_articles_from_html
from database import Article, CrawlLog, db
from datetime import datetime
import time


class SermitsiaqCrawler:
    """Crawler cho sermitsiaq.ag"""
    
    def __init__(self, base_url='https://www.sermitsiaq.ag'):
        self.base_url = base_url
    
    def crawl_section(self, section_url, section_name='erhverv', max_articles=50, scroll_pause=2, headless=True):
        """
        Crawl articles t·ª´ m·ªôt section
        
        Args:
            section_url: URL c·ªßa section (v√≠ d·ª•: https://www.sermitsiaq.ag/tag/erhverv)
            section_name: T√™n section (erhverv, samfund, kultur, sport, job)
            max_articles: S·ªë l∆∞·ª£ng articles t·ªëi ƒëa c·∫ßn crawl
            scroll_pause: Th·ªùi gian ch·ªù gi·ªØa c√°c l·∫ßn scroll (gi√¢y)
        
        Returns:
            dict: K·∫øt qu·∫£ crawl
        """
        crawl_log = CrawlLog(
            crawl_type='section',
            section=section_name,
            status='running',
            started_at=datetime.utcnow()
        )
        db.session.add(crawl_log)
        db.session.commit()
        
        articles_crawled = 0
        articles_created = 0
        errors = []
        
        try:
            print(f"üì∞ Crawling section: {section_name}")
            print(f"üîó URL: {section_url}")
            
            # S·ª≠ d·ª•ng SeleniumBase v·ªõi context manager
            with SB(uc=True, headless=headless) as sb:
                # Navigate to section page
                sb.open(section_url)
                time.sleep(3)  # Wait for page load
                
                # Scroll ƒë·ªÉ load th√™m articles (lazy loading)
                print("üìú Scrolling to load articles...")
                scroll_count = 0
                max_scrolls = 10  # T·ªëi ƒëa scroll 10 l·∫ßn
                
                while scroll_count < max_scrolls and articles_crawled < max_articles:
                    # Scroll down
                    sb.scroll_to_bottom()
                    time.sleep(scroll_pause)
                    
                    # Get current page HTML
                    html_content = sb.get_page_source()
                    articles = parse_articles_from_html(html_content, self.base_url)
                    
                    if len(articles) >= max_articles:
                        break
                    
                    scroll_count += 1
                    print(f"  Scroll {scroll_count}: Found {len(articles)} articles")
                
                # Get final HTML
                html_content = sb.get_page_source()
                articles = parse_articles_from_html(html_content, self.base_url)
            
            # Limit to max_articles
            articles = articles[:max_articles]
            articles_crawled = len(articles)
            
            print(f"‚úÖ Crawled {articles_crawled} articles")
            
            # Save to database
            print("üíæ Saving articles to database...")
            for idx, article_data in enumerate(articles):
                try:
                    # QUAN TR·ªåNG: Override section t·ª´ article_data v·ªõi section_name ƒëang crawl
                    # V√¨ parser c√≥ th·ªÉ l·∫•y section t·ª´ HTML (c√≥ th·ªÉ kh√¥ng ƒë√∫ng)
                    article_data['section'] = section_name
                    
                    # D√πng ID (primary key) l√†m unique identifier
                    # M·ªói l·∫ßn crawl s·∫Ω t·∫°o articles m·ªõi v·ªõi ID m·ªõi
                    # Cho ph√©p c√πng element_guid xu·∫•t hi·ªán ·ªü nhi·ªÅu sections v·ªõi ID kh√°c nhau
                    new_article = Article(
                        element_guid=article_data.get('element_guid'),  # C√≥ th·ªÉ None, kh√¥ng unique
                        title=article_data['title'],
                        slug=article_data['slug'],
                        published_url=article_data['url'],
                        k5a_url=article_data['k5a_url'],
                        section=section_name,
                        site_alias=article_data.get('site_alias', 'sermitsiaq'),
                        instance=article_data.get('instance', ''),
                        published_date=article_data.get('published_date'),
                        is_paywall=article_data['is_paywall'],
                        paywall_class=article_data['paywall_class'],
                        image_data=article_data.get('image_data', {}),
                        display_order=idx,  # Set display_order ƒë·ªÉ match pattern
                    )
                    db.session.add(new_article)
                    articles_created += 1
                    
                    # Commit m·ªói 10 articles ƒë·ªÉ tr√°nh timeout
                    if articles_created % 10 == 0:
                        db.session.commit()
                        print(f"  üíæ Saved {articles_created} articles...")
                
                except Exception as e:
                    error_msg = f"Error saving article {article_data.get('element_guid', 'unknown')}: {str(e)}"
                    errors.append(error_msg)
                    print(f"  ‚ö†Ô∏è  {error_msg}")
                    continue
            
            # Final commit
            db.session.commit()
            
            # Update crawl log
            crawl_log.status = 'success' if not errors else 'partial'
            crawl_log.articles_crawled = articles_crawled
            crawl_log.articles_created = articles_created
            crawl_log.articles_updated = 0  # Kh√¥ng c√≤n update, ch·ªâ create m·ªõi v·ªõi ID m·ªõi
            crawl_log.completed_at = datetime.utcnow()
            if errors:
                crawl_log.errors = '\n'.join(errors[:10])  # Limit to first 10 errors
            db.session.commit()
            
            print(f"‚úÖ Crawl completed!")
            print(f"   üìä Articles crawled: {articles_crawled}")
            print(f"   ‚ûï Articles created: {articles_created} (with new IDs)")
            if errors:
                print(f"   ‚ö†Ô∏è  Errors: {len(errors)}")
            
            return {
                'success': True,
                'articles_crawled': articles_crawled,
                'articles_created': articles_created,
                'articles_updated': 0,
                'errors': errors
            }
        
        except Exception as e:
            error_msg = f"Crawl failed: {str(e)}"
            print(f"‚ùå {error_msg}")
            errors.append(error_msg)
            
            # Update crawl log
            crawl_log.status = 'failed'
            crawl_log.errors = '\n'.join(errors)
            crawl_log.completed_at = datetime.utcnow()
            db.session.commit()
            
            return {
                'success': False,
                'articles_crawled': articles_crawled,
                'articles_created': articles_created,
                'articles_updated': 0,
                'errors': errors
            }


def crawl_erhverv_section(headless=True, max_articles=50):
    """
    Helper function ƒë·ªÉ crawl section erhverv
    
    Args:
        headless: Ch·∫°y browser ·ªü ch·∫ø ƒë·ªô headless
        max_articles: S·ªë l∆∞·ª£ng articles t·ªëi ƒëa
    
    Returns:
        dict: K·∫øt qu·∫£ crawl
    """
    crawler = SermitsiaqCrawler()
    result = crawler.crawl_section(
        section_url='https://www.sermitsiaq.ag/tag/erhverv',
        section_name='erhverv',
        max_articles=max_articles,
        headless=headless
    )
    return result


if __name__ == '__main__':
    # Test crawl
    from app import app
    
    with app.app_context():
        result = crawl_erhverv_section(headless=True, max_articles=50)
        print("\nüìä Final Result:")
        print(result)

