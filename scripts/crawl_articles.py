#!/usr/bin/env python3
"""
Script Ä‘á»ƒ crawl articles tá»« sermitsiaq.ag
Usage: python3 scripts/crawl_articles.py [section] [--max-articles N] [--headless]
"""
import sys
import os
from pathlib import Path

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from app import app
from services.crawl_service import SermitsiaqCrawler, crawl_erhverv_section
import argparse


def main():
    parser = argparse.ArgumentParser(description='Crawl articles from sermitsiaq.ag')
    parser.add_argument('section', nargs='?', default='all', 
                       help='Section to crawl (erhverv, samfund, kultur, sport, job, home, all). Default: all')
    parser.add_argument('--max-articles', type=int, default=50,
                       help='Maximum number of articles to crawl per section (default: 50). Use 0 or >= 1000 to crawl all articles (for home page)')
    parser.add_argument('--headless', action='store_true', default=True,
                       help='Run browser in headless mode (default: True)')
    parser.add_argument('--no-headless', dest='headless', action='store_false',
                       help='Run browser in visible mode')
    parser.add_argument('--url', type=str, default=None,
                       help='Custom URL to crawl (overrides section)')
    parser.add_argument('--delay', type=int, default=5,
                       help='Delay between sections when crawling all (seconds, default: 5)')
    
    args = parser.parse_args()
    
    # Section URLs
    section_urls = {
        'erhverv': 'https://www.sermitsiaq.ag/tag/erhverv',
        'samfund': 'https://www.sermitsiaq.ag/tag/samfund',
        'kultur': 'https://www.sermitsiaq.ag/tag/kultur',
        'sport': 'https://www.sermitsiaq.ag/tag/sport',
        'job': 'https://www.sermitsiaq.ag/tag/job',
        'home': 'https://www.sermitsiaq.ag',  # Home page
    }
    
    # Determine sections to crawl
    if args.url:
        # Custom URL - crawl single section
        sections_to_crawl = [(args.section, args.url)]
    elif args.section == 'all':
        # Crawl all sections (including home)
        sections_to_crawl = [(name, url) for name, url in section_urls.items()]
    elif args.section == 'home':
        # Crawl home page only
        sections_to_crawl = [('home', section_urls['home'])]
    elif args.section in section_urls:
        # Single section
        sections_to_crawl = [(args.section, section_urls[args.section])]
    else:
        print(f"âŒ Unknown section: {args.section}")
        print(f"   Available sections: {', '.join(section_urls.keys())}, all")
        sys.exit(1)
    
    print("=" * 60)
    print("ğŸ•·ï¸  Sermitsiaq Article Crawler")
    print("=" * 60)
    print(f"ğŸ“° Sections to crawl: {len(sections_to_crawl)}")
    if len(sections_to_crawl) > 1:
        print(f"   Sections: {', '.join([s[0] for s in sections_to_crawl])}")
    else:
        print(f"   Section: {sections_to_crawl[0][0]}")
    
    # Default: home page crawl táº¥t cáº£ articles
    if len(sections_to_crawl) == 1 and sections_to_crawl[0][0] == 'home':
        print(f"ğŸ“Š Max articles: ALL (default for home page)")
    else:
        print(f"ğŸ“Š Max articles per section: {args.max_articles}")
    
    print(f"ğŸ‘ï¸  Headless: {args.headless}")
    if len(sections_to_crawl) > 1:
        print(f"â±ï¸  Delay between sections: {args.delay}s")
    print("=" * 60)
    print()
    
    # Run crawl
    crawler = SermitsiaqCrawler()
    
    all_results = []
    total_articles_crawled = 0
    total_articles_created = 0
    total_articles_updated = 0
    all_errors = []
    
    try:
        for idx, (section_name, section_url) in enumerate(sections_to_crawl, 1):
            if len(sections_to_crawl) > 1:
                print(f"\n{'=' * 60}")
                print(f"ğŸ“° Crawling section {idx}/{len(sections_to_crawl)}: {section_name}")
                print(f"{'=' * 60}")
            
            # Use crawl_home for home page, crawl_section for others
            if section_name == 'home':
                # Default: crawl táº¥t cáº£ articles tá»« home (max_articles = 0)
                # Chá»‰ dÃ¹ng max_articles tá»« args náº¿u user chá»‰ Ä‘á»‹nh rÃµ rÃ ng (khÃ´ng pháº£i default 50)
                # Kiá»ƒm tra xem cÃ³ pháº£i default value khÃ´ng báº±ng cÃ¡ch check sys.argv
                import sys
                user_specified_max = '--max-articles' in ' '.join(sys.argv)
                
                if user_specified_max:
                    # User Ä‘Ã£ chá»‰ Ä‘á»‹nh max-articles
                    home_max_articles = args.max_articles if args.max_articles > 0 else 0
                    if home_max_articles == 0:
                        print(f"ğŸ“° Crawling ALL articles from home page (no limit)")
                    else:
                        print(f"ğŸ“° Crawling up to {home_max_articles} articles from home page")
                else:
                    # Default: crawl táº¥t cáº£
                    home_max_articles = 0
                    print(f"ğŸ“° Crawling ALL articles from home page (default: no limit)")
                
                result = crawler.crawl_home(
                    home_url=section_url,
                    max_articles=home_max_articles,
                    headless=args.headless
                )
            else:
                result = crawler.crawl_section(
                    section_url=section_url,
                    section_name=section_name,
                    max_articles=args.max_articles,
                    headless=args.headless
                )
            
            all_results.append({
                'section': section_name,
                'result': result
            })
            
            total_articles_crawled += result['articles_crawled']
            total_articles_created += result['articles_created']
            total_articles_updated += result['articles_updated']
            all_errors.extend(result['errors'])
            
            # Print section result
            print(f"\nâœ… Section '{section_name}' completed:")
            print(f"   ğŸ“° Articles crawled: {result['articles_crawled']}")
            print(f"   â• Articles created: {result['articles_created']}")
            print(f"   ğŸ”„ Articles updated: {result['articles_updated']}")
            if result['errors']:
                print(f"   âš ï¸  Errors: {len(result['errors'])}")
            
            # Delay between sections (except for last one)
            if idx < len(sections_to_crawl) and args.delay > 0:
                print(f"\nâ±ï¸  Waiting {args.delay}s before next section...")
                import time
                time.sleep(args.delay)
        
        # Print summary
        print()
        print("=" * 60)
        print("ğŸ“Š Crawl Summary:")
        print("=" * 60)
        print(f"ğŸ“° Total sections crawled: {len(sections_to_crawl)}")
        print(f"ğŸ“° Total articles crawled: {total_articles_crawled}")
        print(f"â• Total articles created: {total_articles_created}")
        print(f"ğŸ”„ Total articles updated: {total_articles_updated}")
        
        if all_errors:
            print(f"âš ï¸  Total errors: {len(all_errors)}")
            print("\n   First 10 errors:")
            for error in all_errors[:10]:
                print(f"   - {error}")
        
        print("\nğŸ“‹ Results by section:")
        for item in all_results:
            section = item['section']
            result = item['result']
            status = "âœ…" if result['success'] else "âŒ"
            print(f"   {status} {section}: {result['articles_created']} created, {result['articles_updated']} updated")
        
        print("=" * 60)
        
        # Exit with success if at least one section succeeded
        if any(r['result']['success'] for r in all_results):
            sys.exit(0)
        else:
            sys.exit(1)
    
    except KeyboardInterrupt:
        print("\nâš ï¸  Crawl interrupted by user")
        print(f"   Completed {len(all_results)}/{len(sections_to_crawl)} sections")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ Crawl failed: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    with app.app_context():
        main()

