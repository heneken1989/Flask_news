#!/usr/bin/env python3
"""
Script m·ªõi ƒë·ªÉ crawl layout structure c·ªßa home page
Ch·ªâ l·∫•y metadata: published_url, layout_type, display_order
Kh√¥ng t·∫°o articles m·ªõi, ch·ªâ l∆∞u layout structure ƒë·ªÉ link v·ªõi articles ƒë√£ c√≥

Flow:
1. Crawl home page ‚Üí parse layout structure
2. L∆∞u layout structure v√†o JSON ho·∫∑c return dict
3. Script kh√°c s·∫Ω d√πng layout n√†y ƒë·ªÉ link v·ªõi articles ƒë√£ c√≥ trong DB
"""

import sys
import os
import json
import csv
import argparse
from pathlib import Path
from datetime import datetime
from contextlib import contextmanager
import shutil

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from seleniumbase import SB
from services.article_parser import parse_articles_from_html, parse_article_element
from services.image_downloader import download_and_update_image_data
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
import time
import re

# Import app v√† database khi c·∫ßn (s·∫Ω import trong function khi c·∫ßn app context)

# Import database sau khi c√≥ app context


def get_chrome_options_for_headless():
    """
    Tr·∫£ v·ªÅ Chrome options c·∫ßn thi·∫øt cho Linux headless server
    C·∫ßn thi·∫øt khi ch·∫°y v·ªõi root ho·∫∑c kh√¥ng c√≥ display
    """
    # --no-sandbox: B·ªè qua sandbox (c·∫ßn thi·∫øt khi ch·∫°y v·ªõi root)
    # --disable-dev-shm-usage: Tr√°nh l·ªói shared memory tr√™n VPS
    # --disable-gpu: T·∫Øt GPU (kh√¥ng c·∫ßn tr√™n server)
    return "no-sandbox,disable-dev-shm-usage,disable-gpu"


def extract_section_from_url(url):
    """
    Extract section t·ª´ URL
    
    Args:
        url: Article URL (v√≠ d·ª•: https://www.sermitsiaq.ag/kultur/debut-ep-fra-max-5-tassa/2331684)
    
    Returns:
        str: Section name ('kultur', 'samfund', 'erhverv', 'sport', 'podcasti') ho·∫∑c 'home' n·∫øu kh√¥ng match
    """
    if not url:
        return 'home'
    
    # Parse URL ƒë·ªÉ l·∫•y path
    parsed = urlparse(url)
    path = parsed.path.strip('/')
    
    # Valid sections
    valid_sections = ['kultur', 'samfund', 'erhverv', 'sport', 'podcasti']
    
    # Extract section t·ª´ path (ph·∫ßn ƒë·∫ßu ti√™n sau domain)
    # V√≠ d·ª•: /kultur/debut-ep-fra-max-5-tassa/2331684 ‚Üí 'kultur'
    path_parts = path.split('/')
    if path_parts and path_parts[0] in valid_sections:
        return path_parts[0]
    
    # N·∫øu kh√¥ng match ‚Üí return 'home'
    return 'home'


def parse_job_article_manual(soup, article_url, article_title):
    """
    Parse job article t·ª´ sjob.gl manually - ch·ªâ l·∫•y image, title v√† URL
    (V√¨ URL s·∫Ω link sang trang kh√°c, kh√¥ng c·∫ßn parse to√†n b·ªô detail)
    
    Args:
        soup: BeautifulSoup object c·ªßa article page
        article_url: URL c·ªßa article
        article_title: Title t·ª´ layout (fallback)
    
    Returns:
        dict: Article data v·ªõi image, title, url ho·∫∑c None
    """
    try:
        # Extract title
        title = article_title
        title_elem = soup.find('h1') or soup.find('h2', class_='headline') or soup.find('h2')
        if title_elem:
            title = title_elem.get_text(strip=True)
        
        # Extract image (n·∫øu c√≥) - t√¨m image trong article ho·∫∑c header
        image_data = {}
        img_elem = soup.find('img')
        if img_elem:
            img_src = img_elem.get('src', '')
            if img_src:
                if not img_src.startswith('http'):
                    img_src = urljoin(article_url, img_src)
                image_data = {
                    'fallback': img_src,
                    'desktop_webp': img_src,
                    'desktop_jpeg': img_src
                }
        
        # Extract slug t·ª´ URL (ƒë∆°n gi·∫£n)
        slug = ''
        url_parts = article_url.rstrip('/').split('/')
        if len(url_parts) >= 2:
            slug = url_parts[-2]  # L·∫•y ph·∫ßn tr∆∞·ªõc s·ªë ID
        
        # Extract instance t·ª´ URL (s·ªë ID cu·ªëi)
        instance = ''
        if url_parts:
            instance = url_parts[-1] if url_parts[-1].isdigit() else ''
        
        # Extract element_guid (n·∫øu c√≥)
        element_guid = ''
        guid_elem = soup.find(attrs={'data-element-guid': True})
        if guid_elem:
            element_guid = guid_elem.get('data-element-guid', '')
        
        # Ch·ªâ c·∫ßn image, title v√† URL - ƒë∆°n gi·∫£n h√≥a
        return {
            'element_guid': element_guid,
            'title': title,
            'slug': slug,
            'url': article_url,
            'k5a_url': article_url,  # Job articles th∆∞·ªùng kh√¥ng c√≥ k5a_url ri√™ng
            'site_alias': 'sjob',
            'instance': instance,
            'published_date': None,  # Kh√¥ng c·∫ßn date cho job articles
            'is_paywall': False,
            'paywall_class': '',
            'image_data': image_data
        }
    except Exception as e:
        print(f"      ‚ö†Ô∏è  Error parsing job article manually: {e}")
        import traceback
        traceback.print_exc()
        return None


def crawl_home_layout(home_url='https://www.sermitsiaq.ag', language='da', 
                      max_articles=0, scroll_pause=2, headless=True):
    """
    Crawl layout structure c·ªßa home page
    
    Args:
        home_url: URL c·ªßa trang home
        language: Language code ('da', 'kl', 'en')
        max_articles: S·ªë l∆∞·ª£ng articles t·ªëi ƒëa (0 = t·∫•t c·∫£)
        scroll_pause: Th·ªùi gian ch·ªù gi·ªØa c√°c l·∫ßn scroll
        headless: Ch·∫°y browser ·ªü ch·∫ø ƒë·ªô headless
    
    Returns:
        list: List of layout items, m·ªói item c√≥:
            - published_url: URL c·ªßa article
            - layout_type: Lo·∫°i layout (1_full, 2_articles, slider, etc.)
            - display_order: Th·ª© t·ª± hi·ªÉn th·ªã
            - row_index: Index c·ªßa row
            - article_index_in_row: Index trong row
            - total_rows: T·ªïng s·ªë rows
            - grid_size: Grid size (6, 4, 12, etc.)
            - layout_data: Layout data (cho slider)
    """
    print(f"\n{'='*60}")
    print(f"üè† Crawling home layout structure")
    print(f"{'='*60}")
    print(f"   URL: {home_url}")
    print(f"   Language: {language}")
    print(f"   Max articles: {max_articles if max_articles > 0 else 'All'}")
    print(f"   Headless: {headless}")
    
    layout_items = []
    
    # Chrome options cho Linux headless server
    chrome_opts = get_chrome_options_for_headless()
    
    try:
        with SB(uc=True, headless=headless, chromium_arg=chrome_opts) as sb:
            # Navigate to home page
            print(f"\nüì° Opening {home_url}...")
            sb.open(home_url)
            time.sleep(3)
            
            # Scroll ƒë·ªÉ load th√™m articles
            print("üìú Scrolling to load articles...")
            scroll_count = 0
            max_scrolls = 100 if max_articles == 0 else 50
            previous_count = 0
            no_new_articles_count = 0
            
            while scroll_count < max_scrolls:
                sb.scroll_to_bottom()
                time.sleep(scroll_pause)
                scroll_count += 1
                
                # Check s·ªë l∆∞·ª£ng articles hi·ªán t·∫°i
                current_count = len(sb.find_elements('article[data-element-guid]'))
                
                if current_count == previous_count:
                    no_new_articles_count += 1
                    if no_new_articles_count >= 3:
                        print(f"   ‚úÖ No new articles after {no_new_articles_count} scrolls, stopping...")
                        break
                else:
                    no_new_articles_count = 0
                    print(f"   üìä Found {current_count} articles after {scroll_count} scrolls...")
                
                previous_count = current_count
                
                # N·∫øu ƒë√£ ƒë·ªß s·ªë l∆∞·ª£ng articles c·∫ßn thi·∫øt
                if max_articles > 0 and current_count >= max_articles:
                    print(f"   ‚úÖ Reached {max_articles} articles, stopping scroll...")
                    break
            
            # Get HTML content
            print(f"\nüìÑ Extracting HTML content...")
            html_content = sb.get_page_source()
            
            # Parse articles t·ª´ HTML (ch·ªâ l·∫•y layout structure)
            print(f"üîç Parsing layout structure...")
            articles = parse_articles_from_html(html_content, base_url=home_url, is_home=True)
            
            print(f"‚úÖ Parsed {len(articles)} layout items")
            
            # Convert articles th√†nh layout items (ch·ªâ l·∫•y metadata c·∫ßn thi·∫øt)
            for article_data in articles:
                layout_type = article_data.get('layout_type', '')
                layout_data = article_data.get('layout_data', {})
                
                layout_item = {
                    'published_url': article_data.get('url', ''),
                    'layout_type': layout_type,
                    'display_order': article_data.get('display_order', 0),
                    'row_index': article_data.get('row_index', -1),
                    'article_index_in_row': article_data.get('article_index_in_row', -1),
                    'total_rows': article_data.get('total_rows', 0),
                    'grid_size': article_data.get('grid_size', 6),
                    'layout_data': layout_data,
                    'element_guid': article_data.get('element_guid', ''),
                    'k5a_url': article_data.get('k5a_url', ''),
                    'title': article_data.get('title', ''),  # L∆∞u title ƒë·ªÉ d√πng trong CSV
                }
                
                # V·ªõi slider containers, l∆∞u th√™m th√¥ng tin slider
                if layout_type in ['slider', 'job_slider']:
                    layout_item['slider_title'] = layout_data.get('slider_title', '')
                    layout_item['slider_articles'] = []
                    # L∆∞u published_url c·ªßa c√°c articles trong slider
                    for slider_article in layout_data.get('slider_articles', []):
                        if slider_article.get('url'):
                            layout_item['slider_articles'].append({
                                'published_url': slider_article.get('url'),
                                'title': slider_article.get('title', ''),
                                'image_data': slider_article.get('image_data', {})
                            })
                
                # V·ªõi 1_with_list_left/right, ƒë·∫£m b·∫£o list_items ƒë∆∞·ª£c l∆∞u
                elif layout_type in ['1_with_list_left', '1_with_list_right']:
                    list_items = layout_data.get('list_items', [])
                    list_title = layout_data.get('list_title', '')
                    
                    # Log ƒë·ªÉ debug
                    if list_items:
                        print(f"   üìã Found {len(list_items)} list items for {layout_type} (title: {list_title})")
                    else:
                        print(f"   ‚ö†Ô∏è  No list items found for {layout_type} - layout_data: {list(layout_data.keys())}")
                    
                    # ƒê·∫£m b·∫£o list_items ƒë∆∞·ª£c l∆∞u trong layout_data
                    layout_item['list_title'] = list_title
                    layout_item['list_items'] = list_items
                    # C·∫≠p nh·∫≠t l·∫°i layout_data ƒë·ªÉ ƒë·∫£m b·∫£o c√≥ list_items
                    layout_item['layout_data']['list_items'] = list_items
                    layout_item['layout_data']['list_title'] = list_title
                
                layout_items.append(layout_item)
            
            print(f"\n‚úÖ Successfully extracted {len(layout_items)} layout items")
            
            # Print summary
            print(f"\nüìä Layout Summary:")
            layout_types = {}
            for item in layout_items:
                layout_type = item['layout_type']
                layout_types[layout_type] = layout_types.get(layout_type, 0) + 1
            
            for layout_type, count in sorted(layout_types.items()):
                print(f"   - {layout_type}: {count} items")
            
            # Crawl v√† t·∫°o articles t·ª´ home page
            # ‚ö†Ô∏è L∆ØU √ù: 
            # - Crawl t·∫•t c·∫£ articles (1_full, 2_articles, 3_articles, etc.) v√† set is_home=True, section='home'
            # - list_items trong 1_with_list_left/right ch·ªâ l√† links, kh√¥ng c·∫ßn crawl
            # - job_slider: Ch·ªâ t·∫°o container, items t·ª´ sjob.gl kh√¥ng c·∫ßn crawl (l∆∞u trong layout_data)
            print(f"\n{'='*60}")
            print(f"üï∑Ô∏è  Crawling articles from home page")
            print(f"{'='*60}")
            print(f"   ‚ÑπÔ∏è  Note: list_items trong 1_with_list_left/right ch·ªâ l√† links, kh√¥ng c·∫ßn crawl")
            
            # Collect t·∫•t c·∫£ articles c·∫ßn crawl
            articles_to_crawl = []
            
            # 1. Collect c√°c articles th√¥ng th∆∞·ªùng (1_full, 2_articles, 3_articles, 5_articles, etc.)
            # ‚ö†Ô∏è L∆ØU √ù: B·ªè qua c√°c articles kh√¥ng c√≥ URL (ch·ªâ l√† label nh∆∞ "NYHEDER", "Se alle jobs")
            regular_layout_types = ['1_full', '2_articles', '3_articles', '5_articles', '1_with_list_left', '1_with_list_right']
            for item in layout_items:
                layout_type = item.get('layout_type', '')
                published_url = item.get('published_url', '')
                
                # Skip n·∫øu kh√¥ng c√≥ URL (ch·ªâ l√† label)
                if not published_url or not published_url.strip():
                    if layout_type in regular_layout_types:
                        print(f"   ‚è≠Ô∏è  Skipping {layout_type} without URL (label only): {item.get('title', 'N/A')[:50]}")
                    continue
                
                if layout_type in regular_layout_types:
                    # Resolve relative URL
                    if not published_url.startswith('http'):
                        published_url = urljoin(home_url, published_url)
                    
                    if published_url not in [a['url'] for a in articles_to_crawl]:
                        articles_to_crawl.append({
                            'url': published_url,
                            'title': item.get('title', ''),
                            'source': 'home_article',
                            'layout_type': layout_type,
                            'display_order': item.get('display_order', 0),
                            'element_guid': item.get('element_guid', ''),
                            'k5a_url': item.get('k5a_url', '')
                        })
            
            # 2. Log job_slider (ch·ªâ t·∫°o container, kh√¥ng crawl individual items)
            for item in layout_items:
                layout_type = item.get('layout_type', '')
                
                if layout_type == 'job_slider':
                    slider_articles = item.get('slider_articles', [])
                    slider_title = item.get('slider_title', '')
                    print(f"   üíº Found job_slider with {len(slider_articles)} articles (title: {slider_title}) - Container only, items from sjob.gl kh√¥ng c·∫ßn crawl")
            
            # 3. Log list_items (kh√¥ng crawl, ch·ªâ l√† links)
            for item in layout_items:
                layout_type = item.get('layout_type', '')
                if layout_type in ['1_with_list_left', '1_with_list_right']:
                    list_items = item.get('list_items', []) or item.get('layout_data', {}).get('list_items', [])
                    list_title = item.get('list_title', '') or item.get('layout_data', {}).get('list_title', '')
                    print(f"   üìã {layout_type} has {len(list_items)} list items (title: {list_title}) - URLs ƒë√£ l∆∞u trong layout_data, kh√¥ng c·∫ßn crawl")
            
            print(f"\n   üìä Summary:")
            print(f"      - Regular articles to crawl: {len([a for a in articles_to_crawl if a['source'] == 'home_article'])} articles")
            print(f"      - Job slider: Container only (items t·ª´ sjob.gl kh√¥ng c·∫ßn crawl)")
            print(f"      - List items (1_with_list_left/right): Ch·ªâ l√† links, kh√¥ng c·∫ßn crawl")
            
            # Crawl t·∫•t c·∫£ articles
            if articles_to_crawl:
                # Import app v√† database khi c·∫ßn
                from app import app
                from database import db, Article
                
                with app.app_context():
                    articles_created = 0
                    articles_skipped = 0
                    articles_updated = 0
                    
                    # ‚ö†Ô∏è CRITICAL: Track URLs ƒë√£ crawled trong session n√†y ƒë·ªÉ tr√°nh duplicate
                    crawled_urls_in_session = set()
                    
                    for idx, article_info in enumerate(articles_to_crawl, 1):
                        article_url = article_info['url']
                        article_title = article_info['title']
                        source = article_info['source']
                        layout_type = article_info.get('layout_type', '')
                        
                        print(f"\n   [{idx}/{len(articles_to_crawl)}] Crawling: {article_title[:50] if article_title else 'N/A'}...")
                        print(f"      URL: {article_url[:80]}...")
                        print(f"      Source: {source}, Layout: {layout_type}")
                        
                        # ‚ö†Ô∏è CRITICAL: Check xem URL ƒë√£ crawled trong session n√†y ch∆∞a
                        if article_url in crawled_urls_in_session:
                            print(f"      ‚è≠Ô∏è  URL already crawled in this session, skipping...")
                            articles_skipped += 1
                            continue
                        
                        # Check if article already exists in database
                        # D√πng db.session.expire_all() ƒë·ªÉ refresh query v√† tr√°nh cache c≈©
                        db.session.expire_all()
                        existing = Article.query.filter_by(
                            published_url=article_url,
                            language=language
                        ).first()
                        
                        if existing:
                            # ‚ö†Ô∏è QUAN TR·ªåNG: N·∫øu article ƒë√£ c√≥ ·ªü c√°c tag kh√°c (section != 'home'), skip
                            # V√¨ article ƒë√£ ƒë∆∞·ª£c crawl t·ª´ section ƒë√≥ r·ªìi, kh√¥ng c·∫ßn crawl l·∫°i t·ª´ home
                            if existing.section and existing.section != 'home':
                                print(f"      ‚è≠Ô∏è  Article already exists in section '{existing.section}' (ID: {existing.id}), skipping...")
                                print(f"         ‚ÑπÔ∏è  Article ƒë√£ c√≥ ·ªü tag kh√°c, kh√¥ng c·∫ßn crawl l·∫°i t·ª´ home")
                                articles_skipped += 1
                                continue
                            
                            # Update existing article: set is_home=True, section='home'
                            # ‚ö†Ô∏è KH√îNG set is_temp=True khi update (ch·ªâ set khi t·∫°o m·ªõi)
                            if not existing.is_home or existing.section != 'home':
                                existing.is_home = True
                                existing.section = 'home'
                                if article_info.get('display_order'):
                                    existing.display_order = article_info.get('display_order')
                                if layout_type:
                                    existing.layout_type = layout_type
                                db.session.commit()
                                articles_updated += 1
                                print(f"      ‚úÖ Updated existing article (ID: {existing.id}): is_home=True, section='home'")
                            else:
                                print(f"      ‚è≠Ô∏è  Article already exists and is home (ID: {existing.id}), skipping...")
                            articles_skipped += 1
                            # Mark URL as crawled trong session
                            crawled_urls_in_session.add(article_url)
                            continue
                        
                        try:
                            # Navigate to article page
                            sb.open(article_url)
                            time.sleep(2)
                            
                            # Get HTML content
                            html_content = sb.get_page_source()
                            
                            # Parse article element t·ª´ HTML
                            soup = BeautifulSoup(html_content, 'html.parser')
                            
                            # T√¨m article element - th·ª≠ nhi·ªÅu c√°ch
                            article_elem = None
                            
                            # C√°ch 1: T√¨m <article> tag
                            article_elem = soup.find('article')
                            
                            # C√°ch 2: T√¨m b·∫±ng data-element-guid
                            if not article_elem:
                                article_elem = soup.find('div', attrs={'data-element-guid': True})
                            
                            # C√°ch 3: T√¨m b·∫±ng class ho·∫∑c id th∆∞·ªùng d√πng
                            if not article_elem:
                                article_elem = soup.find('div', class_=lambda x: x and ('article' in x.lower() or 'content' in x.lower()))
                            
                            # C√°ch 4: T√¨m main content area
                            if not article_elem:
                                article_elem = soup.find('main') or soup.find('div', id='content') or soup.find('div', id='main')
                            
                            # Parse article data
                            article_data = None
                            
                            if article_elem:
                                # Try parse v·ªõi parse_article_element (cho sermitsiaq.ag)
                                try:
                                    article_data = parse_article_element(article_elem, base_url=home_url)
                                except Exception as e:
                                    print(f"      ‚ö†Ô∏è  Error parsing with parse_article_element: {e}")
                                    article_data = None
                            
                            # N·∫øu kh√¥ng parse ƒë∆∞·ª£c, th·ª≠ parse th·ªß c√¥ng
                            if not article_data:
                                # Check xem c√≥ ph·∫£i job article t·ª´ sjob.gl kh√¥ng
                                if 'sjob.gl' in article_url:
                                    print(f"      ‚ÑπÔ∏è  Job article from sjob.gl, parsing manually...")
                                    article_data = parse_job_article_manual(soup, article_url, article_title)
                                else:
                                    # Th·ª≠ parse th·ªß c√¥ng cho sermitsiaq.ag articles
                                    print(f"      ‚ÑπÔ∏è  Trying manual parsing...")
                                    try:
                                        # Extract basic info t·ª´ HTML
                                        title = article_title
                                        title_elem = soup.find('h1') or soup.find('h2', class_='headline') or soup.find('h2')
                                        if title_elem:
                                            title = title_elem.get_text(strip=True)
                                        
                                        # Extract image - t√¨m trong picture tag v·ªõi source tags
                                        image_data = {}
                                        
                                        # C√°ch 1: T√¨m <picture> tag (∆∞u ti√™n nh·∫•t - c√≥ nhi·ªÅu formats)
                                        picture_elem = soup.find('picture')
                                        if picture_elem:
                                            # L·∫•y c√°c source tags
                                            sources = picture_elem.find_all('source')
                                            
                                            # T√¨m desktop_webp (media="(min-width: 768px)" type="image/webp")
                                            desktop_webp = None
                                            desktop_jpeg = None
                                            mobile_webp = None
                                            mobile_jpeg = None
                                            
                                            for source in sources:
                                                srcset = source.get('srcset', '')
                                                media = source.get('media', '')
                                                source_type = source.get('type', '')
                                                
                                                if not srcset:
                                                    continue
                                                
                                                # Extract URL t·ª´ srcset (c√≥ th·ªÉ c√≥ nhi·ªÅu URLs, l·∫•y ƒë·∫ßu ti√™n)
                                                srcset_url = srcset.split(',')[0].strip().split()[0] if srcset else ''
                                                
                                                # Ph√¢n lo·∫°i theo media v√† type
                                                if '(min-width: 768px)' in media:
                                                    if 'image/webp' in source_type:
                                                        desktop_webp = srcset_url
                                                    elif 'image/jpeg' in source_type or 'image/jpg' in source_type:
                                                        desktop_jpeg = srcset_url
                                                elif '(max-width: 767px)' in media:
                                                    if 'image/webp' in source_type:
                                                        mobile_webp = srcset_url
                                                    elif 'image/jpeg' in source_type or 'image/jpg' in source_type:
                                                        mobile_jpeg = srcset_url
                                            
                                            # L·∫•y fallback t·ª´ img tag trong picture
                                            img_in_picture = picture_elem.find('img')
                                            fallback = None
                                            if img_in_picture:
                                                fallback = img_in_picture.get('src', '')
                                            
                                            # T·∫°o image_data t·ª´ picture sources
                                            if desktop_webp or desktop_jpeg or mobile_webp or mobile_jpeg or fallback:
                                                image_data = {
                                                    'desktop_webp': desktop_webp or fallback or desktop_jpeg,
                                                    'desktop_jpeg': desktop_jpeg or fallback or desktop_webp,
                                                    'mobile_webp': mobile_webp or fallback or mobile_jpeg,
                                                    'mobile_jpeg': mobile_jpeg or fallback or mobile_webp,
                                                    'fallback': fallback or desktop_webp or desktop_jpeg or mobile_webp or mobile_jpeg
                                                }
                                                
                                                # Resolve relative URLs
                                                for key in image_data:
                                                    if image_data[key] and not image_data[key].startswith('http'):
                                                        image_data[key] = urljoin(article_url, image_data[key])
                                                
                                                print(f"         ‚ÑπÔ∏è  Found image from <picture> tag:")
                                                if desktop_webp:
                                                    print(f"            desktop_webp: {desktop_webp[:80]}...")
                                                if fallback:
                                                    print(f"            fallback: {fallback[:80]}...")
                                        
                                        # C√°ch 2: N·∫øu kh√¥ng c√≥ picture, t√¨m img trong figure ho·∫∑c article
                                        if not image_data:
                                            figure_elem = soup.find('figure')
                                            if figure_elem:
                                                img_elem = figure_elem.find('img')
                                            else:
                                                # T√¨m img trong article ho·∫∑c content
                                                article_elem = soup.find('article') or soup.find('div', class_='content')
                                                if article_elem:
                                                    img_elem = article_elem.find('img')
                                                else:
                                                    img_elem = soup.find('img')
                                            
                                            if img_elem:
                                                # ∆Øu ti√™n data-src, data-lazy-src, r·ªìi m·ªõi ƒë·∫øn src
                                                img_src = (img_elem.get('data-src') or 
                                                          img_elem.get('data-lazy-src') or 
                                                          img_elem.get('src') or 
                                                          img_elem.get('data-original', ''))
                                                
                                                # B·ªè qua logo.svg v√† c√°c image kh√¥ng ph·∫£i article image
                                                if img_src and 'logo.svg' not in img_src and 'image.sermitsiaq.ag' in img_src:
                                                    # Resolve relative URL
                                                    if not img_src.startswith('http'):
                                                        img_src = urljoin(article_url, img_src)
                                                    
                                                    # T·∫°o image_data v·ªõi t·∫•t c·∫£ formats
                                                    image_data = {
                                                        'fallback': img_src,
                                                        'desktop_webp': img_src,
                                                        'desktop_jpeg': img_src,
                                                        'mobile_webp': img_src,
                                                        'mobile_jpeg': img_src
                                                    }
                                                    
                                                    print(f"         ‚ÑπÔ∏è  Found image from <img> tag: {img_src[:80]}...")
                                                elif img_src and 'logo.svg' in img_src:
                                                    print(f"         ‚ö†Ô∏è  Skipping logo.svg image")
                                        
                                        # Extract slug v√† instance t·ª´ URL
                                        url_parts = article_url.rstrip('/').split('/')
                                        slug = url_parts[-2] if len(url_parts) >= 2 else ''
                                        instance = url_parts[-1] if url_parts and url_parts[-1].isdigit() else ''
                                        
                                        # Extract element_guid
                                        element_guid = ''
                                        guid_elem = soup.find(attrs={'data-element-guid': True})
                                        if guid_elem:
                                            element_guid = guid_elem.get('data-element-guid', '')
                                        
                                        # Extract published_date t·ª´ <time itemprop="datePublished" datetime="...">
                                        published_date = None
                                        time_elem = soup.find('time', attrs={'itemprop': 'datePublished'})
                                        if time_elem:
                                            datetime_attr = time_elem.get('datetime', '')
                                            if datetime_attr:
                                                try:
                                                    # Parse ISO format: 2025-11-08T11:32:36+01:00
                                                    # Thay Z th√†nh +00:00 n·∫øu c√≥
                                                    datetime_str = datetime_attr.replace('Z', '+00:00')
                                                    published_date = datetime.fromisoformat(datetime_str)
                                                    print(f"         ‚úÖ Extracted published_date: {published_date}")
                                                except Exception as e:
                                                    print(f"         ‚ö†Ô∏è  Could not parse datetime '{datetime_attr}': {e}")
                                                    # Th·ª≠ parse format kh√°c n·∫øu c·∫ßn
                                                    try:
                                                        # Th·ª≠ format: 2025-11-08T11:32:36
                                                        if 'T' in datetime_attr and '+' not in datetime_attr and 'Z' not in datetime_attr:
                                                            published_date = datetime.fromisoformat(datetime_attr)
                                                            print(f"         ‚úÖ Extracted published_date (no timezone): {published_date}")
                                                    except:
                                                        pass
                                        
                                        article_data = {
                                            'element_guid': element_guid,
                                            'title': title,
                                            'slug': slug,
                                            'url': article_url,
                                            'k5a_url': article_url,
                                            'site_alias': 'sermitsiaq',
                                            'instance': instance,
                                            'published_date': published_date,
                                            'is_paywall': False,
                                            'paywall_class': '',
                                            'image_data': image_data
                                        }
                                        print(f"      ‚úÖ Manual parsing successful")
                                    except Exception as e:
                                        print(f"      ‚ö†Ô∏è  Error in manual parsing: {e}")
                                        article_data = None
                                
                                if not article_data:
                                    print(f"      ‚ö†Ô∏è  Could not parse article data, skipping...")
                                    print(f"         ‚ÑπÔ∏è  URL: {article_url}")
                                    continue
                            
                            # Download image
                            image_data = article_data.get('image_data', {})
                            if image_data:
                                # Log image_data tr∆∞·ªõc khi download ƒë·ªÉ debug
                                print(f"      ‚ÑπÔ∏è  Image data before download:")
                                for key, value in image_data.items():
                                    if value:
                                        print(f"         {key}: {value[:100]}...")
                                
                                try:
                                    image_data = download_and_update_image_data(
                                        image_data,
                                        base_url='https://www.sermitsiaq.com',
                                        download_all_formats=False
                                    )
                                    
                                    # Log imageId n·∫øu extract ƒë∆∞·ª£c
                                    from services.image_downloader import extract_image_id_from_url
                                    image_id = None
                                    for key in ['desktop_webp', 'desktop_jpeg', 'mobile_webp', 'mobile_jpeg', 'fallback']:
                                        if image_data.get(key):
                                            image_id = extract_image_id_from_url(image_data[key])
                                            if image_id:
                                                print(f"      ‚úÖ Extracted imageId: {image_id} from {key}")
                                                break
                                    
                                    if not image_id:
                                        print(f"      ‚ö†Ô∏è  Could not extract imageId - checking image URLs:")
                                        for key, value in image_data.items():
                                            if value:
                                                print(f"         {key}: {value[:120]}")
                                except Exception as e:
                                    print(f"      ‚ö†Ô∏è  Error downloading image: {e}")
                                    import traceback
                                    traceback.print_exc()
                            
                            # Set section='home' v√† is_home=True cho t·∫•t c·∫£ articles t·ª´ home page
                            # ‚ö†Ô∏è QUAN TR·ªåNG: T·∫•t c·∫£ articles t·ª´ home page ƒë·ªÅu c√≥ section='home' v√† is_home=True
                            
                            # Create Article record
                            # ‚ö†Ô∏è QUAN TR·ªåNG: Set is_temp=True cho 1_article, 2_article, 3_article
                            # (c·∫ßn crawl detail tr∆∞·ªõc khi show tr√™n home)
                            layout_type_final = layout_type or source
                            is_temp_value = layout_type_final in ['1_article', '2_articles', '3_articles']
                            
                            # ‚ö†Ô∏è QUAN TR·ªåNG: V·ªõi c√°c layout types c√≥ published_url (articles th√¥ng th∆∞·ªùng), 
                            # detect section t·ª´ URL. C√°c lo·∫°i kh√°c (sliders, containers) gi·ªØ section='home'
                            # Layout types c·∫ßn detect section: 1_full, 1_article, 2_articles, 3_articles, 1_special_bg
                            article_layout_types = ['1_full', '1_article', '2_articles', '3_articles', '1_special_bg']
                            
                            if layout_type_final in article_layout_types:
                                article_section = extract_section_from_url(article_url)
                            else:
                                # Sliders, containers, 1_with_list_left, 1_with_list_right, 5_articles, etc. gi·ªØ section='home'
                                article_section = 'home'
                            
                            new_article = Article(
                                element_guid=article_data.get('element_guid', '') or article_info.get('element_guid', ''),
                                title=article_data.get('title', article_title),
                                slug=article_data.get('slug', ''),
                                published_url=article_url,
                                k5a_url=article_data.get('k5a_url', '') or article_info.get('k5a_url', ''),
                                section=article_section,  # ‚ö†Ô∏è Detect section t·ª´ URL cho 1_article, 2_articles, 3_articles
                                site_alias=article_data.get('site_alias', 'sermitsiaq'),
                                instance=article_data.get('instance', ''),
                                published_date=article_data.get('published_date'),
                                is_paywall=article_data.get('is_paywall', False),
                                paywall_class=article_data.get('paywall_class', ''),
                                image_data=image_data,
                                language=language,
                                original_language=language,
                                is_home=True,  # ‚ö†Ô∏è T·∫•t c·∫£ articles t·ª´ home page c√≥ is_home=True
                                layout_type=layout_type_final,
                                display_order=article_info.get('display_order', 0),
                                is_temp=is_temp_value  # ‚ö†Ô∏è Set is_temp=True cho 1_article, 2_article, 3_article
                            )
                            
                            db.session.add(new_article)
                            
                            # ‚ö†Ô∏è CRITICAL: Wrap commit trong try-except ƒë·ªÉ catch IntegrityError
                            # (race condition n·∫øu 2 processes t·∫°o c√πng article)
                            try:
                                db.session.commit()
                                articles_created += 1
                                
                                # Mark URL as crawled trong session
                                crawled_urls_in_session.add(article_url)
                                
                                print(f"      ‚úÖ Created article (ID: {new_article.id})")
                                
                                # Commit m·ªói 5 articles
                                if articles_created % 5 == 0:
                                    print(f"   üíæ Created {articles_created} articles so far...")
                            except Exception as commit_error:
                                # IntegrityError ho·∫∑c unique constraint violation
                                db.session.rollback()
                                error_msg = str(commit_error)
                                if 'unique' in error_msg.lower() or 'duplicate' in error_msg.lower():
                                    print(f"      ‚è≠Ô∏è  Article already exists (duplicate detected during commit), skipping...")
                                    articles_skipped += 1
                                    crawled_urls_in_session.add(article_url)
                                else:
                                    print(f"      ‚ö†Ô∏è  Error committing article: {commit_error}")
                                    raise  # Re-raise if not duplicate error
                        
                        except Exception as e:
                            print(f"      ‚ö†Ô∏è  Error crawling article: {e}")
                            import traceback
                            traceback.print_exc()
                            db.session.rollback()
                            # Mark URL as attempted (failed) ƒë·ªÉ tr√°nh retry trong c√πng session
                            crawled_urls_in_session.add(article_url)
                            continue
                    
                    print(f"\n‚úÖ Home articles crawl completed:")
                    print(f"   - Created: {articles_created}")
                    print(f"   - Updated: {articles_updated}")
                    print(f"   - Skipped (already exist): {articles_skipped}")
            
            return layout_items
            
    except Exception as e:
        print(f"‚ùå Error crawling home layout: {e}")
        import traceback
        traceback.print_exc()
        return []


def save_layout_to_file(layout_items, output_file=None, language='da'):
    """
    L∆∞u layout structure v√†o file JSON
    
    Args:
        layout_items: List of layout items
        output_file: Path to output file (n·∫øu None, t·ª± ƒë·ªông t·∫°o t√™n)
        language: Language code ƒë·ªÉ t·∫°o t√™n file
    """
    if output_file is None:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_file = f"home_layout_{language}_{timestamp}.json"
    
    output_path = Path(__file__).parent / 'home_layouts' / output_file
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    layout_data = {
        'language': language,
        'crawled_at': datetime.now().isoformat(),
        'total_items': len(layout_items),
        'layout_items': layout_items
    }
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(layout_data, f, indent=2, ensure_ascii=False)
    
    print(f"\nüíæ Saved layout to: {output_path}")
    return str(output_path)


def save_layout_to_csv(layout_items, output_file=None, language='da'):
    """
    L∆∞u layout structure v√†o file CSV
    
    Args:
        layout_items: List of layout items
        output_file: Path to output file (n·∫øu None, t·ª± ƒë·ªông t·∫°o t√™n)
        language: Language code ƒë·ªÉ t·∫°o t√™n file
    
    Returns:
        str: Path to CSV file
    """
    if output_file is None:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_file = f"home_layout_{language}_{timestamp}.csv"
    
    output_path = Path(__file__).parent / 'home_layouts' / output_file
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    # Group items theo row_index
    row_to_items = {}  # Map row_index -> list of items
    
    for item in layout_items:
        row_index = item.get('row_index', -1)
        if row_index not in row_to_items:
            row_to_items[row_index] = []
        row_to_items[row_index].append(item)
    
    # S·∫Øp x·∫øp c√°c row_index v√† ƒë√°nh s·ªë l·∫°i li√™n t·ª•c (b·ªè qua h√†ng tr·ªëng)
    sorted_row_indices = sorted([idx for idx in row_to_items.keys() if idx >= 0])
    
    # Write CSV
    with open(output_path, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        
        # Header
        writer.writerow([
            'H√†ng',
            'Th·ª© t·ª±',
            'D·∫°ng Layout',
            'URL',
            'Ti√™u ƒë·ªÅ',
            'Grid Size',
            'Ghi ch√∫'
        ])
        
        # Ghi ch·ªâ c√°c h√†ng c√≥ items, ƒë√°nh s·ªë l·∫°i li√™n t·ª•c t·ª´ 1
        display_row_number = 1
        for original_row_idx in sorted_row_indices:
            items = row_to_items[original_row_idx]
            
            for item in items:
                display_order = item.get('display_order', 0)
                layout_type = item.get('layout_type', '')
                published_url = item.get('published_url', '')
                grid_size = item.get('grid_size', '')
                
                # L·∫•y title t·ª´ layout_data ho·∫∑c slider_title
                title = item.get('title', '')  # L·∫•y title t·ª´ layout_item n·∫øu c√≥
                note = ''
                
                if layout_type in ['slider', 'job_slider']:
                    slider_title = item.get('slider_title', '')
                    if slider_title:
                        title = slider_title
                    slider_articles = item.get('slider_articles', [])
                    note = f"Slider v·ªõi {len(slider_articles)} articles"
                    
                    # Ghi slider container
                    writer.writerow([
                        display_row_number,  # ƒê√°nh s·ªë l·∫°i li√™n t·ª•c t·ª´ 1 (b·ªè qua h√†ng tr·ªëng)
                        display_order,
                        layout_type,
                        '',  # Slider container kh√¥ng c√≥ URL
                        title,
                        grid_size,
                        note
                    ])
                    
                    # Ghi t·ª´ng article trong slider (m·ªói article m·ªôt d√≤ng, c√πng s·ªë h√†ng)
                    for slider_article in slider_articles:
                        slider_article_url = slider_article.get('published_url', '')
                        slider_article_title = slider_article.get('title', '')
                        
                        writer.writerow([
                            display_row_number,  # C√πng s·ªë h√†ng v·ªõi slider container
                            '',  # Kh√¥ng c√≥ display_order ri√™ng cho t·ª´ng article trong slider
                            f'{layout_type}_item',  # ƒê√°nh d·∫•u l√† item trong slider
                            slider_article_url,
                            slider_article_title,
                            '',  # Kh√¥ng c√≥ grid_size ri√™ng
                            'Article trong slider'
                        ])
                elif layout_type in ['1_with_list_left', '1_with_list_right']:
                    # Article v·ªõi list
                    list_title = item.get('list_title', '') or item.get('layout_data', {}).get('list_title', '')
                    list_items = item.get('list_items', []) or item.get('layout_data', {}).get('list_items', [])
                    note = f"List: {list_title} ({len(list_items)} items)"
                    
                    # Ghi article ch√≠nh
                    writer.writerow([
                        display_row_number,
                        display_order,
                        layout_type,
                        published_url,
                        title,
                        grid_size,
                        note
                    ])
                    
                    # Ghi t·ª´ng item trong list (m·ªói item m·ªôt d√≤ng, c√πng s·ªë h√†ng)
                    for list_item in list_items:
                        list_item_url = list_item.get('url', '')
                        list_item_title = list_item.get('title', '')
                        
                        writer.writerow([
                            display_row_number,  # C√πng s·ªë h√†ng v·ªõi article ch√≠nh
                            '',  # Kh√¥ng c√≥ display_order ri√™ng
                            f'{layout_type}_list_item',  # ƒê√°nh d·∫•u l√† item trong list
                            list_item_url,
                            list_item_title,
                            '',  # Kh√¥ng c√≥ grid_size ri√™ng
                            f'Item trong list "{list_title}"'
                        ])
                else:
                    # Article th√¥ng th∆∞·ªùng (kh√¥ng ph·∫£i slider)
                    if not title and published_url:
                        # Fallback: L·∫•y slug t·ª´ URL n·∫øu kh√¥ng c√≥ title
                        try:
                            title = published_url.split('/')[-2] if published_url else ''
                        except:
                            title = ''
                    
                    writer.writerow([
                        display_row_number,  # ƒê√°nh s·ªë l·∫°i li√™n t·ª•c t·ª´ 1 (b·ªè qua h√†ng tr·ªëng)
                        display_order,
                        layout_type,
                        published_url,
                        title,
                        grid_size,
                        note
                    ])
            
            # TƒÉng s·ªë h√†ng sau khi ghi xong t·∫•t c·∫£ items trong h√†ng n√†y
            display_row_number += 1
    
    print(f"\nüíæ Saved layout CSV to: {output_path}")
    return str(output_path)


def main():
    parser = argparse.ArgumentParser(
        description='Crawl home page layout structure (ch·ªâ metadata, kh√¥ng t·∫°o articles)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Crawl DA home layout
  python scripts/crawl_home_layout.py --language da
  
  # Crawl KL home layout
  python scripts/crawl_home_layout.py --language kl --url https://kl.sermitsiaq.ag
  
  # Crawl v√† l∆∞u v√†o file
  python scripts/crawl_home_layout.py --language da --save
  
  # Crawl v·ªõi no-headless ƒë·ªÉ debug
  python scripts/crawl_home_layout.py --language da --no-headless
        """
    )
    
    parser.add_argument('--url', '-u', default='https://www.sermitsiaq.ag',
                       help='URL c·ªßa trang home (default: https://www.sermitsiaq.ag)')
    parser.add_argument('--language', '-l', default='da', choices=['da', 'kl', 'en'],
                       help='Language code (default: da)')
    parser.add_argument('--max-articles', '-n', type=int, default=0,
                       help='S·ªë l∆∞·ª£ng articles t·ªëi ƒëa (0 = t·∫•t c·∫£)')
    parser.add_argument('--scroll-pause', type=float, default=2.0,
                       help='Th·ªùi gian ch·ªù gi·ªØa c√°c l·∫ßn scroll (seconds)')
    parser.add_argument('--no-headless', action='store_true',
                       help='Ch·∫°y browser ·ªü ch·∫ø ƒë·ªô no-headless (ƒë·ªÉ debug)')
    parser.add_argument('--save', '-s', action='store_true',
                       help='L∆∞u layout v√†o file JSON')
    parser.add_argument('--csv', action='store_true',
                       help='L∆∞u layout v√†o file CSV')
    parser.add_argument('--output', '-o',
                       help='T√™n file output (n·∫øu kh√¥ng c√≥, t·ª± ƒë·ªông t·∫°o)')
    
    args = parser.parse_args()
    
    # Adjust URL based on language
    if args.language == 'kl':
        if 'kl.' not in args.url:
            args.url = args.url.replace('www.', 'kl.')
    elif args.language == 'en':
        # EN c√≥ th·ªÉ d√πng c√πng URL v·ªõi DA ho·∫∑c URL ri√™ng
        pass
    
    # Crawl layout
    layout_items = crawl_home_layout(
        home_url=args.url,
        language=args.language,
        max_articles=args.max_articles,
        scroll_pause=args.scroll_pause,
        headless=not args.no_headless
    )
    
    if not layout_items:
        print("‚ùå No layout items found")
        return
    
    # Save to file if requested
    if args.csv:
        csv_path = save_layout_to_csv(layout_items, args.output, args.language)
        print(f"\n‚úÖ CSV file saved: {csv_path}")
    
    if args.save or args.output:
        json_path = save_layout_to_file(layout_items, args.output, args.language)
        if not args.csv:
            print(f"\n‚úÖ JSON file saved: {json_path}")
    
    if not args.save and not args.csv and not args.output:
        # Print summary
        print(f"\nüìã Layout Items (first 10):")
        for i, item in enumerate(layout_items[:10], 1):
            print(f"   {i}. display_order={item['display_order']}, "
                  f"layout_type={item['layout_type']}, "
                  f"url={item['published_url'][:60] if item['published_url'] else 'N/A'}...")
        
        if len(layout_items) > 10:
            print(f"   ... and {len(layout_items) - 10} more items")
        
        print(f"\nüí° Tip: Use --save to save layout to file for later use")


if __name__ == '__main__':
    main()

