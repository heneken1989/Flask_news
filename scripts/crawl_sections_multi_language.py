#!/usr/bin/env python3
"""
Script ƒë·ªÉ crawl v√† translate articles t·ª´ c√°c sections (tags) v√† home page cho c·∫£ 3 ng√¥n ng·ªØ

Workflow cho m·ªói section:
1. Crawl DK t·ª´ https://www.sermitsiaq.ag/tag/{section} (ho·∫∑c /podcasti cho podcasti)
2. Crawl KL t·ª´ https://kl.sermitsiaq.ag/tag/{section} (ho·∫∑c /podcasti cho podcasti)
3. Match articles gi·ªØa DK v√† KL
4. Translate DK articles sang EN

Workflow cho home:
1. Crawl DK t·ª´ https://www.sermitsiaq.ag/
2. Crawl KL t·ª´ https://kl.sermitsiaq.ag/
3. Match articles gi·ªØa DK v√† KL
4. Translate DK articles sang EN

Sections: erhverv, samfund, kultur, sport, podcasti, home
Note: podcasti s·ª≠ d·ª•ng URL /podcasti thay v√¨ /tag/podcasti
"""

import sys
import os
from pathlib import Path

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from app import app
from database import db, Article
from services.crawl_service import SermitsiaqCrawler
from services.article_matcher import match_and_link_articles
from services.translation_service import translate_articles_batch
from scripts.translate_article_urls import translate_url
from scripts.crawl_article_details_batch import crawl_all as crawl_article_details
import argparse
import time
import subprocess

# Section URLs mapping
SECTION_URLS = {
    'erhverv': {
        'da': 'https://www.sermitsiaq.ag/tag/erhverv',
        'kl': 'https://kl.sermitsiaq.ag/tag/erhverv'
    },
    'samfund': {
        'da': 'https://www.sermitsiaq.ag/tag/samfund',
        'kl': 'https://kl.sermitsiaq.ag/tag/samfund'
    },
    'kultur': {
        'da': 'https://www.sermitsiaq.ag/tag/kultur',
        'kl': 'https://kl.sermitsiaq.ag/tag/kultur'
    },
    'sport': {
        'da': 'https://www.sermitsiaq.ag/tag/sport',
        'kl': 'https://kl.sermitsiaq.ag/tag/sport'
    },
    'podcasti': {
        'da': 'https://www.sermitsiaq.ag/podcasti',
        'kl': 'https://kl.sermitsiaq.ag/podcasti'
    }
}


def crawl_danish_section(section_name, max_articles=0):
    """Crawl Danish section"""
    print("\n" + "="*60)
    print(f"üá©üá∞ Crawling Danish (DK) section: {section_name}")
    print("="*60)
    
    if section_name not in SECTION_URLS:
        print(f"‚ùå Invalid section: {section_name}")
        return 0
    
    crawler = SermitsiaqCrawler(
        base_url='https://www.sermitsiaq.ag',
        language='da'
    )
    
    result = crawler.crawl_section(
        section_url=SECTION_URLS[section_name]['da'],
        section_name=section_name,
        max_articles=max_articles,
        headless=True,
        language='da'
    )
    
    if result['success']:
        print(f"‚úÖ Danish crawl completed: {result['articles_created']} articles")
        return result['articles_created']
    else:
        print(f"‚ùå Danish crawl failed: {result['errors']}")
        return 0


def crawl_greenlandic_section(section_name, max_articles=0):
    """Crawl Greenlandic section"""
    print("\n" + "="*60)
    print(f"üá¨üá± Crawling Greenlandic (KL) section: {section_name}")
    print("="*60)
    
    if section_name not in SECTION_URLS:
        print(f"‚ùå Invalid section: {section_name}")
        return 0
    
    crawler = SermitsiaqCrawler(
        base_url='https://kl.sermitsiaq.ag',
        language='kl'
    )
    
    result = crawler.crawl_section(
        section_url=SECTION_URLS[section_name]['kl'],
        section_name=section_name,
        max_articles=max_articles,
        headless=True,
        language='kl'
    )
    
    if result['success']:
        print(f"‚úÖ Greenlandic crawl completed: {result['articles_created']} articles")
        return result['articles_created']
    else:
        print(f"‚ùå Greenlandic crawl failed: {result['errors']}")
        return 0


def remove_duplicate_da_articles_in_section(section_name):
    """Remove duplicate DA articles trong section (c√πng published_url trong c√πng section)"""
    print("\n" + "="*60)
    print(f"üîç Checking for duplicate DA articles in section: {section_name}")
    print("="*60)
    
    # ‚ö†Ô∏è QUAN TR·ªåNG: Ch·ªâ x√≥a duplicate trong c√πng section
    # L·∫•y t·∫•t c·∫£ DA articles trong section (bao g·ªìm c·∫£ is_home=True v√† is_home=False)
    all_da_articles = Article.query.filter_by(
        language='da',
        section=section_name
    ).all()
    
    # Group by published_url ƒë·ªÉ t√¨m duplicate trong c√πng section
    url_to_articles = {}
    for article in all_da_articles:
        if article.published_url:
            if article.published_url not in url_to_articles:
                url_to_articles[article.published_url] = []
            url_to_articles[article.published_url].append(article)
    
    # T√¨m v√† x√≥a duplicate
    duplicates_removed = 0
    for published_url, articles in url_to_articles.items():
        if len(articles) > 1:
            # C√≥ duplicate trong c√πng section, ∆∞u ti√™n gi·ªØ article c√≥ is_home=True (v√¨ ƒë√£ ƒë∆∞·ª£c link v√†o home)
            # N·∫øu c·∫£ hai ƒë·ªÅu c√≥ is_home=True ho·∫∑c c·∫£ hai ƒë·ªÅu c√≥ is_home=False, gi·ªØ ID nh·ªè nh·∫•t
            articles_sorted = sorted(articles, key=lambda x: (not x.is_home, x.id))
            article_to_keep = articles_sorted[0]
            articles_to_delete = articles_sorted[1:]
            
            print(f"   ‚ö†Ô∏è  Found {len(articles)} duplicate DA articles in section '{section_name}' with published_url: {published_url[:60]}...")
            print(f"      Keeping article ID: {article_to_keep.id} (is_home: {article_to_keep.is_home})")
            
            for article_to_delete in articles_to_delete:
                print(f"      Deleting duplicate article ID: {article_to_delete.id} (is_home: {article_to_delete.is_home})")
                db.session.delete(article_to_delete)
                duplicates_removed += 1
    
    if duplicates_removed > 0:
        db.session.commit()
        print(f"‚úÖ Removed {duplicates_removed} duplicate DA articles in section '{section_name}'")
    else:
        print(f"‚úÖ No duplicate DA articles found in section '{section_name}'")
    
    return duplicates_removed


def match_dk_kl_section_articles(section_name):
    """Match DK v√† KL articles trong section"""
    print("\n" + "="*60)
    print(f"üîó Matching DK and KL articles in section: {section_name}")
    print("="*60)
    
    # Get DK articles
    dk_articles = Article.query.filter_by(
        language='da',
        section=section_name,
        is_home=False  # Section articles, not home
    ).all()
    
    # Get KL articles
    kl_articles = Article.query.filter_by(
        language='kl',
        section=section_name,
        is_home=False
    ).all()
    
    print(f"   Found {len(dk_articles)} DK articles")
    print(f"   Found {len(kl_articles)} KL articles")
    
    if not dk_articles or not kl_articles:
        print("‚ö†Ô∏è  No articles to match")
        return
    
    # Match and link
    stats = match_and_link_articles(dk_articles, kl_articles)
    
    print(f"‚úÖ Matching completed: {stats['matched_count']} articles matched")
    return stats


def translate_dk_section_to_en(section_name):
    """Translate DK section articles to EN"""
    print("\n" + "="*60)
    print(f"üåê Translating DK section '{section_name}' to EN...")
    print("="*60)
    
    # Get ALL DK articles in section (s·∫Ω t·∫°o temp EN articles)
    dk_articles = Article.query.filter_by(
        language='da',
        section=section_name,
        is_home=False
    ).all()
    
    print(f"   Found {len(dk_articles)} DK articles to translate")
    
    if not dk_articles:
        print("‚ö†Ô∏è  No articles to translate")
        return
    
    # Translate articles (skip n·∫øu ƒë√£ c√≥)
    translated, errors, stats = translate_articles_batch(
        dk_articles,
        target_language='en',
        save_to_db=True,
        delay=0.5
    )
    
    print(f"‚úÖ Translation completed for section: {section_name}")
    if errors:
        print(f"‚ö†Ô∏è  {len(errors)} errors occurred during translation")
    
    # Translate URLs cho EN articles m·ªõi (skip n·∫øu ƒë√£ c√≥ published_url_en)
    print("\n" + "="*60)
    print(f"üåê Translating URLs for EN articles in section: {section_name}")
    print("="*60)
    
    en_articles = Article.query.filter_by(
        language='en',
        section=section_name,
        is_home=False
    ).all()
    
    url_translated_count = 0
    url_skipped_count = 0
    url_error_count = 0
    
    for article in en_articles:
        # Skip n·∫øu ƒë√£ c√≥ published_url_en
        if article.published_url_en and article.published_url_en.strip():
            url_skipped_count += 1
            continue
        
        # Skip n·∫øu kh√¥ng c√≥ published_url
        if not article.published_url or not article.published_url.strip():
            continue
        
        try:
            # Translate URL
            en_url = translate_url(article.published_url, delay=0.3)
            if en_url:
                article.published_url_en = en_url
                db.session.commit()
                url_translated_count += 1
                if url_translated_count % 10 == 0:
                    print(f"   ‚úÖ Translated {url_translated_count} URLs...")
            else:
                url_error_count += 1
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Error translating URL for article {article.id}: {e}")
            url_error_count += 1
            db.session.rollback()
            continue
    
    print(f"‚úÖ URL translation completed:")
    print(f"   - Translated: {url_translated_count}")
    print(f"   - Skipped (already translated): {url_skipped_count}")
    if url_error_count > 0:
        print(f"   - Errors: {url_error_count}")
    
    # Check v√† remove duplicate EN articles trong section
    print("\n" + "="*60)
    print(f"üîç Checking for duplicate EN articles in section: {section_name}")
    print("="*60)
    
    # L·∫•y t·∫•t c·∫£ EN articles trong section
    all_en_articles = Article.query.filter_by(
        language='en',
        section=section_name,
        is_home=False
    ).all()
    
    # Group by published_url ƒë·ªÉ t√¨m duplicate
    url_to_articles = {}
    for article in all_en_articles:
        if article.published_url:
            if article.published_url not in url_to_articles:
                url_to_articles[article.published_url] = []
            url_to_articles[article.published_url].append(article)
    
    # T√¨m v√† x√≥a duplicate
    duplicates_removed = 0
    for published_url, articles in url_to_articles.items():
        if len(articles) > 1:
            # C√≥ duplicate, gi·ªØ l·∫°i article ƒë·∫ßu ti√™n (ID nh·ªè nh·∫•t), x√≥a c√°c article c√≤n l·∫°i
            articles_sorted = sorted(articles, key=lambda x: x.id)
            article_to_keep = articles_sorted[0]
            articles_to_delete = articles_sorted[1:]
            
            print(f"   ‚ö†Ô∏è  Found {len(articles)} duplicate EN articles with published_url: {published_url[:60]}...")
            print(f"      Keeping article ID: {article_to_keep.id}")
            
            for article_to_delete in articles_to_delete:
                print(f"      Deleting duplicate article ID: {article_to_delete.id}")
                db.session.delete(article_to_delete)
                duplicates_removed += 1
    
    if duplicates_removed > 0:
        db.session.commit()
        print(f"‚úÖ Removed {duplicates_removed} duplicate EN articles")
    else:
        print(f"‚úÖ No duplicate EN articles found")
    
    # Check c√°c DA articles ch∆∞a c√≥ EN version v√† t·∫°o n·∫øu ch∆∞a c√≥
    print("\n" + "="*60)
    print(f"üîç Checking DA articles without EN version in section: {section_name}")
    print("="*60)
    
    # L·∫•y t·∫•t c·∫£ DA articles trong section
    dk_articles = Article.query.filter_by(
        language='da',
        section=section_name,
        is_home=False
    ).all()
    
    missing_en_count = 0
    translated_count = 0
    error_count = 0
    
    for dk_article in dk_articles:
        # Skip n·∫øu kh√¥ng c√≥ published_url
        if not dk_article.published_url or not dk_article.published_url.strip():
            continue
        
        # Check xem ƒë√£ c√≥ EN version ch∆∞a (d·ª±a tr√™n published_url + language='en' + section)
        existing_en = Article.query.filter_by(
            published_url=dk_article.published_url,
            language='en',
            section=section_name,
            is_home=False
        ).first()
        
        if not existing_en:
            missing_en_count += 1
            print(f"   ‚ö†Ô∏è  DA article ID {dk_article.id} (URL: {dk_article.published_url[:60]}...) missing EN version")
            
            try:
                # Translate article n√†y
                from services.translation_service import translate_article
                en_article = translate_article(
                    dk_article,
                    target_language='en',
                    delay=0.5
                )
                
                if en_article:
                    # Translate URL cho EN article m·ªõi
                    if dk_article.published_url:
                        en_url = translate_url(dk_article.published_url, delay=0.3)
                        if en_url:
                            en_article.published_url_en = en_url
                    
                    # Save v√†o database
                    db.session.add(en_article)
                    db.session.commit()
                    
                    translated_count += 1
                    if translated_count % 5 == 0:
                        print(f"   ‚úÖ Translated {translated_count} missing EN articles...")
                else:
                    error_count += 1
            except Exception as e:
                print(f"   ‚ùå Error translating DA article {dk_article.id}: {e}")
                error_count += 1
                db.session.rollback()
                continue
    
    if missing_en_count > 0:
        print(f"‚úÖ Missing EN articles check completed:")
        print(f"   - Found {missing_en_count} DA articles without EN version")
        print(f"   - Translated: {translated_count}")
        if error_count > 0:
            print(f"   - Errors: {error_count}")
    else:
        print(f"‚úÖ All DA articles already have EN versions")
    
    return translated, errors


def crawl_danish_home(max_articles=0):
    """Crawl Danish home page"""
    print("\n" + "="*60)
    print(f"üá©üá∞ Crawling Danish (DK) home page...")
    print("="*60)
    
    crawler = SermitsiaqCrawler(
        base_url='https://www.sermitsiaq.ag',
        language='da'
    )
    
    result = crawler.crawl_home(
        home_url='https://www.sermitsiaq.ag',
        max_articles=max_articles,  # 0 = crawl all articles (no limit)
        headless=True
    )
    
    if result['success']:
        print(f"‚úÖ Danish home crawl completed: {result['articles_created']} articles")
        return result['articles_created']
    else:
        print(f"‚ùå Danish home crawl failed: {result['errors']}")
        return 0


def crawl_greenlandic_home(max_articles=0):
    """Crawl Greenlandic home page"""
    print("\n" + "="*60)
    print(f"üá¨üá± Crawling Greenlandic (KL) home page...")
    print("="*60)
    
    crawler = SermitsiaqCrawler(
        base_url='https://kl.sermitsiaq.ag',
        language='kl'
    )
    
    result = crawler.crawl_home(
        home_url='https://kl.sermitsiaq.ag',
        max_articles=max_articles,  # 0 = crawl all articles (no limit)
        headless=True
    )
    
    if result['success']:
        print(f"‚úÖ Greenlandic home crawl completed: {result['articles_created']} articles")
        return result['articles_created']
    else:
        print(f"‚ùå Greenlandic home crawl failed: {result['errors']}")
        return 0


def remove_duplicate_da_home_articles():
    """Remove duplicate DA articles trong home (c√πng published_url)"""
    print("\n" + "="*60)
    print(f"üîç Checking for duplicate DA articles in home")
    print("="*60)
    
    # L·∫•y t·∫•t c·∫£ DA articles v·ªõi is_home=True
    all_da_articles = Article.query.filter_by(
        language='da',
        is_home=True
    ).all()
    
    # Group by published_url ƒë·ªÉ t√¨m duplicate
    url_to_articles = {}
    for article in all_da_articles:
        if article.published_url:
            if article.published_url not in url_to_articles:
                url_to_articles[article.published_url] = []
            url_to_articles[article.published_url].append(article)
    
    # T√¨m v√† x√≥a duplicate
    duplicates_removed = 0
    for published_url, articles in url_to_articles.items():
        if len(articles) > 1:
            # C√≥ duplicate, gi·ªØ article c√≥ ID nh·ªè nh·∫•t (article c≈© h∆°n)
            articles_sorted = sorted(articles, key=lambda x: x.id)
            article_to_keep = articles_sorted[0]
            articles_to_delete = articles_sorted[1:]
            
            print(f"   ‚ö†Ô∏è  Found {len(articles)} duplicate DA articles with published_url: {published_url[:60]}...")
            print(f"      Keeping article ID: {article_to_keep.id}")
            
            for article_to_delete in articles_to_delete:
                print(f"      Deleting duplicate article ID: {article_to_delete.id}")
                db.session.delete(article_to_delete)
                duplicates_removed += 1
    
    if duplicates_removed > 0:
        db.session.commit()
        print(f"‚úÖ Removed {duplicates_removed} duplicate DA articles")
    else:
        print(f"‚úÖ No duplicate DA articles found")
    
    return duplicates_removed


def match_dk_kl_home_articles():
    """Match DK v√† KL articles trong home"""
    print("\n" + "="*60)
    print(f"üîó Matching DK and KL home articles")
    print("="*60)
    
    # Get DK articles
    dk_articles = Article.query.filter_by(
        language='da',
        is_home=True
    ).all()
    
    # Get KL articles
    kl_articles = Article.query.filter_by(
        language='kl',
        is_home=True
    ).all()
    
    print(f"   Found {len(dk_articles)} DK articles")
    print(f"   Found {len(kl_articles)} KL articles")
    
    if not dk_articles or not kl_articles:
        print("‚ö†Ô∏è  No articles to match")
        return
    
    # Match and link
    stats = match_and_link_articles(dk_articles, kl_articles)
    
    print(f"‚úÖ Matching completed: {stats['matched_count']} articles matched")
    return stats


def translate_dk_home_to_en():
    """Translate DK home articles to EN"""
    print("\n" + "="*60)
    print(f"üåê Translating DK home articles to EN...")
    print("="*60)
    
    # Get ALL DK home articles (s·∫Ω t·∫°o temp EN articles)
    dk_articles = Article.query.filter_by(
        language='da',
        is_home=True
    ).all()
    
    print(f"   Found {len(dk_articles)} DK articles to translate")
    
    if not dk_articles:
        print("‚ö†Ô∏è  No articles to translate")
        return
    
    # Translate articles (skip n·∫øu ƒë√£ c√≥)
    translated, errors, stats = translate_articles_batch(
        dk_articles,
        target_language='en',
        save_to_db=True,
        delay=0.5
    )
    
    print(f"‚úÖ Translation completed for home articles")
    if errors:
        print(f"‚ö†Ô∏è  {len(errors)} errors occurred during translation")
    
    # Translate URLs cho EN home articles m·ªõi (skip n·∫øu ƒë√£ c√≥ published_url_en)
    print("\n" + "="*60)
    print(f"üåê Translating URLs for EN home articles")
    print("="*60)
    
    en_articles = Article.query.filter_by(
        language='en',
        is_home=True
    ).all()
    
    url_translated_count = 0
    url_skipped_count = 0
    url_error_count = 0
    
    for article in en_articles:
        # Skip n·∫øu ƒë√£ c√≥ published_url_en
        if article.published_url_en and article.published_url_en.strip():
            url_skipped_count += 1
            continue
        
        # Skip n·∫øu kh√¥ng c√≥ published_url
        if not article.published_url or not article.published_url.strip():
            continue
        
        try:
            # Translate URL
            en_url = translate_url(article.published_url, delay=0.3)
            if en_url:
                article.published_url_en = en_url
                db.session.commit()
                url_translated_count += 1
                if url_translated_count % 10 == 0:
                    print(f"   ‚úÖ Translated {url_translated_count} URLs...")
            else:
                url_error_count += 1
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Error translating URL for article {article.id}: {e}")
            url_error_count += 1
            db.session.rollback()
            continue
    
    print(f"‚úÖ URL translation completed:")
    print(f"   - Translated: {url_translated_count}")
    print(f"   - Skipped (already translated): {url_skipped_count}")
    if url_error_count > 0:
        print(f"   - Errors: {url_error_count}")
    
    # ‚ö†Ô∏è REMOVED: Logic remove duplicate EN articles kh√¥ng c√≤n c·∫ßn thi·∫øt
    # V√¨ logic check duplicate trong translate_articles_batch() ƒë√£ ƒë∆∞·ª£c s·ª≠a
    # ƒë·ªÉ ch·ªâ check theo published_url + language cho home articles (kh√¥ng check section)
    # ‚Üí Kh√¥ng c√≤n t·∫°o duplicate n·ªØa
    
    # Check c√°c DA articles ch∆∞a c√≥ EN version v√† t·∫°o n·∫øu ch∆∞a c√≥
    print("\n" + "="*60)
    print(f"üîç Checking DA articles without EN version in home")
    print("="*60)
    
    # L·∫•y t·∫•t c·∫£ DA articles trong home
    dk_articles = Article.query.filter_by(
        language='da',
        is_home=True
    ).all()
    
    missing_en_count = 0
    translated_count = 0
    error_count = 0
    
    for dk_article in dk_articles:
        # Skip n·∫øu kh√¥ng c√≥ published_url
        if not dk_article.published_url or not dk_article.published_url.strip():
            continue
        
        # Check xem ƒë√£ c√≥ EN version ch∆∞a (d·ª±a tr√™n published_url + language='en' + is_home=True)
        existing_en = Article.query.filter_by(
            published_url=dk_article.published_url,
            language='en',
            is_home=True
        ).first()
        
        if not existing_en:
            missing_en_count += 1
            print(f"   ‚ö†Ô∏è  DA article ID {dk_article.id} (URL: {dk_article.published_url[:60]}...) missing EN version")
            
            try:
                # Translate article n√†y
                from services.translation_service import translate_article
                en_article = translate_article(
                    dk_article,
                    target_language='en',
                    delay=0.5
                )
                
                if en_article:
                    # Translate URL cho EN article m·ªõi
                    if dk_article.published_url:
                        en_url = translate_url(dk_article.published_url, delay=0.3)
                        if en_url:
                            en_article.published_url_en = en_url
                    
                    # Save v√†o database
                    db.session.add(en_article)
                    db.session.commit()
                    
                    translated_count += 1
                    if translated_count % 5 == 0:
                        print(f"   ‚úÖ Translated {translated_count} missing EN articles...")
                else:
                    error_count += 1
            except Exception as e:
                print(f"   ‚ùå Error translating DA article {dk_article.id}: {e}")
                error_count += 1
                db.session.rollback()
                continue
    
    if missing_en_count > 0:
        print(f"‚úÖ Missing EN articles check completed:")
        print(f"   - Found {missing_en_count} DA articles without EN version")
        print(f"   - Translated: {translated_count}")
        if error_count > 0:
            print(f"   - Errors: {error_count}")
    else:
        print(f"‚úÖ All DA articles already have EN versions")
    
    return translated, errors


def process_home(max_articles=0, skip_crawl=False):
    """Process home page: crawl, match, translate"""
    print("\n" + "="*80)
    print(f"üè† PROCESSING HOME PAGE")
    print("="*80)
    
    if not skip_crawl:
        crawl_danish_home(max_articles)
        crawl_greenlandic_home(max_articles)
    
    # ‚ö†Ô∏è T·∫†M COMMENT: Remove duplicate DA articles tr∆∞·ªõc khi match v√† translate
    # remove_duplicate_da_home_articles()
    print(f"\n‚ö†Ô∏è  TEMPORARY: Skipping duplicate removal for home to track where duplicates are created")
    
    match_dk_kl_home_articles()
    translate_dk_home_to_en()
    
    # Print summary
    dk_count = Article.query.filter_by(language='da', is_home=True).count()
    kl_count = Article.query.filter_by(language='kl', is_home=True).count()
    en_count = Article.query.filter_by(language='en', is_home=True).count()
    
    print(f"\nüìä Summary for home:")
    print(f"   - Danish (DK): {dk_count} articles")
    print(f"   - Greenlandic (KL): {kl_count} articles")
    print(f"   - English (EN): {en_count} articles")


def process_section(section_name, max_articles=0, skip_crawl=False):
    """Process m·ªôt section: crawl, match, translate"""
    print("\n" + "="*80)
    print(f"üì∞ PROCESSING SECTION: {section_name.upper()}")
    print("="*80)
    
    if not skip_crawl:
        crawl_danish_section(section_name, max_articles)
        crawl_greenlandic_section(section_name, max_articles)
    
    # ‚ö†Ô∏è T·∫†M COMMENT: Remove duplicate DA articles tr∆∞·ªõc khi match v√† translate
    # remove_duplicate_da_articles_in_section(section_name)
    print(f"\n‚ö†Ô∏è  TEMPORARY: Skipping duplicate removal for section '{section_name}' to track where duplicates are created")
    
    match_dk_kl_section_articles(section_name)
    translate_dk_section_to_en(section_name)
    
    # Print summary
    dk_count = Article.query.filter_by(language='da', section=section_name, is_home=False).count()
    kl_count = Article.query.filter_by(language='kl', section=section_name, is_home=False).count()
    en_count = Article.query.filter_by(language='en', section=section_name, is_home=False).count()
    
    print(f"\nüìä Summary for {section_name}:")
    print(f"   - Danish (DK): {dk_count} articles")
    print(f"   - Greenlandic (KL): {kl_count} articles")
    print(f"   - English (EN): {en_count} articles")


def main():
    """Main function"""
    parser = argparse.ArgumentParser(description='Crawl and translate sections and home for multi-language')
    parser.add_argument('--section', choices=['erhverv', 'samfund', 'kultur', 'sport', 'podcasti', 'home', 'all', 'sections'],
                       default='sections', help='Section to process (default: sections = all sections without home). Use "home" for home page, "sections" for all sections without home, "all" for everything.')
    parser.add_argument('--max-articles', type=int, default=0,
                       help='Maximum articles per section (default: 0 = crawl all). Use 0 to crawl all articles without limit.')
    parser.add_argument('--skip-crawl', action='store_true',
                       help='Skip crawling, only match and translate')
    args = parser.parse_args()
    
    with app.app_context():
        # Handle home separately
        if args.section == 'home':
            try:
                process_home(max_articles=args.max_articles, skip_crawl=args.skip_crawl)
            except Exception as e:
                print(f"‚ùå Error processing home: {e}")
                import traceback
                traceback.print_exc()
        elif args.section == 'sections':
            # Process all sections only (skip home)
            sections = ['erhverv', 'samfund', 'kultur', 'sport', 'podcasti']
            for section in sections:
                try:
                    process_section(section, max_articles=args.max_articles, skip_crawl=args.skip_crawl)
                except Exception as e:
                    print(f"‚ùå Error processing section {section}: {e}")
                    import traceback
                    traceback.print_exc()
                    continue
        elif args.section == 'all':
            # Process all sections first
            sections = ['erhverv', 'samfund', 'kultur', 'sport', 'podcasti']
            for section in sections:
                try:
                    process_section(section, max_articles=args.max_articles, skip_crawl=args.skip_crawl)
                except Exception as e:
                    print(f"‚ùå Error processing section {section}: {e}")
                    import traceback
                    traceback.print_exc()
                    continue
            
            # Then process home last
            try:
                process_home(max_articles=args.max_articles, skip_crawl=args.skip_crawl)
            except Exception as e:
                print(f"‚ùå Error processing home: {e}")
                import traceback
                traceback.print_exc()
        else:
            # Process single section
            sections = [args.section]
            for section in sections:
                try:
                    process_section(section, max_articles=args.max_articles, skip_crawl=args.skip_crawl)
                except Exception as e:
                    print(f"‚ùå Error processing section {section}: {e}")
                    import traceback
                    traceback.print_exc()
                    continue
        
        print("\n" + "="*80)
        print("‚úÖ All processing completed!")
        print("="*80)
        
        # Print overall summary
        print(f"\nüìä Overall Summary:")
        
        # Home summary
        dk_home = Article.query.filter_by(language='da', is_home=True).count()
        kl_home = Article.query.filter_by(language='kl', is_home=True).count()
        en_home = Article.query.filter_by(language='en', is_home=True).count()
        print(f"   HOME: DK={dk_home}, KL={kl_home}, EN={en_home}")
        
        # Sections summary
        sections = ['erhverv', 'samfund', 'kultur', 'sport', 'podcasti']
        for section in sections:
            dk = Article.query.filter_by(language='da', section=section, is_home=False).count()
            kl = Article.query.filter_by(language='kl', section=section, is_home=False).count()
            en = Article.query.filter_by(language='en', section=section, is_home=False).count()
            print(f"   {section.upper()}: DK={dk}, KL={kl}, EN={en}")
        
        # T·ª± ƒë·ªông ch·∫°y crawl article details sau khi ho√†n th√†nh
        print("\n" + "="*80)
        print("üìÑ Starting article details crawl...")
        print("="*80)
        try:
            crawl_article_details(
                language=None,  # Crawl t·∫•t c·∫£ languages (DA v√† KL)
                section=None,   # Crawl t·∫•t c·∫£ sections
                limit=None,     # Kh√¥ng gi·ªõi h·∫°n
                headless=True,  # Headless mode
                delay=2.0,      # Delay 2s gi·ªØa c√°c requests
                auto_translate=True,  # T·ª± ƒë·ªông translate DA sang EN
                translate_delay=0.3   # Delay 0.3s gi·ªØa c√°c l·∫ßn translate
            )
            print("\n" + "="*80)
            print("‚úÖ Article details crawl completed!")
            print("="*80)
        except Exception as e:
            print(f"\n‚ö†Ô∏è  Error during article details crawl: {e}")
            import traceback
            traceback.print_exc()
        
        # T·ª± ƒë·ªông ch·∫°y link_home_articles sau khi crawl article details xong
        print("\n" + "="*80)
        print("üîó Starting home articles linking...")
        print("="*80)
        try:
            # G·ªçi script link_home_articles.py
            script_path = Path(__file__).parent / 'link_home_articles.py'
            result = subprocess.run(
                [sys.executable, str(script_path)],
                cwd=str(Path(__file__).parent),
                check=False
            )
            
            if result.returncode == 0:
                print("\n" + "="*80)
                print("‚úÖ Home articles linking completed!")
                print("="*80)
            else:
                print(f"\n‚ö†Ô∏è  link_home_articles.py exited with code {result.returncode}")
        except Exception as e:
            print(f"\n‚ö†Ô∏è  Error running link_home_articles.py: {e}")
            import traceback
            traceback.print_exc()


if __name__ == '__main__':
    main()