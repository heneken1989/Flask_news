#!/usr/bin/env python3
"""
Script ƒë·ªÉ test SEO meta tags c·ªßa website
Usage: python test_seo_meta.py <url>
"""

import sys
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse

def test_seo_meta(url, expected_title=None, expected_description=None):
    """
    Test SEO meta tags c·ªßa m·ªôt URL
    """
    print(f"\n{'='*60}")
    print(f"üîç Testing SEO for: {url}")
    print(f"{'='*60}")
    
    try:
        response = requests.get(url, timeout=10, headers={
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        })
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        
        errors = []
        warnings = []
        
        # Test title
        title = soup.find('title')
        if title:
            title_text = title.text.strip()
            print(f"‚úÖ Title: {title_text[:80]}...")
            if expected_title and expected_title.lower() not in title_text.lower():
                warnings.append(f"Title kh√¥ng kh·ªõp v·ªõi expected: {expected_title}")
            if len(title_text) > 60:
                warnings.append(f"Title qu√° d√†i ({len(title_text)} chars, n√™n < 60)")
        else:
            errors.append("Kh√¥ng t√¨m th·∫•y <title> tag")
        
        # Test meta description
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc:
            desc = meta_desc.get('content', '').strip()
            print(f"‚úÖ Meta Description: {desc[:80]}...")
            if len(desc) > 160:
                warnings.append(f"Description qu√° d√†i ({len(desc)} chars, n√™n < 160)")
            if len(desc) < 50:
                warnings.append(f"Description qu√° ng·∫Øn ({len(desc)} chars, n√™n > 50)")
            if expected_description and expected_description.lower() not in desc.lower():
                warnings.append(f"Description kh√¥ng kh·ªõp v·ªõi expected")
        else:
            errors.append("Kh√¥ng t√¨m th·∫•y meta description")
        
        # Test og:title
        og_title = soup.find('meta', attrs={'property': 'og:title'})
        if og_title:
            og_title_text = og_title.get('content', '').strip()
            print(f"‚úÖ OG Title: {og_title_text[:80]}...")
            if title and title_text != og_title_text:
                warnings.append("og:title kh√°c v·ªõi <title>")
        else:
            errors.append("Kh√¥ng t√¨m th·∫•y og:title")
        
        # Test og:description
        og_desc = soup.find('meta', attrs={'property': 'og:description'})
        if og_desc:
            print(f"‚úÖ OG Description: {og_desc.get('content', '')[:80]}...")
        else:
            errors.append("Kh√¥ng t√¨m th·∫•y og:description")
        
        # Test og:image
        og_image = soup.find('meta', attrs={'property': 'og:image'})
        if og_image:
            img_url = og_image.get('content', '').strip()
            print(f"‚úÖ OG Image: {img_url[:80]}...")
            # Check if image URL is accessible
            if img_url.startswith('http'):
                try:
                    img_response = requests.head(img_url, timeout=5, allow_redirects=True)
                    if img_response.status_code == 200:
                        print(f"   ‚úÖ Image accessible")
                        # Check image dimensions
                        content_type = img_response.headers.get('Content-Type', '')
                        if 'image' in content_type:
                            print(f"   ‚úÖ Image type: {content_type}")
                    else:
                        warnings.append(f"Image kh√¥ng accessible (status: {img_response.status_code})")
                except:
                    warnings.append("Kh√¥ng th·ªÉ check image accessibility")
            elif img_url.startswith('/'):
                # Relative URL - check if it exists
                full_img_url = urljoin(url, img_url)
                try:
                    img_response = requests.head(full_img_url, timeout=5)
                    if img_response.status_code == 200:
                        print(f"   ‚úÖ Image accessible at {full_img_url}")
                    else:
                        warnings.append(f"Image kh√¥ng accessible (status: {img_response.status_code})")
                except:
                    warnings.append("Kh√¥ng th·ªÉ check image accessibility")
        else:
            errors.append("Kh√¥ng t√¨m th·∫•y og:image")
        
        # Test og:url
        og_url = soup.find('meta', attrs={'property': 'og:url'})
        if og_url:
            print(f"‚úÖ OG URL: {og_url.get('content', '')}")
        else:
            warnings.append("Kh√¥ng t√¨m th·∫•y og:url")
        
        # Test canonical
        canonical = soup.find('link', attrs={'rel': 'canonical'})
        if canonical:
            canonical_url = canonical.get('href', '').strip()
            print(f"‚úÖ Canonical: {canonical_url}")
            if canonical_url != url:
                warnings.append(f"Canonical URL kh√°c v·ªõi current URL")
        else:
            errors.append("Kh√¥ng t√¨m th·∫•y canonical URL")
        
        # Test hreflang
        hreflangs = soup.find_all('link', attrs={'rel': 'alternate', 'hreflang': True})
        if hreflangs:
            print(f"‚úÖ Hreflang tags ({len(hreflangs)}):")
            for hreflang in hreflangs:
                lang = hreflang.get('hreflang')
                href = hreflang.get('href', '')
                print(f"   - {lang}: {href}")  # Hi·ªÉn th·ªã full URL
        else:
            warnings.append("Kh√¥ng t√¨m th·∫•y hreflang tags")
        
        # Test structured data
        json_ld_scripts = soup.find_all('script', attrs={'type': 'application/ld+json'})
        if json_ld_scripts:
            import json
            print(f"‚úÖ Structured Data ({len(json_ld_scripts)} script(s)):")
            for script in json_ld_scripts:
                try:
                    data = json.loads(script.string)
                    if isinstance(data, list):
                        for schema in data:
                            schema_type = schema.get('@type', 'Unknown')
                            print(f"   - {schema_type}")
                    else:
                        schema_type = data.get('@type', 'Unknown')
                        print(f"   - {schema_type}")
                except json.JSONDecodeError as e:
                    errors.append(f"Structured data kh√¥ng parse ƒë∆∞·ª£c: {e}")
        else:
            errors.append("Kh√¥ng t√¨m th·∫•y structured data (JSON-LD)")
        
        # Summary
        print(f"\n{'='*60}")
        if errors:
            print(f"‚ùå ERRORS ({len(errors)}):")
            for error in errors:
                print(f"   - {error}")
        if warnings:
            print(f"‚ö†Ô∏è  WARNINGS ({len(warnings)}):")
            for warning in warnings:
                print(f"   - {warning}")
        if not errors and not warnings:
            print("‚úÖ All checks passed!")
        print(f"{'='*60}\n")
        
        return len(errors) == 0
        
    except requests.exceptions.RequestException as e:
        print(f"‚ùå ERROR: Kh√¥ng th·ªÉ fetch URL: {e}")
        return False
    except Exception as e:
        print(f"‚ùå ERROR: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == '__main__':
    if len(sys.argv) < 2:
        print("Usage: python test_seo_meta.py <url>")
        print("Example: python test_seo_meta.py http://localhost:5000/")
        sys.exit(1)
    
    url = sys.argv[1]
    test_seo_meta(url)

