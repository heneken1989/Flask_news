"""
Script ƒë·ªÉ crawl article detail pages theo batch
- List t·∫•t c·∫£ articles v·ªõi published_url v√† ID
- Check xem ArticleDetail ƒë√£ t·ªìn t·∫°i ch∆∞a (d·ª±a v√†o published_url)
- Ch·ªâ crawl nh·ªØng article detail ch∆∞a c√≥

Usage:
    # List t·∫•t c·∫£ articles c·∫ßn crawl
    python scripts/crawl_article_details_batch.py --list
    
    # Crawl t·∫•t c·∫£ articles ch∆∞a c√≥ detail
    python scripts/crawl_article_details_batch.py --crawl-all
    
    # Crawl theo language
    python scripts/crawl_article_details_batch.py --crawl-all --language en
    
    # Crawl theo section
    python scripts/crawl_article_details_batch.py --crawl-all --section samfund
    
    # Crawl gi·ªõi h·∫°n s·ªë l∆∞·ª£ng
    python scripts/crawl_article_details_batch.py --crawl-all --limit 10
    
    # Crawl v·ªõi headless mode (default)
    python scripts/crawl_article_details_batch.py --crawl-all --headless
    
    # Crawl v·ªõi no-headless mode (ƒë·ªÉ debug)
    python scripts/crawl_article_details_batch.py --crawl-all --no-headless
"""
import sys
import os
import argparse
from datetime import datetime
import time
from contextlib import contextmanager
import shutil

# Add parent directory to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app import app
from database import db, Article, ArticleDetail
from services.article_detail_parser import ArticleDetailParser
from services.image_downloader import download_and_update_image_data
from seleniumbase import SB
from deep_translator import GoogleTranslator
import re
import json

# User data directory ƒë·ªÉ l∆∞u session login
USER_DATA_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'user_data')
LOGIN_URL = "https://www.sermitsiaq.ag/login"
LOGIN_EMAIL = "aluu@greenland.org"
LOGIN_PASSWORD = "LEn924924jfkjfk"


def get_chrome_options_for_headless():
    """
    Tr·∫£ v·ªÅ Chrome options c·∫ßn thi·∫øt cho Linux headless server
    C·∫ßn thi·∫øt khi ch·∫°y v·ªõi root ho·∫∑c kh√¥ng c√≥ display
    """
    # --no-sandbox: B·ªè qua sandbox (c·∫ßn thi·∫øt khi ch·∫°y v·ªõi root)
    # --disable-dev-shm-usage: Tr√°nh l·ªói shared memory tr√™n VPS
    # --disable-gpu: T·∫Øt GPU (kh√¥ng c·∫ßn tr√™n server)
    return "no-sandbox,disable-dev-shm-usage,disable-gpu"


def kill_chrome_processes():
    """
    Kill t·∫•t c·∫£ Chrome/Chromium processes ƒëang ch·∫°y ƒë·ªÉ tr√°nh conflict
    """
    import subprocess
    try:
        # T√¨m t·∫•t c·∫£ Chrome processes
        result = subprocess.run(
            ['ps', 'aux'],
            capture_output=True,
            text=True
        )
        
        chrome_pids = []
        for line in result.stdout.split('\n'):
            if any(keyword in line.lower() for keyword in ['chrome', 'chromium', 'chromedriver']):
                # Tr√°nh kill ch√≠nh script n√†y
                if 'check_chrome_status.py' not in line and 'python' not in line.lower()[:50]:
                    parts = line.split()
                    if len(parts) > 1:
                        try:
                            pid = int(parts[1])
                            chrome_pids.append(pid)
                        except:
                            pass
        
        if chrome_pids:
            print(f"   üî™ Killing {len(chrome_pids)} Chrome/Chromium processes: {chrome_pids[:5]}{'...' if len(chrome_pids) > 5 else ''}")
            for pid in chrome_pids:
                try:
                    os.kill(pid, 9)  # SIGKILL
                except ProcessLookupError:
                    # Process ƒë√£ ch·∫øt
                    pass
                except Exception as e:
                    print(f"   ‚ö†Ô∏è  Error killing process {pid}: {e}")
            
            # ƒê·ª£i m·ªôt ch√∫t ƒë·ªÉ processes ƒë∆∞·ª£c kill
            time.sleep(2)
            print(f"   ‚úÖ Killed Chrome processes")
            return len(chrome_pids)
        else:
            return 0
            
    except Exception as e:
        print(f"   ‚ö†Ô∏è  Error killing Chrome processes: {e}")
        return 0


def cleanup_user_data_dir():
    """
    X√≥a user_data_dir c≈© n·∫øu t·ªìn t·∫°i ƒë·ªÉ tr√°nh conflict
    """
    if os.path.exists(USER_DATA_DIR):
        try:
            print(f"   üóëÔ∏è  Removing old user_data_dir: {USER_DATA_DIR}")
            shutil.rmtree(USER_DATA_DIR)
            print(f"   ‚úÖ Removed old user_data_dir")
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Error removing user_data_dir: {e}")
            # Th·ª≠ x√≥a t·ª´ng file n·∫øu kh√¥ng x√≥a ƒë∆∞·ª£c c·∫£ th∆∞ m·ª•c
            try:
                for root, dirs, files in os.walk(USER_DATA_DIR):
                    for f in files:
                        try:
                            os.remove(os.path.join(root, f))
                        except:
                            pass
                    for d in dirs:
                        try:
                            os.rmdir(os.path.join(root, d))
                        except:
                            pass
                os.rmdir(USER_DATA_DIR)
                print(f"   ‚úÖ Removed user_data_dir (file by file)")
            except:
                pass


def create_fresh_user_data_dir():
    """
    T·∫°o user_data_dir m·ªõi v·ªõi permissions ƒë√∫ng
    """
    cleanup_user_data_dir()
    os.makedirs(USER_DATA_DIR, exist_ok=True)
    os.chmod(USER_DATA_DIR, 0o755)  # rwxr-xr-x
    print(f"   ‚úÖ Created fresh user_data_dir: {USER_DATA_DIR}")


@contextmanager
def start_browser_with_retry(headless=True, max_retries=2):
    """
    Start browser v·ªõi retry logic: n·∫øu kh√¥ng start ƒë∆∞·ª£c, x√≥a user_data_dir v√† th·ª≠ l·∫°i
    
    Args:
        headless: Run browser in headless mode
        max_retries: S·ªë l·∫ßn retry t·ªëi ƒëa
    
    Yields:
        SB instance
    """
    chrome_opts = get_chrome_options_for_headless()
    sb_context = None
    sb = None
    sb_entered = False
    
    for attempt in range(max_retries + 1):
        try:
            # Kill Chrome processes tr∆∞·ªõc khi start (tr√°nh conflict)
            if attempt == 0:
                # L·∫ßn ƒë·∫ßu: kill Chrome processes n·∫øu c√≥
                killed = kill_chrome_processes()
                if killed > 0:
                    print(f"   ‚è≥ Waiting 3 seconds for processes to fully terminate...")
                    time.sleep(3)
            else:
                # C√°c l·∫ßn retry: kill l·∫°i v√† t·∫°o l·∫°i user_data_dir m·ªõi
                print(f"   üîÑ Retry attempt {attempt}/{max_retries}: Killing Chrome processes and creating fresh user_data_dir...")
                kill_chrome_processes()
                time.sleep(2)
                create_fresh_user_data_dir()
            
            # T·∫°o user_data_dir n·∫øu ch∆∞a t·ªìn t·∫°i
            if attempt == 0:
                # L·∫ßn ƒë·∫ßu: th·ª≠ v·ªõi user_data_dir hi·ªán t·∫°i (n·∫øu c√≥)
                os.makedirs(USER_DATA_DIR, exist_ok=True)
                os.chmod(USER_DATA_DIR, 0o755)
            
            # Th·ª≠ start browser - s·ª≠ d·ª•ng context manager ƒë√∫ng c√°ch
            sb_context = SB(uc=True, headless=headless, user_data_dir=USER_DATA_DIR, chromium_arg=chrome_opts)
            sb = sb_context.__enter__()  # Start browser v√† l·∫•y SB instance
            sb_entered = True
            print(f"   ‚úÖ Browser started successfully (attempt {attempt + 1}/{max_retries + 1})")
            try:
                yield sb
            finally:
                # Cleanup khi exit context
                if sb_entered and sb_context:
                    try:
                        sb_context.__exit__(None, None, None)
                    except Exception as cleanup_error:
                        print(f"   ‚ö†Ô∏è  Error during browser cleanup: {cleanup_error}")
            return
            
        except Exception as e:
            error_msg = str(e)
            error_type = str(type(e))
            
            # Cleanup n·∫øu browser ƒë√£ ƒë∆∞·ª£c start nh∆∞ng c√≥ l·ªói
            if sb_entered and sb_context:
                try:
                    sb_context.__exit__(None, None, None)
                except:
                    pass
                sb_entered = False
                sb_context = None
                sb = None
            
            # Ki·ªÉm tra xem c√≥ ph·∫£i l·ªói browser start kh√¥ng
            is_browser_start_error = (
                'SessionNotCreatedException' in error_type or 
                'cannot connect to chrome' in error_msg.lower() or 
                'chrome not reachable' in error_msg.lower() or
                'session not created' in error_msg.lower()
            )
            
            if is_browser_start_error:
                if attempt < max_retries:
                    print(f"   ‚ö†Ô∏è  Browser start failed (attempt {attempt + 1}/{max_retries + 1}): {error_msg[:150]}")
                    print(f"   üîÑ Will retry with fresh user_data_dir...")
                    sb = None
                    continue
                else:
                    print(f"   ‚ùå Browser start failed after {max_retries + 1} attempts")
                    print(f"   ‚ùå Last error: {error_msg[:200]}")
                    raise
            else:
                # L·ªói kh√°c, kh√¥ng retry - re-raise ngay
                print(f"   ‚ùå Browser start failed with unexpected error: {error_msg[:200]}")
                raise


def get_articles_to_crawl(language=None, section=None, limit=None):
    """
    L·∫•y danh s√°ch articles c·∫ßn crawl (ch∆∞a c√≥ ArticleDetail)
    
    Args:
        language: Filter theo language (da, kl, en)
        section: Filter theo section (samfund, sport, kultur, etc.)
        limit: Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng articles
    
    Returns:
        List of Article objects
    """
    query = Article.query.filter(
        Article.published_url.isnot(None),
        Article.published_url != ''
    )
    
    # Lo·∫°i b·ªè www.sjob.gl
    query = query.filter(~Article.published_url.contains('www.sjob.gl'))
    
    # Lo·∫°i b·ªè articles c√≥ language='en' (ƒë∆∞·ª£c d·ªãch t·ª´ DK, kh√¥ng ph·∫£i articles g·ªëc)
    query = query.filter(Article.language != 'en')
    
    # Filter theo language
    if language:
        query = query.filter_by(language=language)
    
    # Filter theo section
    if section:
        query = query.filter_by(section=section)
    
    # Order by published_date desc
    query = query.order_by(Article.published_date.desc().nullslast())
    
    # Limit
    if limit:
        query = query.limit(limit)
    
    articles = query.all()
    
    # Filter: ch·ªâ l·∫•y nh·ªØng articles ch∆∞a c√≥ ArticleDetail
    articles_to_crawl = []
    for article in articles:
        # Double check: lo·∫°i b·ªè www.sjob.gl (n·∫øu c√≥)
        if 'www.sjob.gl' in article.published_url:
            continue
        
        # Double check: lo·∫°i b·ªè articles c√≥ language='en' (ƒë∆∞·ª£c d·ªãch t·ª´ DK)
        if article.language == 'en':
            continue
        
        existing_detail = ArticleDetail.query.filter_by(
            published_url=article.published_url
        ).first()
        
        if not existing_detail:
            articles_to_crawl.append(article)
    
    return articles_to_crawl


def convert_da_url_to_en_url(da_url: str) -> str:
    """
    Convert URL t·ª´ DA sang EN
    V√≠ d·ª•: https://www.sermitsiaq.ag/... -> https://www.sermitsiaq.ag/... (gi·ªØ nguy√™n)
    Ho·∫∑c: https://kl.sermitsiaq.ag/... -> https://www.sermitsiaq.ag/...
    
    Args:
        da_url: URL ti·∫øng ƒêan M·∫°ch
        
    Returns:
        URL ti·∫øng Anh t∆∞∆°ng ·ª©ng
    """
    # Lo·∫°i b·ªè kl. prefix n·∫øu c√≥
    en_url = da_url.replace('kl.sermitsiaq.ag', 'www.sermitsiaq.ag')
    # ƒê·∫£m b·∫£o l√† www.sermitsiaq.ag (kh√¥ng ph·∫£i kl.)
    en_url = re.sub(r'https?://kl\.', 'https://www.', en_url)
    return en_url


def translate_content_blocks(content_blocks: list, source_lang: str = 'da', target_lang: str = 'en', delay: float = 0.3) -> list:
    """
    D·ªãch content_blocks t·ª´ source_lang sang target_lang
    S·ª≠ d·ª•ng GoogleTranslator t·ª´ deep_translator (gi·ªëng translation_service)
    
    Args:
        content_blocks: List of content blocks
        source_lang: Source language code ('da')
        target_lang: Target language code ('en')
        delay: Delay gi·ªØa c√°c l·∫ßn translate (gi√¢y) ƒë·ªÉ tr√°nh rate limit
        
    Returns:
        Translated content blocks
    """
    if not content_blocks:
        return []
    
    translator = GoogleTranslator(source=source_lang, target=target_lang)
    translated_blocks = []
    
    for block in content_blocks:
        translated_block = block.copy()
        
        # Ch·ªâ d·ªãch c√°c block c√≥ text content
        if block.get('type') in ['paragraph', 'heading', 'intro', 'subtitle', 'title']:
            # D·ªãch text
            if block.get('text'):
                try:
                    original_text = block['text']
                    translated_text = translator.translate(original_text)
                    
                    # Vi·∫øt hoa ch·ªØ ƒë·∫ßu c√¢u n·∫øu ch∆∞a vi·∫øt hoa
                    # Ch·ªâ vi·∫øt hoa n·∫øu ch·ªØ ƒë·∫ßu l√† ch·ªØ th∆∞·ªùng
                    if translated_text and translated_text[0].islower():
                        translated_text = translated_text[0].upper() + translated_text[1:]
                    
                    translated_block['text'] = translated_text
                    time.sleep(delay)  # Delay ƒë·ªÉ tr√°nh rate limit
                except Exception as e:
                    print(f"      ‚ö†Ô∏è  Translation error for text: {e}")
                    # Gi·ªØ nguy√™n text n·∫øu d·ªãch l·ªói
                    translated_block['text'] = block['text']
            
            # D·ªãch HTML content (ch·ªâ d·ªãch text trong tags, gi·ªØ nguy√™n tags)
            if block.get('html'):
                try:
                    html = block['html']
                    
                    # B∆Ø·ªöC 1: S·ª≠a l·ªói thi·∫øu kho·∫£ng tr·∫Øng TR∆Ø·ªöC KHI d·ªãch
                    # Th√™m kho·∫£ng tr·∫Øng tr∆∞·ªõc tag n·∫øu word k·∫øt th√∫c b·∫±ng ch·ªØ c√°i v√† tag b·∫Øt ƒë·∫ßu b·∫±ng ch·ªØ c√°i
                    # V√≠ d·ª•: "candies<span" -> "candies <span"
                    html = re.sub(r'([a-zA-Z])(<[a-zA-Z/])', r'\1 \2', html)
                    
                    # B∆Ø·ªöC 1.5: ƒê·∫£m b·∫£o c√≥ kho·∫£ng tr·∫Øng sau d·∫•u ph·∫©y v√† tr∆∞·ªõc ch·ªØ c√°i (n·∫øu thi·∫øu)
                    # V√≠ d·ª•: ",KNQK" -> ", KNQK" ho·∫∑c "</span>KNQK" -> "</span> KNQK"
                    # Nh∆∞ng kh√¥ng s·ª≠a n·∫øu ƒë√£ c√≥ kho·∫£ng tr·∫Øng ho·∫∑c tag
                    html = re.sub(r'(,)([A-Za-z])', r'\1 \2', html)  # D·∫•u ph·∫©y tr∆∞·ªõc ch·ªØ c√°i
                    html = re.sub(r'(</[^>]+>)([A-Za-z])', r'\1 \2', html)  # Closing tag tr∆∞·ªõc ch·ªØ c√°i
                    
                    # T√¨m t·∫•t c·∫£ text nodes v√† d·ªãch
                    first_text_node = True  # Flag ƒë·ªÉ bi·∫øt text node ƒë·∫ßu ti√™n
                    def translate_html_text(match):
                        nonlocal first_text_node
                        text = match.group(1)
                        if text.strip():
                            try:
                                translated = translator.translate(text)
                                
                                # Vi·∫øt hoa ch·ªØ ƒë·∫ßu c·ªßa text node ƒë·∫ßu ti√™n trong block
                                # (th∆∞·ªùng l√† ƒë·∫ßu c√¢u/ƒëo·∫°n)
                                if first_text_node and translated:
                                    # T√¨m ch·ªØ c√°i ƒë·∫ßu ti√™n v√† vi·∫øt hoa
                                    for i, char in enumerate(translated):
                                        if char.isalpha():
                                            translated = translated[:i] + char.upper() + translated[i+1:]
                                            break
                                    first_text_node = False
                                
                                time.sleep(delay)
                                return f'>{translated}<'
                            except:
                                return match.group(0)
                        return match.group(0)
                    
                    # D·ªãch text gi·ªØa c√°c tags
                    translated_html = re.sub(r'>([^<]+)<', translate_html_text, html)
                    
                    # B∆Ø·ªöC 2: S·ª≠a l·∫°i sau khi d·ªãch (ƒë·∫£m b·∫£o kh√¥ng c√≥ l·ªói m·ªõi)
                    # Th√™m kho·∫£ng tr·∫Øng n·∫øu v·∫´n c√≤n pattern "word<tag" sau khi d·ªãch
                    translated_html = re.sub(r'([a-zA-Z])(<[a-zA-Z/])', r'\1 \2', translated_html)
                    
                    # ƒê·∫£m b·∫£o c√≥ kho·∫£ng tr·∫Øng sau d·∫•u ph·∫©y (sau khi d·ªãch c√≥ th·ªÉ b·ªã m·∫•t)
                    translated_html = re.sub(r'(,)([A-Za-z])', r'\1 \2', translated_html)
                    translated_html = re.sub(r'(</[^>]+>)([A-Za-z])', r'\1 \2', translated_html)
                    
                    # S·ª≠a l·ªói "candiesis" -> "candies is" n·∫øu c√≥
                    translated_html = re.sub(r'candiesis', 'candies is', translated_html, flags=re.IGNORECASE)
                    
                    translated_block['html'] = translated_html
                    
                    # C·∫≠p nh·∫≠t text field t·ª´ HTML sau khi d·ªãch (ƒë·ªÉ ƒë·∫£m b·∫£o text v√† HTML ƒë·ªìng b·ªô)
                    # Extract text t·ª´ HTML ƒë·ªÉ c√≥ text ch√≠nh x√°c
                    try:
                        from bs4 import BeautifulSoup
                        soup = BeautifulSoup(translated_html, 'html.parser')
                        extracted_text = soup.get_text(separator=' ', strip=True)
                        # Normalize kho·∫£ng tr·∫Øng (ƒë·∫£m b·∫£o c√≥ kho·∫£ng sau d·∫•u ph·∫©y)
                        extracted_text = re.sub(r'(,)([A-Za-z])', r'\1 \2', extracted_text)
                        # Ch·ªâ c·∫≠p nh·∫≠t n·∫øu text field ƒë√£ ƒë∆∞·ª£c d·ªãch (tr√°nh ghi ƒë√® text g·ªëc)
                        if block.get('text') and translated_block.get('text'):
                            translated_block['text'] = extracted_text
                    except:
                        # N·∫øu kh√¥ng extract ƒë∆∞·ª£c, gi·ªØ nguy√™n text ƒë√£ d·ªãch
                        pass
                except Exception as e:
                    print(f"      ‚ö†Ô∏è  Translation error for HTML: {e}")
                    # Gi·ªØ nguy√™n HTML n·∫øu d·ªãch l·ªói
                    translated_block['html'] = block['html']
        
        # D·ªãch header_image_caption block
        if block.get('type') == 'header_image_caption':
            # D·ªãch caption n·∫øu c√≥
            if block.get('caption'):
                try:
                    translated_caption = translator.translate(block['caption'])
                    translated_block['caption'] = translated_caption
                    time.sleep(delay)
                except Exception as e:
                    print(f"      ‚ö†Ô∏è  Translation error for header_image_caption caption: {e}")
                    translated_block['caption'] = block['caption']
            
            # Author kh√¥ng c·∫ßn d·ªãch (t√™n ng∆∞·ªùi)
            # Nh∆∞ng c√≥ th·ªÉ c·∫ßn x·ª≠ l√Ω prefix "Foto: " / "Assi: "
            if block.get('author'):
                # Author ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω prefix trong parser, ch·ªâ gi·ªØ nguy√™n
                translated_block['author'] = block['author']
        
        # D·ªãch article_meta block (bylines descriptions v√† dates)
        if block.get('type') == 'article_meta':
            if block.get('bylines'):
                translated_bylines = []
                for byline in block.get('bylines', []):
                    translated_byline = byline.copy()
                    # D·ªãch description n·∫øu c√≥
                    if byline.get('description'):
                        try:
                            translated_desc = translator.translate(byline['description'])
                            translated_byline['description'] = translated_desc
                            time.sleep(delay)
                        except Exception as e:
                            print(f"      ‚ö†Ô∏è  Translation error for byline description: {e}")
                            # Gi·ªØ nguy√™n n·∫øu d·ªãch l·ªói
                            translated_byline['description'] = byline['description']
                    translated_bylines.append(translated_byline)
                translated_block['bylines'] = translated_bylines
            
            # D·ªãch dates
            if block.get('dates'):
                translated_dates = {}
                for date_type, date_info in block.get('dates', {}).items():
                    translated_date_info = date_info.copy()
                    
                    # D·ªãch label (v√≠ d·ª•: "Offentliggjort" -> "Published")
                    if date_info.get('label'):
                        try:
                            translated_label = translator.translate(date_info['label'])
                            translated_date_info['label'] = translated_label
                            time.sleep(delay)
                        except Exception as e:
                            print(f"      ‚ö†Ô∏è  Translation error for date label: {e}")
                            translated_date_info['label'] = date_info['label']
                    
                    # D·ªãch title (v√≠ d·ª•: "Offentliggjort mandag 19. jan 2026 06:04" -> "Published Monday 19. Jan 2026 06:04")
                    if date_info.get('title'):
                        try:
                            translated_title = translator.translate(date_info['title'])
                            translated_date_info['title'] = translated_title
                            time.sleep(delay)
                        except Exception as e:
                            print(f"      ‚ö†Ô∏è  Translation error for date title: {e}")
                            translated_date_info['title'] = date_info['title']
                    
                    # D·ªãch text (v√≠ d·ª•: "mandag 19. jan 2026 06:04" -> "Monday 19. Jan 2026 06:04")
                    if date_info.get('text'):
                        try:
                            translated_text = translator.translate(date_info['text'])
                            translated_date_info['text'] = translated_text
                            time.sleep(delay)
                        except Exception as e:
                            print(f"      ‚ö†Ô∏è  Translation error for date text: {e}")
                            translated_date_info['text'] = date_info['text']
                    
                    # datetime gi·ªØ nguy√™n (ISO format)
                    if date_info.get('datetime'):
                        translated_date_info['datetime'] = date_info['datetime']
                    
                    translated_dates[date_type] = translated_date_info
                
                translated_block['dates'] = translated_dates
        
        # D·ªãch article_footer_tags block (tags text)
        if block.get('type') == 'article_footer_tags':
            if block.get('tags'):
                translated_tags = []
                for tag in block.get('tags', []):
                    translated_tag = tag.copy()
                    # D·ªãch tag text n·∫øu c√≥
                    if tag.get('text'):
                        try:
                            translated_text = translator.translate(tag['text'])
                            translated_tag['text'] = translated_text
                            time.sleep(delay)
                        except Exception as e:
                            print(f"      ‚ö†Ô∏è  Translation error for tag text: {e}")
                            # Gi·ªØ nguy√™n n·∫øu d·ªãch l·ªói
                            translated_tag['text'] = tag['text']
                    translated_tags.append(translated_tag)
                translated_block['tags'] = translated_tags
        
        # D·ªãch image block (caption v√† author)
        if block.get('type') == 'image':
            # D·ªãch caption n·∫øu c√≥
            if block.get('caption'):
                try:
                    translated_caption = translator.translate(block['caption'])
                    translated_block['caption'] = translated_caption
                    time.sleep(delay)
                except Exception as e:
                    print(f"      ‚ö†Ô∏è  Translation error for image caption: {e}")
                    # Gi·ªØ nguy√™n n·∫øu d·ªãch l·ªói
                    translated_block['caption'] = block['caption']
            
            # D·ªãch author n·∫øu c√≥
            if block.get('author'):
                try:
                    translated_author = translator.translate(block['author'])
                    translated_block['author'] = translated_author
                    time.sleep(delay)
                except Exception as e:
                    print(f"      ‚ö†Ô∏è  Translation error for image author: {e}")
                    # Gi·ªØ nguy√™n n·∫øu d·ªãch l·ªói
                    translated_block['author'] = block['author']
        
        # Gi·ªØ nguy√™n c√°c block kh√°c (ads, paywall_offers, etc.)
        translated_blocks.append(translated_block)
    
    return translated_blocks


def create_en_article_detail_from_da(da_article_detail: ArticleDetail, delay: float = 0.3) -> ArticleDetail:
    """
    T·∫°o article_detail EN t·ª´ article_detail DA
    
    Args:
        da_article_detail: ArticleDetail object v·ªõi language='da'
        delay: Delay gi·ªØa c√°c l·∫ßn translate (gi√¢y) ƒë·ªÉ tr√°nh rate limit
        
    Returns:
        ArticleDetail object v·ªõi language='en' ho·∫∑c existing n·∫øu ƒë√£ t·ªìn t·∫°i
    """
    if da_article_detail.language != 'da':
        raise ValueError(f"Source article_detail must be in Danish (da), got {da_article_detail.language}")
    
    # Convert URL t·ª´ DA sang EN
    en_url = convert_da_url_to_en_url(da_article_detail.published_url)
    
    # CH·ªà ki·ªÉm tra xem ƒë√£ c√≥ EN version ch∆∞a (kh√¥ng check DA version)
    # V√¨ unique constraint l√† (published_url, language), n√™n c√≥ th·ªÉ c√≥ c·∫£ DA v√† EN v·ªõi c√πng URL
    existing_en_detail = ArticleDetail.query.filter_by(published_url=en_url, language='en').first()
    
    if existing_en_detail:
        # ƒê√£ c√≥ EN version ‚Üí skip
        print(f"   ‚ÑπÔ∏è  EN version already exists (ID: {existing_en_detail.id}), skipping translation")
        return existing_en_detail
    
    # D·ªãch content_blocks
    print(f"   üåê Translating content blocks from DA to EN...")
    translated_blocks = translate_content_blocks(
        da_article_detail.content_blocks or [],
        source_lang='da',
        target_lang='en',
        delay=delay
    )
    
    # T·∫°o ArticleDetail m·ªõi v·ªõi language='en'
    en_article_detail = ArticleDetail(
        published_url=en_url,
        content_blocks=translated_blocks,
        language='en',
        element_guid=da_article_detail.element_guid
    )
    
    try:
        db.session.add(en_article_detail)
        db.session.commit()
        print(f"   ‚úÖ Created EN article_detail (ID: {en_article_detail.id}, Blocks: {len(translated_blocks)})")
        return en_article_detail
    except Exception as e:
        # Rollback n·∫øu l·ªói (ƒë·∫∑c bi·ªát l√† IntegrityError)
        db.session.rollback()
        
        # Ki·ªÉm tra xem c√≥ ph·∫£i do duplicate kh√¥ng (n·∫øu migration ch∆∞a ch·∫°y, v·∫´n c√≥ th·ªÉ b·ªã l·ªói unique)
        from sqlalchemy.exc import IntegrityError
        if isinstance(e, IntegrityError) or 'unique' in str(e).lower() or 'duplicate' in str(e).lower():
            # Ki·ªÉm tra l·∫°i xem ƒë√£ c√≥ EN version ch∆∞a
            existing_en = ArticleDetail.query.filter_by(published_url=en_url, language='en').first()
            if existing_en:
                print(f"   ‚ÑπÔ∏è  EN version already exists (ID: {existing_en.id}), skipping translation")
                return existing_en
            else:
                print(f"   ‚ö†Ô∏è  Unique constraint error - might need to run migration script")
                print(f"   ‚ö†Ô∏è  Run: python deploy/migrate_article_details_composite_unique.py")
        
        print(f"   ‚ùå Error creating EN article_detail: {e}")
        raise


def translate_da_article_details_to_en(limit=None):
    """
    D·ªãch t·∫•t c·∫£ article_detail t·ª´ DA sang EN
    
    Args:
        limit: Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng articles ƒë·ªÉ d·ªãch
    """
    # L·∫•y t·∫•t c·∫£ article_detail c√≥ language='da' v√† published_url kh√¥ng ph·∫£i kl.sermitsiaq.ag
    query = ArticleDetail.query.filter(
        ArticleDetail.language == 'da',
        ~ArticleDetail.published_url.contains('kl.sermitsiaq.ag')
    )
    
    if limit:
        query = query.limit(limit)
    
    da_details = query.all()
    
    if not da_details:
        print("\n‚úÖ Kh√¥ng c√≥ article_detail DA n√†o c·∫ßn d·ªãch!")
        return
    
    print(f"\nüåê B·∫Øt ƒë·∫ßu d·ªãch {len(da_details)} article_detail t·ª´ DA sang EN...\n")
    
    success_count = 0
    skip_count = 0
    fail_count = 0
    
    for i, da_detail in enumerate(da_details, 1):
        print(f"\n[{i}/{len(da_details)}] Processing: {da_detail.published_url[:70]}...")
        
        try:
            # Convert URL sang EN
            en_url = convert_da_url_to_en_url(da_detail.published_url)
            
            # CH·ªà ki·ªÉm tra xem ƒë√£ c√≥ EN version ch∆∞a (kh√¥ng check DA version)
            existing_en = ArticleDetail.query.filter_by(published_url=en_url, language='en').first()
            if existing_en:
                print(f"   ‚è≠Ô∏è  Skipped - EN version already exists (ID: {existing_en.id})")
                skip_count += 1
                continue
            
            # T·∫°o EN version
            en_detail = create_en_article_detail_from_da(da_detail)
            if en_detail:
                success_count += 1
            else:
                skip_count += 1  # Kh√¥ng ph·∫£i l·ªói, ch·ªâ l√† skip
                
        except Exception as e:
            print(f"   ‚ùå Error: {e}")
            import traceback
            traceback.print_exc()
            # Rollback session n·∫øu c√≥ l·ªói
            try:
                db.session.rollback()
            except:
                pass
            fail_count += 1
    
    print(f"\n{'='*60}")
    print(f"‚úÖ Ho√†n th√†nh d·ªãch!")
    print(f"   Success: {success_count}/{len(da_details)}")
    print(f"   Skipped: {skip_count}/{len(da_details)}")
    print(f"   Failed: {fail_count}/{len(da_details)}")
    print(f"{'='*60}\n")


def list_articles_to_crawl(language=None, section=None, limit=None):
    """
    List t·∫•t c·∫£ articles c·∫ßn crawl
    """
    articles = get_articles_to_crawl(language=language, section=section, limit=limit)
    
    print(f"\nüìã Articles c·∫ßn crawl:")
    print(f"   Total: {len(articles)} articles")
    
    if language:
        print(f"   Language: {language}")
    if section:
        print(f"   Section: {section}")
    if limit:
        print(f"   Limit: {limit}")
    
    print(f"\n   Articles:")
    for i, article in enumerate(articles, 1):
        print(f"   {i}. ID: {article.id:6d} | {article.language:2s} | {article.section:12s} | {article.published_url[:70]}...")
    
    return articles


def handle_cookie_popup(sb):
    """
    X·ª≠ l√Ω cookie consent popup n·∫øu c√≥ xu·∫•t hi·ªán
    
    Args:
        sb: SeleniumBase instance
    """
    try:
        # Ki·ªÉm tra xem c√≥ popup kh√¥ng
        sb.sleep(1)  # ƒê·ª£i popup xu·∫•t hi·ªán
        
        # T√¨m v√† click button "ACCEPTER ALLE" (Accept All)
        # S·ª≠ d·ª•ng JavaScript ƒë·ªÉ t√¨m button c√≥ text ch·ª©a "ACCEPTER" ho·∫∑c "Accept"
        buttons = sb.execute_script("""
            var buttons = document.querySelectorAll('button');
            for (var i = 0; i < buttons.length; i++) {
                var text = buttons[i].textContent || buttons[i].innerText;
                if (text.includes('ACCEPTER') || text.includes('Accepter') || 
                    text.includes('ACCEPT') || text.includes('Accept')) {
                    return buttons[i];
                }
            }
            return null;
        """)
        
        if buttons:
            sb.execute_script("arguments[0].click();", buttons)
            print("   ‚úÖ Accepted cookie consent popup")
            sb.sleep(3)  # Wait for popup to close and page to stabilize
            
            # Ki·ªÉm tra xem popup ƒë√£ ƒë√≥ng ch∆∞a
            try:
                # ƒê·ª£i popup bi·∫øn m·∫•t
                sb.wait_for_element_not_visible('button:contains("ACCEPTER")', timeout=5)
            except:
                pass
            
            # ƒê·∫£m b·∫£o ƒëang ·ªü ƒë√∫ng trang login sau khi accept cookie
            current_url = sb.get_current_url()
            if 'login' not in current_url.lower():
                print("   üîÑ Reloading login page after cookie acceptance...")
                sb.open(LOGIN_URL)
                sb.sleep(3)  # Wait for page to load
            
            return True
        
        # Fallback: T√¨m b·∫±ng text content
        try:
            page_source = sb.get_page_source()
            if 'Du bestemmer over dine data' in page_source or 'cookie' in page_source.lower():
                # T√¨m button b·∫±ng xpath
                try:
                    accept_btn = sb.find_element('//button[contains(text(), "ACCEPTER") or contains(text(), "Accepter")]', timeout=3)
                    if accept_btn:
                        sb.click(accept_btn)
                        print("   ‚úÖ Accepted cookie consent popup (via XPath)")
                        sb.sleep(3)  # Wait for popup to close
                        
                        # ƒê·∫£m b·∫£o ƒëang ·ªü ƒë√∫ng trang login
                        current_url = sb.get_current_url()
                        if 'login' not in current_url.lower():
                            print("   üîÑ Reloading login page after cookie acceptance...")
                            sb.open(LOGIN_URL)
                            sb.sleep(3)
                        
                        return True
                except:
                    pass
        except:
            pass
        
        # N·∫øu kh√¥ng t√¨m th·∫•y popup, c√≥ th·ªÉ ƒë√£ ƒë∆∞·ª£c accept ho·∫∑c kh√¥ng c√≥
        return False
    except Exception as e:
        # Kh√¥ng c√≥ popup ho·∫∑c ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω
        return False


def ensure_login(sb):
    """
    ƒê·∫£m b·∫£o ƒë√£ login v√†o sermitsiaq.ag
    Ki·ªÉm tra b·∫±ng c√°ch m·ªü trang login v√† xem c√≥ n√∫t "Log ud" kh√¥ng
    N·∫øu ƒë√£ login, trang login s·∫Ω hi·ªÉn th·ªã "Du er allerede logget ind: Log ud"
    
    Args:
        sb: SeleniumBase instance
    """
    print("üîê Checking login status...")
    
    # M·ªü trang login ƒë·ªÉ ki·ªÉm tra
    try:
        sb.open(LOGIN_URL)
        sb.sleep(2)  # Wait for page to load
        
        # X·ª≠ l√Ω cookie popup tr∆∞·ªõc khi ki·ªÉm tra login
        handle_cookie_popup(sb)
        sb.sleep(1)  # Wait a bit after handling popup
        
        # Ki·ªÉm tra xem c√≥ n√∫t "Log ud" (Logout) ho·∫∑c text "Du er allerede logget ind" kh√¥ng
        # CH·ªà ki·ªÉm tra n·∫øu th·ª±c s·ª± ƒë√£ login (n√∫t logout ch·ªâ xu·∫•t hi·ªán khi ƒë√£ login)
        try:
            # T√¨m n√∫t logout
            logout_button = sb.find_element('button.logout', timeout=3)
            if logout_button:
                print("   ‚úÖ Already logged in (found Log ud button)")
                # Ch·ª•p m√†n h√¨nh ƒë·ªÉ x√°c nh·∫≠n
                screenshot_path = os.path.join(USER_DATA_DIR, 'login_check_logged_in.png')
                sb.save_screenshot(screenshot_path)
                print(f"   üì∏ Screenshot saved: {screenshot_path}")
                return True
        except:
            pass
        
        # Ki·ªÉm tra text "Du er allerede logget ind" trong page source
        try:
            page_source = sb.get_page_source()
            if 'Du er allerede logget ind' in page_source:
                # T√¨m n√∫t logout trong page source
                if 'button.logout' in page_source or 'class="logout"' in page_source:
                    print("   ‚úÖ Already logged in (found login status message with logout button)")
                    # Ch·ª•p m√†n h√¨nh ƒë·ªÉ x√°c nh·∫≠n
                    screenshot_path = os.path.join(USER_DATA_DIR, 'login_check_logged_in.png')
                    sb.save_screenshot(screenshot_path)
                    print(f"   üì∏ Screenshot saved: {screenshot_path}")
                    return True
        except:
            pass
        
        # N·∫øu kh√¥ng t√¨m th·∫•y logout button ho·∫∑c message, ch∆∞a login
        print("   ‚ö†Ô∏è  Not logged in (no Log ud button found), attempting login...")
        needs_login = True
        
    except Exception as e:
        print(f"   ‚ö†Ô∏è  Error checking login status: {e}, attempting login...")
        needs_login = True
    
    if needs_login:
        try:
            # ƒê·∫£m b·∫£o ƒëang ·ªü trang login (c√≥ th·ªÉ ƒë√£ m·ªü ·ªü tr√™n)
            current_url = sb.get_current_url()
            if 'login' not in current_url.lower():
                print(f"   üîë Navigating to {LOGIN_URL}...")
                sb.open(LOGIN_URL)
                sb.sleep(3)  # Wait for page to load
            
            # X·ª≠ l√Ω cookie popup tr∆∞·ªõc khi login
            handle_cookie_popup(sb)
            
            # ƒê·∫£m b·∫£o ƒëang ·ªü trang login v√† ƒë·ª£i form load
            current_url = sb.get_current_url()
            if 'login' not in current_url.lower():
                print(f"   üîÑ Navigating to {LOGIN_URL}...")
                sb.open(LOGIN_URL)
                sb.sleep(3)
            
            # Ki·ªÉm tra l·∫°i xem c√≥ ƒë√£ login ch∆∞a (c√≥ th·ªÉ ƒë√£ login trong l√∫c ch·ªù)
            try:
                logout_button = sb.find_element('button.logout', timeout=2)
                if logout_button:
                    print("   ‚úÖ Already logged in (found Log ud button after navigation)")
                    # Ch·ª•p m√†n h√¨nh ƒë·ªÉ x√°c nh·∫≠n
                    screenshot_path = os.path.join(USER_DATA_DIR, 'login_check_after_nav.png')
                    sb.save_screenshot(screenshot_path)
                    print(f"   üì∏ Screenshot saved: {screenshot_path}")
                    return True
            except:
                pass
            
            print(f"   üîë Logging in...")
            
            # Ch·ª•p m√†n h√¨nh tr∆∞·ªõc khi login ƒë·ªÉ debug
            screenshot_path = os.path.join(USER_DATA_DIR, 'before_login.png')
            sb.save_screenshot(screenshot_path)
            print(f"   üì∏ Screenshot before login: {screenshot_path}")
            
            # Form login n·∫±m trong iframe 0, c·∫ßn switch v√†o iframe tr∆∞·ªõc
            print("   üîÑ Switching to iframe containing login form...")
            try:
                # ƒê·ª£i iframe load
                sb.sleep(2)
                # Switch v√†o iframe 0 (iframe ƒë·∫ßu ti√™n ch·ª©a form login)
                sb.switch_to_frame(0)
                sb.sleep(2)  # ƒê·ª£i iframe content load
                print("   ‚úÖ Switched to iframe")
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Could not switch to iframe: {e}")
                # Th·ª≠ reload v√† switch l·∫°i
                sb.switch_to_default_content()
                sb.open(LOGIN_URL)
                sb.sleep(3)
                handle_cookie_popup(sb)
                sb.sleep(2)
                sb.switch_to_frame(0)
                sb.sleep(2)
            
            # ƒê·ª£i form login xu·∫•t hi·ªán trong iframe
            try:
                sb.wait_for_element('#id_subscriber', timeout=10)
            except:
                print("   ‚ö†Ô∏è  Login form not found in iframe, trying to reload...")
                sb.switch_to_default_content()
                sb.open(LOGIN_URL)
                sb.sleep(3)
                handle_cookie_popup(sb)
                sb.sleep(2)
                sb.switch_to_frame(0)
                sb.sleep(2)
                sb.wait_for_element('#id_subscriber', timeout=10)
            
            # Fill in email/subscriber field
            subscriber_input = sb.find_element('#id_subscriber', timeout=10)
            if subscriber_input:
                sb.type('#id_subscriber', LOGIN_EMAIL)
                print(f"   ‚úÖ Filled subscriber field")
            else:
                print("   ‚ùå Could not find subscriber input field")
                return False
            
            # Fill in password field
            password_input = sb.find_element('#id_password', timeout=10)
            if password_input:
                sb.type('#id_password', LOGIN_PASSWORD)
                print(f"   ‚úÖ Filled password field")
            else:
                print("   ‚ùå Could not find password input field")
                return False
            
            # Click login button
            login_button = sb.find_element('button[type="submit"]', timeout=10)
            if login_button:
                sb.click('button[type="submit"]')
                print(f"   ‚úÖ Clicked login button")
                sb.sleep(5)  # Wait for login to complete
                
                # Switch v·ªÅ default content ƒë·ªÉ ki·ªÉm tra login status
                sb.switch_to_default_content()
                sb.sleep(2)
                
                # Ki·ªÉm tra xem ƒë√£ login th√†nh c√¥ng ch∆∞a b·∫±ng c√°ch t√¨m n√∫t "Log ud"
                try:
                    logout_button = sb.find_element('button.logout', timeout=3)
                    if logout_button:
                        print("   ‚úÖ Login successful! (found Log ud button)")
                        # Ch·ª•p m√†n h√¨nh ƒë·ªÉ x√°c nh·∫≠n
                        screenshot_path = os.path.join(USER_DATA_DIR, 'login_success.png')
                        sb.save_screenshot(screenshot_path)
                        print(f"   üì∏ Screenshot saved: {screenshot_path}")
                        return True
                except:
                    pass
                
                # Fallback: ki·ªÉm tra page source
                try:
                    page_source = sb.get_page_source()
                    if 'Du er allerede logget ind' in page_source or 'Log ud' in page_source:
                        print("   ‚úÖ Login successful! (found login status message)")
                        # Ch·ª•p m√†n h√¨nh ƒë·ªÉ x√°c nh·∫≠n
                        screenshot_path = os.path.join(USER_DATA_DIR, 'login_success.png')
                        sb.save_screenshot(screenshot_path)
                        print(f"   üì∏ Screenshot saved: {screenshot_path}")
                        return True
                except:
                    pass
                
                # Fallback: ki·ªÉm tra URL
                current_url = sb.get_current_url()
                if 'login' not in current_url.lower():
                    print("   ‚úÖ Login successful! (redirected away from login page)")
                    # Ch·ª•p m√†n h√¨nh ƒë·ªÉ x√°c nh·∫≠n
                    screenshot_path = os.path.join(USER_DATA_DIR, 'login_success_redirect.png')
                    sb.save_screenshot(screenshot_path)
                    print(f"   üì∏ Screenshot saved: {screenshot_path}")
                    return True
                else:
                    print("   ‚ùå Login failed - still on login page")
                    # Ch·ª•p m√†n h√¨nh ƒë·ªÉ debug
                    screenshot_path = os.path.join(USER_DATA_DIR, 'login_failed.png')
                    sb.save_screenshot(screenshot_path)
                    print(f"   üì∏ Screenshot saved: {screenshot_path}")
                    return False
            else:
                print("   ‚ùå Could not find login button")
                return False
                
        except Exception as e:
            print(f"   ‚ùå Error during login: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    return True


def crawl_article_detail(url: str, language: str = 'da', headless: bool = True, download_images: bool = True):
    """
    Crawl article detail page v√† l∆∞u v√†o database
    
    Args:
        url: URL c·ªßa article detail page
        language: Language code ('da', 'kl', 'en')
        headless: Run browser in headless mode
        download_images: Download images v·ªÅ .com domain n·∫øu True
    
    Returns:
        ArticleDetail object or None
    """
    print(f"üîç Crawling: {url[:70]}...")
    
    # Start browser v·ªõi retry logic (t·ª± ƒë·ªông x√≥a v√† t·∫°o l·∫°i user_data_dir n·∫øu c·∫ßn)
    with start_browser_with_retry(headless=headless) as sb:
        # ƒê·∫£m b·∫£o ƒë√£ login tr∆∞·ªõc khi crawl (ch·ªâ login 1 l·∫ßn cho batch)
        # Note: Login s·∫Ω ƒë∆∞·ª£c th·ª±c hi·ªán ·ªü batch_crawl_articles, kh√¥ng c·∫ßn login l·∫°i m·ªói article
        try:
            # Navigate to URL
            sb.open(url)
            sb.sleep(2)  # Wait for page to load
            
            # Ki·ªÉm tra xem c√≥ ph·∫£i liveblog kh√¥ng (c√≥ .livefeed)
            is_liveblog = False
            try:
                livefeed_elem = sb.find_element('.livefeed', timeout=3)
                if livefeed_elem:
                    is_liveblog = True
                    print(f"   üì∫ Detected liveblog format")
            except:
                pass
            
            # Wait for content (bodytext ho·∫∑c livefeed)
            if is_liveblog:
                try:
                    sb.wait_for_element('.livefeed', timeout=10)
                except:
                    print(f"   ‚ö†Ô∏è  Timeout waiting for .livefeed")
                    return None
            else:
                try:
                    sb.wait_for_element('.bodytext', timeout=10)
                except:
                    print(f"   ‚ö†Ô∏è  Timeout waiting for .bodytext")
                    return None
            
            # Get article title
            title = None
            try:
                title_elem = sb.find_element('h1.headline.mainTitle', timeout=5)
                if title_elem:
                    title = title_elem.text
            except:
                pass
            
            # Get excerpt
            excerpt = None
            try:
                excerpt_elem = sb.find_element('h2.subtitle', timeout=5)
                if excerpt_elem:
                    excerpt = excerpt_elem.text
            except:
                pass
            
            # Get articleHeader HTML (ch·ª©a subtitle v√† meta) - ƒë·ªÉ parser c√≥ th·ªÉ parse subtitle
            article_header_html = None
            meta_html = None
            try:
                # Try to find articleHeader
                article_header = sb.find_element('.articleHeader', timeout=5)
                if article_header:
                    article_header_html = article_header.get_attribute('outerHTML')
                    print(f"   ‚úÖ Found articleHeader via Selenium ({len(article_header_html)} chars)")
                    
                    # Extract meta t·ª´ articleHeader
                    try:
                        meta_elem = article_header.find_element('.meta', timeout=3)
                        if meta_elem:
                            meta_html = meta_elem.get_attribute('outerHTML')
                    except:
                        pass
            except:
                # Fallback: parse from page source
                try:
                    from bs4 import BeautifulSoup
                    page_source = sb.get_page_source()
                    soup = BeautifulSoup(page_source, 'html.parser')
                    article_header = soup.find('div', class_='articleHeader')
                    if article_header:
                        article_header_html = str(article_header)
                        print(f"   ‚úÖ Found articleHeader from page source ({len(article_header_html)} chars)")
                        
                        meta_div = article_header.find('div', class_='meta')
                        if meta_div:
                            meta_html = str(meta_div)
                except:
                    pass
            
            # Get content HTML (bodytext ho·∫∑c livefeed)
            bodytext_html = None
            livefeed_html = None
            
            try:
                page_source = sb.get_page_source()
                from bs4 import BeautifulSoup
                soup = BeautifulSoup(page_source, 'html.parser')
                
                if is_liveblog:
                    # T√¨m livefeed container
                    livefeed_div = soup.find('div', class_='livefeed')
                    if not livefeed_div:
                        # Try by ID pattern
                        livefeed_div = soup.find('div', id=lambda x: x and 'livefeed' in str(x))
                    
                    if livefeed_div:
                        livefeed_html = str(livefeed_div)
                        print(f"   ‚úÖ Found livefeed HTML ({len(livefeed_html)} chars)")
                    else:
                        # Fallback: try Selenium element
                        try:
                            livefeed_elem = sb.find_element('.livefeed', timeout=5)
                            if livefeed_elem:
                                livefeed_html = livefeed_elem.get_attribute('outerHTML')
                                print(f"   ‚úÖ Found livefeed via Selenium ({len(livefeed_html)} chars)")
                        except:
                            pass
                else:
                    # T√¨m bodytext nh∆∞ b√¨nh th∆∞·ªùng
                    bodytext_div = soup.find('div', class_='bodytext', attrs={'data-element-guid': True})
                    if bodytext_div:
                        bodytext_html = str(bodytext_div)
                    else:
                        # Fallback: try Selenium element
                        try:
                            bodytext_elem = sb.find_element('div.bodytext.large-12', timeout=5)
                            if bodytext_elem:
                                bodytext_html = bodytext_elem.get_attribute('outerHTML')
                        except:
                            pass
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Error parsing content: {e}")
                return None
            
            # Get paywall offers section
            offers_html = None
            try:
                offers_elem = sb.find_element('.iteras-offers', timeout=5)
                if offers_elem:
                    offers_html = offers_elem.get_attribute('outerHTML')
            except:
                pass
            
            # Get article footer tags section
            footer_html = None
            try:
                footer_elem = sb.find_element('.articleFooter', timeout=5)
                if footer_elem:
                    footer_html = footer_elem.get_attribute('outerHTML')
            except:
                # Fallback: parse from page source
                try:
                    from bs4 import BeautifulSoup
                    page_source = sb.get_page_source()
                    soup = BeautifulSoup(page_source, 'html.parser')
                    footer_div = soup.find('div', class_='articleFooter')
                    if footer_div:
                        footer_html = str(footer_div)
                except:
                    pass
            
            # Combine HTML - articleHeader (ch·ª©a subtitle) n√™n ƒë·∫∑t ƒë·∫ßu ti√™n, sau ƒë√≥ meta, r·ªìi bodytext/livefeed
            full_html = ''
            if article_header_html:
                # S·ª≠ d·ª•ng articleHeader HTML ƒë·ªÉ parser c√≥ th·ªÉ parse subtitle
                full_html = article_header_html
                print(f"   ‚úÖ Added articleHeader to HTML ({len(article_header_html)} chars)")
            elif meta_html:
                # Fallback: ch·ªâ c√≥ meta n·∫øu kh√¥ng c√≥ articleHeader
                full_html = meta_html
            
            # Th√™m livefeed ho·∫∑c bodytext
            if livefeed_html:
                full_html += livefeed_html
                print(f"   ‚úÖ Added livefeed to HTML ({len(livefeed_html)} chars)")
            elif bodytext_html:
                full_html += bodytext_html
                print(f"   ‚úÖ Added bodytext to HTML ({len(bodytext_html)} chars)")
            
            if offers_html:
                full_html += offers_html
            if footer_html:
                full_html += footer_html
            
            if not full_html:
                print(f"   ‚ùå Could not find content")
                return None
            
            # Get element_guid from bodytext ho·∫∑c livefeed
            element_guid = None
            if livefeed_html:
                from bs4 import BeautifulSoup
                soup = BeautifulSoup(livefeed_html, 'html.parser')
                # T√¨m element_guid t·ª´ livefeed container
                livefeed_div = soup.find('div', class_='livefeed')
                if livefeed_div:
                    element_guid = livefeed_div.get('id', '').replace('livefeed_', '')
            elif bodytext_html:
                from bs4 import BeautifulSoup
                soup = BeautifulSoup(bodytext_html, 'html.parser')
                bodytext_div = soup.find('div', class_='bodytext')
                if bodytext_div:
                    element_guid = bodytext_div.get('data-element-guid', '')
            
            # Parse and save to database
            article_detail = ArticleDetailParser.save_article_detail(
                published_url=url,
                html_content=full_html,
                language=language,
                element_guid=element_guid
            )
            
            print(f"   ‚úÖ Saved (Detail ID: {article_detail.id}, Blocks: {len(article_detail.content_blocks)})")
            
            # Update article if exists
            article = Article.query.filter_by(published_url=url).first()
            if article:
                if title and not article.title:
                    article.title = title
                if excerpt and not article.excerpt:
                    article.excerpt = excerpt
                
                # Download v√† c·∫≠p nh·∫≠t images t·ª´ article.image_data (header image)
                # Function download_and_update_image_data s·∫Ω t·ª± ki·ªÉm tra t·ª´ng URL v√† ch·ªâ download nh·ªØng URL ch∆∞a c√≥ .com
                if download_images and article.image_data:
                    try:
                        # Parse image_data (c√≥ th·ªÉ l√† dict ho·∫∑c JSON string)
                        img_data = article.image_data
                        if isinstance(img_data, str):
                            try:
                                img_data = json.loads(img_data)
                            except:
                                img_data = {}
                        
                        # Lu√¥n g·ªçi download_and_update_image_data ƒë·ªÉ ki·ªÉm tra v√† download t·ª´ng URL
                        if isinstance(img_data, dict) and len(img_data) > 0:
                            print(f"   üì• Processing header image for article (ID: {article.id})...")
                            try:
                                updated_image_data = download_and_update_image_data(
                                    img_data,
                                    base_url='https://www.sermitsiaq.com',
                                    download_all_formats=True  # Download t·∫•t c·∫£ formats cho article detail
                                )
                                # Update image_data trong Article
                                article.image_data = updated_image_data
                                print(f"   ‚úÖ Updated header image_data")
                            except Exception as e:
                                print(f"   ‚ö†Ô∏è  Error downloading header images: {e}")
                                # Gi·ªØ nguy√™n image_data g·ªëc n·∫øu l·ªói
                        else:
                            print(f"   ‚ÑπÔ∏è  No header image_data to process")
                    except Exception as e:
                        print(f"   ‚ö†Ô∏è  Error processing header image_data: {e}")
                
                # Download v√† c·∫≠p nh·∫≠t images t·ª´ article_detail.content_blocks (images trong n·ªôi dung)
                if download_images and article_detail.content_blocks:
                    try:
                        content_blocks = article_detail.content_blocks
                        if isinstance(content_blocks, str):
                            try:
                                content_blocks = json.loads(content_blocks)
                            except:
                                content_blocks = []
                        
                        if isinstance(content_blocks, list):
                            image_blocks_count = 0
                            updated_blocks_count = 0
                            
                            # T·∫°o list m·ªõi ƒë·ªÉ SQLAlchemy detect ƒë∆∞·ª£c thay ƒë·ªïi
                            updated_content_blocks = []
                            
                            for block in content_blocks:
                                # T√¨m c√°c image blocks
                                if block.get('type') == 'image' and block.get('image_sources'):
                                    image_blocks_count += 1
                                    image_sources = block.get('image_sources', {})
                                    
                                    if isinstance(image_sources, dict) and len(image_sources) > 0:
                                        print(f"   üì• Processing image block #{image_blocks_count}...")
                                        try:
                                            # Download v√† c·∫≠p nh·∫≠t image_sources
                                            updated_image_sources = download_and_update_image_data(
                                                image_sources,
                                                base_url='https://www.sermitsiaq.com',
                                                download_all_formats=True  # Download t·∫•t c·∫£ formats
                                            )
                                            # T·∫°o block m·ªõi v·ªõi image_sources ƒë√£ update
                                            updated_block = block.copy()
                                            updated_block['image_sources'] = updated_image_sources
                                            updated_content_blocks.append(updated_block)
                                            updated_blocks_count += 1
                                            print(f"      ‚úÖ Updated image block #{image_blocks_count}")
                                        except Exception as e:
                                            print(f"      ‚ö†Ô∏è  Error downloading image block #{image_blocks_count}: {e}")
                                            # Gi·ªØ nguy√™n block n·∫øu l·ªói
                                            updated_content_blocks.append(block)
                                    else:
                                        # Kh√¥ng c√≥ image_sources, gi·ªØ nguy√™n block
                                        updated_content_blocks.append(block)
                                else:
                                    # Kh√¥ng ph·∫£i image block, gi·ªØ nguy√™n
                                    updated_content_blocks.append(block)
                            
                            if image_blocks_count > 0:
                                # Update content_blocks trong ArticleDetail v·ªõi list m·ªõi
                                article_detail.content_blocks = updated_content_blocks
                                # Force mark as modified ƒë·ªÉ ƒë·∫£m b·∫£o SQLAlchemy detect thay ƒë·ªïi
                                from sqlalchemy.orm.attributes import flag_modified
                                flag_modified(article_detail, 'content_blocks')
                                print(f"   ‚úÖ Updated {updated_blocks_count}/{image_blocks_count} image blocks in content")
                            else:
                                print(f"   ‚ÑπÔ∏è  No image blocks found in content")
                    except Exception as e:
                        print(f"   ‚ö†Ô∏è  Error processing content image blocks: {e}")
                
                db.session.commit()
            
            # N·∫øu l√† article_detail DA (kh√¥ng ph·∫£i kl.sermitsiaq.ag), t·ª± ƒë·ªông t·∫°o EN version
            if language == 'da' and 'kl.sermitsiaq.ag' not in url:
                try:
                    en_url = convert_da_url_to_en_url(url)
                    existing_en = ArticleDetail.query.filter_by(published_url=en_url, language='en').first()
                    if not existing_en:
                        print(f"   üåê Auto-creating EN version...")
                        create_en_article_detail_from_da(article_detail)
                except Exception as e:
                    print(f"   ‚ö†Ô∏è  Error creating EN version: {e}")
            
            return article_detail
                
        except Exception as e:
            print(f"   ‚ùå Error: {e}")
            import traceback
            traceback.print_exc()
            return None


def update_is_temp_flag():
    """
    Update is_temp=False cho t·∫•t c·∫£ articles c√≥ is_temp=True v√† ƒë√£ c√≥ ArticleDetail
    Function n√†y lu√¥n ƒë∆∞·ª£c g·ªçi ·ªü cu·ªëi crawl_all() ƒë·ªÉ ƒë·∫£m b·∫£o articles ƒë√£ c√≥ detail ƒë∆∞·ª£c set is_temp=False
    """
    print(f"\n{'='*60}")
    print(f"üîÑ Updating is_temp=False for articles with details crawled")
    print(f"{'='*60}")
    
    try:
        # T√¨m t·∫•t c·∫£ articles c√≥ is_temp=True v√† ƒë√£ c√≥ ArticleDetail
        temp_articles = Article.query.filter_by(is_temp=True).all()
        updated_count = 0
        
        for article in temp_articles:
            # Check xem ƒë√£ c√≥ ArticleDetail ch∆∞a
            if article.published_url:
                existing_detail = ArticleDetail.query.filter_by(
                    published_url=article.published_url
                ).first()
                
                if existing_detail:
                    # ƒê√£ c√≥ detail ‚Üí set is_temp=False
                    article.is_temp = False
                    updated_count += 1
        
        if updated_count > 0:
            db.session.commit()
            print(f"   ‚úÖ Updated {updated_count} articles: is_temp=True ‚Üí is_temp=False")
        else:
            print(f"   ‚ÑπÔ∏è  No articles to update (all temp articles still missing details)")
        
        # ƒê·∫øm s·ªë articles v·∫´n c√≤n is_temp=True
        remaining_temp = Article.query.filter_by(is_temp=True).count()
        if remaining_temp > 0:
            print(f"   ‚ö†Ô∏è  {remaining_temp} articles still have is_temp=True (details not crawled yet)")
        
    except Exception as e:
        print(f"   ‚ùå Error updating is_temp: {e}")
        import traceback
        traceback.print_exc()
        db.session.rollback()


def crawl_all(language=None, section=None, limit=None, headless=True, delay=2, auto_translate=True, translate_delay=0.3, download_images=True):
    """
    Crawl t·∫•t c·∫£ articles ch∆∞a c√≥ detail
    
    Args:
        language: Filter theo language
        section: Filter theo section
        limit: Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng
        headless: Run browser in headless mode
        delay: Delay gi·ªØa c√°c requests (seconds)
        auto_translate: T·ª± ƒë·ªông translate article_detail DA sang EN sau khi crawl xong
        translate_delay: Delay gi·ªØa c√°c l·∫ßn translate (seconds)
        download_images: Download images v·ªÅ .com domain n·∫øu True
    """
    articles = get_articles_to_crawl(language=language, section=section, limit=limit)
    
    if not articles:
        print("\n‚úÖ Kh√¥ng c√≥ articles n√†o c·∫ßn crawl!")
        # N·∫øu auto_translate=True, v·∫´n ch·∫°y translate cho c√°c article_detail DA ƒë√£ c√≥
        if auto_translate:
            print("\nüåê Kh√¥ng c√≥ articles c·∫ßn crawl, nh∆∞ng s·∫Ω ki·ªÉm tra v√† translate c√°c article_detail DA ƒë√£ c√≥...")
            translate_da_article_details_to_en(limit=None)
        
        # ‚ö†Ô∏è QUAN TR·ªåNG: V·∫´n ph·∫£i update is_temp=False cho articles ƒë√£ c√≥ detail
        update_is_temp_flag()
        return
    
    print(f"\nüöÄ B·∫Øt ƒë·∫ßu crawl {len(articles)} articles...")
    if language:
        print(f"   Language: {language}")
    if section:
        print(f"   Section: {section}")
    if limit:
        print(f"   Limit: {limit}")
    print(f"   Headless: {headless}")
    print(f"   Delay: {delay}s gi·ªØa c√°c requests")
    print(f"   Auto-translate: {auto_translate}")
    if auto_translate:
        print(f"   Translate delay: {translate_delay}s")
    print(f"   Download images: {download_images}\n")
    
    # Login m·ªôt l·∫ßn tr∆∞·ªõc khi b·∫Øt ƒë·∫ßu crawl (s·ª≠ d·ª•ng user_data_dir ƒë·ªÉ l∆∞u session)
    print("üîê Initializing browser session with login...")
    # Start browser v·ªõi retry logic (t·ª± ƒë·ªông x√≥a v√† t·∫°o l·∫°i user_data_dir n·∫øu c·∫ßn)
    with start_browser_with_retry(headless=headless) as sb:
        if not ensure_login(sb):
            print("‚ùå Failed to login, cannot proceed with crawling")
            return
    
    # Sau khi login xong, session ƒë√£ ƒë∆∞·ª£c l∆∞u trong user_data_dir
    # C√°c l·∫ßn crawl ti·∫øp theo s·∫Ω t·ª± ƒë·ªông s·ª≠ d·ª•ng session ƒë√£ l∆∞u
    print("‚úÖ Login session saved, starting to crawl articles...\n")
    
    success_count = 0
    fail_count = 0
    crawled_da_details = []  # L∆∞u c√°c article_detail DA ƒë√£ crawl ƒë·ªÉ translate sau
    
    for i, article in enumerate(articles, 1):
        print(f"\n[{i}/{len(articles)}] Article ID: {article.id}")
        
        result = crawl_article_detail(
            url=article.published_url,
            language=article.language,
            headless=headless,
            download_images=download_images
        )
        
        if result:
            success_count += 1
            # L∆∞u l·∫°i article_detail DA ƒë·ªÉ translate sau (ch·ªâ DA, kh√¥ng ph·∫£i kl.sermitsiaq.ag)
            if result.language == 'da' and 'kl.sermitsiaq.ag' not in result.published_url:
                crawled_da_details.append(result)
        else:
            fail_count += 1
        
        # Delay gi·ªØa c√°c requests
        if i < len(articles):
            time.sleep(delay)
    
    print(f"\n{'='*60}")
    print(f"‚úÖ Crawl ho√†n th√†nh!")
    print(f"   Success: {success_count}/{len(articles)}")
    print(f"   Failed: {fail_count}/{len(articles)}")
    print(f"{'='*60}\n")
    
    # T·ª± ƒë·ªông translate article_detail DA sang EN sau khi crawl xong
    if auto_translate and crawled_da_details:
        print(f"\n{'='*60}")
        print(f"üåê B·∫Øt ƒë·∫ßu translate {len(crawled_da_details)} article_detail t·ª´ DA sang EN...")
        print(f"{'='*60}\n")
        
        translate_success = 0
        translate_skip = 0
        translate_fail = 0
        
        for idx, da_detail in enumerate(crawled_da_details, 1):
            try:
                print(f"\n[{idx}/{len(crawled_da_details)}] Translating article_detail ID: {da_detail.id}")
                print(f"   URL: {da_detail.published_url[:70]}...")
                
                # CH·ªà ki·ªÉm tra xem ƒë√£ c√≥ EN version ch∆∞a (kh√¥ng check DA version)
                en_url = convert_da_url_to_en_url(da_detail.published_url)
                existing_en = ArticleDetail.query.filter_by(published_url=en_url, language='en').first()
                
                if existing_en:
                    print(f"   ‚è≠Ô∏è  Skipped - EN version already exists (ID: {existing_en.id})")
                    translate_skip += 1
                    continue
                
                # Ch·ªâ translate n·∫øu ch∆∞a c√≥ ArticleDetail v·ªõi URL n√†y
                en_detail = create_en_article_detail_from_da(da_detail, delay=translate_delay)
                
                if en_detail:
                    translate_success += 1
                else:
                    translate_skip += 1  # Kh√¥ng ph·∫£i l·ªói, ch·ªâ l√† skip
                    
            except Exception as e:
                print(f"   ‚ùå Error translating: {e}")
                import traceback
                traceback.print_exc()
                # Rollback session n·∫øu c√≥ l·ªói
                try:
                    db.session.rollback()
                except:
                    pass
                translate_fail += 1
                continue
        
        print(f"\n{'='*60}")
        print(f"‚úÖ Translation ho√†n th√†nh!")
        print(f"   Success: {translate_success}/{len(crawled_da_details)}")
        print(f"   Skipped: {translate_skip}/{len(crawled_da_details)}")
        print(f"   Failed: {translate_fail}/{len(crawled_da_details)}")
        print(f"{'='*60}\n")
    elif auto_translate and not crawled_da_details:
        print("\n‚ÑπÔ∏è  Kh√¥ng c√≥ article_detail DA n√†o ƒë·ªÉ translate (t·∫•t c·∫£ ƒë·ªÅu l√† KL ho·∫∑c kh√¥ng crawl ƒë∆∞·ª£c)\n")
    
    # ‚ö†Ô∏è QUAN TR·ªåNG: Lu√¥n update is_temp=False ·ªü cu·ªëi (b·∫•t k·ªÉ c√≥ crawl hay kh√¥ng)
    # ƒê·ªÉ ƒë·∫£m b·∫£o articles ƒë√£ c√≥ ArticleDetail ƒë∆∞·ª£c set is_temp=False
    update_is_temp_flag()


def main():
    parser = argparse.ArgumentParser(
        description='Crawl article detail pages theo batch',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # M·∫∑c ƒë·ªãnh: Crawl v√† translate t·∫•t c·∫£ articles ch∆∞a c√≥ detail
  python scripts/crawl_article_details_batch.py
  
  # List t·∫•t c·∫£ articles c·∫ßn crawl
  python scripts/crawl_article_details_batch.py --list
  
  # Crawl t·∫•t c·∫£ articles ch∆∞a c√≥ detail (t∆∞∆°ng ƒë∆∞∆°ng v·ªõi kh√¥ng c√≥ flag)
  python scripts/crawl_article_details_batch.py --crawl-all
  
  # Crawl theo language
  python scripts/crawl_article_details_batch.py --language en
  
  # Crawl theo section
  python scripts/crawl_article_details_batch.py --section samfund
  
  # Crawl gi·ªõi h·∫°n s·ªë l∆∞·ª£ng
  python scripts/crawl_article_details_batch.py --limit 10
  
  # Crawl nh∆∞ng kh√¥ng translate
  python scripts/crawl_article_details_batch.py --no-auto-translate
  
  # Ch·ªâ translate c√°c article_detail DA ƒë√£ c√≥
  python scripts/crawl_article_details_batch.py --translate-only
  
  # Crawl nh∆∞ng kh√¥ng download images
  python scripts/crawl_article_details_batch.py --no-download-images
        """
    )
    
    parser.add_argument('--list', action='store_true',
                        help='List t·∫•t c·∫£ articles c·∫ßn crawl')
    parser.add_argument('--crawl-all', action='store_true',
                        help='Crawl t·∫•t c·∫£ articles ch∆∞a c√≥ detail (m·∫∑c ƒë·ªãnh: b·∫≠t n·∫øu kh√¥ng c√≥ flag kh√°c)')
    parser.add_argument('--language', '-l', choices=['da', 'kl', 'en'],
                        help='Filter theo language')
    parser.add_argument('--section', '-s',
                        help='Filter theo section (samfund, sport, kultur, etc.)')
    parser.add_argument('--limit', '-n', type=int,
                        help='Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng articles')
    parser.add_argument('--no-headless', action='store_true',
                        help='Run browser in no-headless mode (ƒë·ªÉ debug)')
    parser.add_argument('--delay', '-d', type=float, default=2.0,
                        help='Delay gi·ªØa c√°c requests (seconds, default: 2.0)')
    parser.add_argument('--no-auto-translate', action='store_true',
                        help='T·∫Øt t·ª± ƒë·ªông translate sau khi crawl (m·∫∑c ƒë·ªãnh: b·∫≠t)')
    parser.add_argument('--translate-delay', type=float, default=0.3,
                        help='Delay gi·ªØa c√°c l·∫ßn translate (seconds, default: 0.3)')
    parser.add_argument('--translate-only', action='store_true',
                        help='Ch·ªâ d·ªãch c√°c article_detail DA ƒë√£ c√≥, kh√¥ng crawl m·ªõi')
    parser.add_argument('--translate-limit', type=int,
                        help='Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng article_detail ƒë·ªÉ d·ªãch')
    parser.add_argument('--no-download-images', action='store_true',
                        help='T·∫Øt t·ª± ƒë·ªông download images v·ªÅ .com domain (m·∫∑c ƒë·ªãnh: b·∫≠t)')
    
    args = parser.parse_args()
    
    with app.app_context():
        if args.translate_only:
            # Ch·ªâ d·ªãch, kh√¥ng crawl
            translate_da_article_details_to_en(limit=args.translate_limit)
        elif args.list:
            list_articles_to_crawl(
                language=args.language,
                section=args.section,
                limit=args.limit
            )
        else:
            # M·∫∑c ƒë·ªãnh: crawl v√† translate (n·∫øu kh√¥ng c√≥ flag --list ho·∫∑c --translate-only)
            # C√≥ th·ªÉ d√πng --crawl-all ho·∫∑c kh√¥ng c·∫ßn flag g√¨ c≈©ng ƒë∆∞·ª£c
            crawl_all(
                language=args.language,
                section=args.section,
                limit=args.limit,
                headless=not args.no_headless,
                delay=args.delay,
                auto_translate=not args.no_auto_translate,  # M·∫∑c ƒë·ªãnh b·∫≠t auto-translate
                translate_delay=args.translate_delay,
                download_images=not args.no_download_images  # M·∫∑c ƒë·ªãnh b·∫≠t download images
            )


if __name__ == '__main__':
    main()

