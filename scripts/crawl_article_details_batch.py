"""
Script ƒë·ªÉ crawl article detail pages theo batch
- List t·∫•t c·∫£ articles v·ªõi published_url v√† ID
- Check xem ArticleDetail ƒë√£ t·ªìn t·∫°i ch∆∞a (d·ª±a v√†o published_url)
- Ch·ªâ crawl nh·ªØng article detail ch∆∞a c√≥

Usage:
    # List t·∫•t c·∫£ articles c·∫ßn crawl
    python scripts/crawl_article_details_batch.py --list
    
    # Crawl t·∫•t c·∫£ articles ch∆∞a c√≥ detail
    python scripts/crawl_article_details_batch.py --crawl-all
    
    # Crawl theo language
    python scripts/crawl_article_details_batch.py --crawl-all --language en
    
    # Crawl theo section
    python scripts/crawl_article_details_batch.py --crawl-all --section samfund
    
    # Crawl gi·ªõi h·∫°n s·ªë l∆∞·ª£ng
    python scripts/crawl_article_details_batch.py --crawl-all --limit 10
    
    # Crawl v·ªõi headless mode (default)
    python scripts/crawl_article_details_batch.py --crawl-all --headless
    
    # Crawl v·ªõi no-headless mode (ƒë·ªÉ debug)
    python scripts/crawl_article_details_batch.py --crawl-all --no-headless
"""
import sys
import os
import argparse
from datetime import datetime
import time

# Add parent directory to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app import app
from database import db, Article, ArticleDetail
from services.article_detail_parser import ArticleDetailParser
from seleniumbase import SB
from googletrans import Translator
import re

# User data directory ƒë·ªÉ l∆∞u session login
USER_DATA_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'user_data')
LOGIN_URL = "https://www.sermitsiaq.ag/login"
LOGIN_EMAIL = "aluu@greenland.org"
LOGIN_PASSWORD = "LEn924924jfkjfk"


def get_articles_to_crawl(language=None, section=None, limit=None):
    """
    L·∫•y danh s√°ch articles c·∫ßn crawl (ch∆∞a c√≥ ArticleDetail)
    
    Args:
        language: Filter theo language (da, kl, en)
        section: Filter theo section (samfund, sport, kultur, etc.)
        limit: Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng articles
    
    Returns:
        List of Article objects
    """
    query = Article.query.filter(
        Article.published_url.isnot(None),
        Article.published_url != ''
    )
    
    # Lo·∫°i b·ªè www.sjob.gl
    query = query.filter(~Article.published_url.contains('www.sjob.gl'))
    
    # Lo·∫°i b·ªè articles c√≥ language='en' (ƒë∆∞·ª£c d·ªãch t·ª´ DK, kh√¥ng ph·∫£i articles g·ªëc)
    query = query.filter(Article.language != 'en')
    
    # Filter theo language
    if language:
        query = query.filter_by(language=language)
    
    # Filter theo section
    if section:
        query = query.filter_by(section=section)
    
    # Order by published_date desc
    query = query.order_by(Article.published_date.desc().nullslast())
    
    # Limit
    if limit:
        query = query.limit(limit)
    
    articles = query.all()
    
    # Filter: ch·ªâ l·∫•y nh·ªØng articles ch∆∞a c√≥ ArticleDetail
    articles_to_crawl = []
    for article in articles:
        # Double check: lo·∫°i b·ªè www.sjob.gl (n·∫øu c√≥)
        if 'www.sjob.gl' in article.published_url:
            continue
        
        # Double check: lo·∫°i b·ªè articles c√≥ language='en' (ƒë∆∞·ª£c d·ªãch t·ª´ DK)
        if article.language == 'en':
            continue
        
        existing_detail = ArticleDetail.query.filter_by(
            published_url=article.published_url
        ).first()
        
        if not existing_detail:
            articles_to_crawl.append(article)
    
    return articles_to_crawl


def convert_da_url_to_en_url(da_url: str) -> str:
    """
    Convert URL t·ª´ DA sang EN
    V√≠ d·ª•: https://www.sermitsiaq.ag/... -> https://www.sermitsiaq.ag/... (gi·ªØ nguy√™n)
    Ho·∫∑c: https://kl.sermitsiaq.ag/... -> https://www.sermitsiaq.ag/...
    
    Args:
        da_url: URL ti·∫øng ƒêan M·∫°ch
        
    Returns:
        URL ti·∫øng Anh t∆∞∆°ng ·ª©ng
    """
    # Lo·∫°i b·ªè kl. prefix n·∫øu c√≥
    en_url = da_url.replace('kl.sermitsiaq.ag', 'www.sermitsiaq.ag')
    # ƒê·∫£m b·∫£o l√† www.sermitsiaq.ag (kh√¥ng ph·∫£i kl.)
    en_url = re.sub(r'https?://kl\.', 'https://www.', en_url)
    return en_url


def translate_content_blocks(content_blocks: list, source_lang: str = 'da', target_lang: str = 'en') -> list:
    """
    D·ªãch content_blocks t·ª´ source_lang sang target_lang
    
    Args:
        content_blocks: List of content blocks
        source_lang: Source language code ('da')
        target_lang: Target language code ('en')
        
    Returns:
        Translated content blocks
    """
    if not content_blocks:
        return []
    
    translator = Translator()
    translated_blocks = []
    
    for block in content_blocks:
        translated_block = block.copy()
        
        # Ch·ªâ d·ªãch c√°c block c√≥ text content
        if block.get('type') in ['paragraph', 'heading', 'intro']:
            # D·ªãch text
            if block.get('text'):
                try:
                    translated_text = translator.translate(
                        block['text'], 
                        src=source_lang, 
                        dest=target_lang
                    ).text
                    translated_block['text'] = translated_text
                except Exception as e:
                    print(f"   ‚ö†Ô∏è  Translation error for text: {e}")
                    # Gi·ªØ nguy√™n text n·∫øu d·ªãch l·ªói
                    translated_block['text'] = block['text']
            
            # D·ªãch HTML content (ch·ªâ d·ªãch text trong tags, gi·ªØ nguy√™n tags)
            if block.get('html'):
                try:
                    # T√°ch HTML th√†nh tags v√† text
                    html = block['html']
                    # T√¨m t·∫•t c·∫£ text nodes v√† d·ªãch
                    def translate_html_text(match):
                        text = match.group(1)
                        if text.strip():
                            try:
                                translated = translator.translate(text, src=source_lang, dest=target_lang).text
                                return f'>{translated}<'
                            except:
                                return match.group(0)
                        return match.group(0)
                    
                    # D·ªãch text gi·ªØa c√°c tags
                    translated_html = re.sub(r'>([^<]+)<', translate_html_text, html)
                    translated_block['html'] = translated_html
                except Exception as e:
                    print(f"   ‚ö†Ô∏è  Translation error for HTML: {e}")
                    # Gi·ªØ nguy√™n HTML n·∫øu d·ªãch l·ªói
                    translated_block['html'] = block['html']
        
        # Gi·ªØ nguy√™n c√°c block kh√°c (images, ads, paywall_offers, etc.)
        translated_blocks.append(translated_block)
    
    return translated_blocks


def create_en_article_detail_from_da(da_article_detail: ArticleDetail) -> ArticleDetail:
    """
    T·∫°o article_detail EN t·ª´ article_detail DA
    
    Args:
        da_article_detail: ArticleDetail object v·ªõi language='da'
        
    Returns:
        ArticleDetail object v·ªõi language='en' ho·∫∑c None n·∫øu ƒë√£ t·ªìn t·∫°i
    """
    # Convert URL t·ª´ DA sang EN
    en_url = convert_da_url_to_en_url(da_article_detail.published_url)
    
    # Ki·ªÉm tra xem ƒë√£ c√≥ EN version ch∆∞a
    existing_en_detail = ArticleDetail.query.filter_by(published_url=en_url, language='en').first()
    if existing_en_detail:
        print(f"   ‚ÑπÔ∏è  EN version already exists for {en_url}")
        return existing_en_detail
    
    # D·ªãch content_blocks
    print(f"   üåê Translating content blocks from DA to EN...")
    translated_blocks = translate_content_blocks(
        da_article_detail.content_blocks or [],
        source_lang='da',
        target_lang='en'
    )
    
    # T·∫°o ArticleDetail m·ªõi v·ªõi language='en'
    en_article_detail = ArticleDetail(
        published_url=en_url,
        content_blocks=translated_blocks,
        language='en',
        element_guid=da_article_detail.element_guid
    )
    
    db.session.add(en_article_detail)
    db.session.commit()
    
    print(f"   ‚úÖ Created EN article_detail (ID: {en_article_detail.id})")
    return en_article_detail


def translate_da_article_details_to_en(limit=None):
    """
    D·ªãch t·∫•t c·∫£ article_detail t·ª´ DA sang EN
    
    Args:
        limit: Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng articles ƒë·ªÉ d·ªãch
    """
    # L·∫•y t·∫•t c·∫£ article_detail c√≥ language='da' v√† published_url kh√¥ng ph·∫£i kl.sermitsiaq.ag
    query = ArticleDetail.query.filter(
        ArticleDetail.language == 'da',
        ~ArticleDetail.published_url.contains('kl.sermitsiaq.ag')
    )
    
    if limit:
        query = query.limit(limit)
    
    da_details = query.all()
    
    if not da_details:
        print("\n‚úÖ Kh√¥ng c√≥ article_detail DA n√†o c·∫ßn d·ªãch!")
        return
    
    print(f"\nüåê B·∫Øt ƒë·∫ßu d·ªãch {len(da_details)} article_detail t·ª´ DA sang EN...\n")
    
    success_count = 0
    skip_count = 0
    fail_count = 0
    
    for i, da_detail in enumerate(da_details, 1):
        print(f"\n[{i}/{len(da_details)}] Processing: {da_detail.published_url[:70]}...")
        
        try:
            # Convert URL sang EN
            en_url = convert_da_url_to_en_url(da_detail.published_url)
            
            # Ki·ªÉm tra xem ƒë√£ c√≥ EN version ch∆∞a
            existing_en = ArticleDetail.query.filter_by(published_url=en_url, language='en').first()
            if existing_en:
                print(f"   ‚è≠Ô∏è  Skipped - EN version already exists")
                skip_count += 1
                continue
            
            # T·∫°o EN version
            en_detail = create_en_article_detail_from_da(da_detail)
            if en_detail:
                success_count += 1
            else:
                fail_count += 1
                
        except Exception as e:
            print(f"   ‚ùå Error: {e}")
            import traceback
            traceback.print_exc()
            fail_count += 1
    
    print(f"\n{'='*60}")
    print(f"‚úÖ Ho√†n th√†nh d·ªãch!")
    print(f"   Success: {success_count}/{len(da_details)}")
    print(f"   Skipped: {skip_count}/{len(da_details)}")
    print(f"   Failed: {fail_count}/{len(da_details)}")
    print(f"{'='*60}\n")


def list_articles_to_crawl(language=None, section=None, limit=None):
    """
    List t·∫•t c·∫£ articles c·∫ßn crawl
    """
    articles = get_articles_to_crawl(language=language, section=section, limit=limit)
    
    print(f"\nüìã Articles c·∫ßn crawl:")
    print(f"   Total: {len(articles)} articles")
    
    if language:
        print(f"   Language: {language}")
    if section:
        print(f"   Section: {section}")
    if limit:
        print(f"   Limit: {limit}")
    
    print(f"\n   Articles:")
    for i, article in enumerate(articles, 1):
        print(f"   {i}. ID: {article.id:6d} | {article.language:2s} | {article.section:12s} | {article.published_url[:70]}...")
    
    return articles


def handle_cookie_popup(sb):
    """
    X·ª≠ l√Ω cookie consent popup n·∫øu c√≥ xu·∫•t hi·ªán
    
    Args:
        sb: SeleniumBase instance
    """
    try:
        # Ki·ªÉm tra xem c√≥ popup kh√¥ng
        sb.sleep(1)  # ƒê·ª£i popup xu·∫•t hi·ªán
        
        # T√¨m v√† click button "ACCEPTER ALLE" (Accept All)
        # S·ª≠ d·ª•ng JavaScript ƒë·ªÉ t√¨m button c√≥ text ch·ª©a "ACCEPTER" ho·∫∑c "Accept"
        buttons = sb.execute_script("""
            var buttons = document.querySelectorAll('button');
            for (var i = 0; i < buttons.length; i++) {
                var text = buttons[i].textContent || buttons[i].innerText;
                if (text.includes('ACCEPTER') || text.includes('Accepter') || 
                    text.includes('ACCEPT') || text.includes('Accept')) {
                    return buttons[i];
                }
            }
            return null;
        """)
        
        if buttons:
            sb.execute_script("arguments[0].click();", buttons)
            print("   ‚úÖ Accepted cookie consent popup")
            sb.sleep(3)  # Wait for popup to close and page to stabilize
            
            # Ki·ªÉm tra xem popup ƒë√£ ƒë√≥ng ch∆∞a
            try:
                # ƒê·ª£i popup bi·∫øn m·∫•t
                sb.wait_for_element_not_visible('button:contains("ACCEPTER")', timeout=5)
            except:
                pass
            
            # ƒê·∫£m b·∫£o ƒëang ·ªü ƒë√∫ng trang login sau khi accept cookie
            current_url = sb.get_current_url()
            if 'login' not in current_url.lower():
                print("   üîÑ Reloading login page after cookie acceptance...")
                sb.open(LOGIN_URL)
                sb.sleep(3)  # Wait for page to load
            
            return True
        
        # Fallback: T√¨m b·∫±ng text content
        try:
            page_source = sb.get_page_source()
            if 'Du bestemmer over dine data' in page_source or 'cookie' in page_source.lower():
                # T√¨m button b·∫±ng xpath
                try:
                    accept_btn = sb.find_element('//button[contains(text(), "ACCEPTER") or contains(text(), "Accepter")]', timeout=3)
                    if accept_btn:
                        sb.click(accept_btn)
                        print("   ‚úÖ Accepted cookie consent popup (via XPath)")
                        sb.sleep(3)  # Wait for popup to close
                        
                        # ƒê·∫£m b·∫£o ƒëang ·ªü ƒë√∫ng trang login
                        current_url = sb.get_current_url()
                        if 'login' not in current_url.lower():
                            print("   üîÑ Reloading login page after cookie acceptance...")
                            sb.open(LOGIN_URL)
                            sb.sleep(3)
                        
                        return True
                except:
                    pass
        except:
            pass
        
        # N·∫øu kh√¥ng t√¨m th·∫•y popup, c√≥ th·ªÉ ƒë√£ ƒë∆∞·ª£c accept ho·∫∑c kh√¥ng c√≥
        return False
    except Exception as e:
        # Kh√¥ng c√≥ popup ho·∫∑c ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω
        return False


def ensure_login(sb):
    """
    ƒê·∫£m b·∫£o ƒë√£ login v√†o sermitsiaq.ag
    Ki·ªÉm tra b·∫±ng c√°ch m·ªü trang login v√† xem c√≥ n√∫t "Log ud" kh√¥ng
    N·∫øu ƒë√£ login, trang login s·∫Ω hi·ªÉn th·ªã "Du er allerede logget ind: Log ud"
    
    Args:
        sb: SeleniumBase instance
    """
    print("üîê Checking login status...")
    
    # M·ªü trang login ƒë·ªÉ ki·ªÉm tra
    try:
        sb.open(LOGIN_URL)
        sb.sleep(2)  # Wait for page to load
        
        # X·ª≠ l√Ω cookie popup tr∆∞·ªõc khi ki·ªÉm tra login
        handle_cookie_popup(sb)
        sb.sleep(1)  # Wait a bit after handling popup
        
        # Ki·ªÉm tra xem c√≥ n√∫t "Log ud" (Logout) ho·∫∑c text "Du er allerede logget ind" kh√¥ng
        # CH·ªà ki·ªÉm tra n·∫øu th·ª±c s·ª± ƒë√£ login (n√∫t logout ch·ªâ xu·∫•t hi·ªán khi ƒë√£ login)
        try:
            # T√¨m n√∫t logout
            logout_button = sb.find_element('button.logout', timeout=3)
            if logout_button:
                print("   ‚úÖ Already logged in (found Log ud button)")
                # Ch·ª•p m√†n h√¨nh ƒë·ªÉ x√°c nh·∫≠n
                screenshot_path = os.path.join(USER_DATA_DIR, 'login_check_logged_in.png')
                sb.save_screenshot(screenshot_path)
                print(f"   üì∏ Screenshot saved: {screenshot_path}")
                return True
        except:
            pass
        
        # Ki·ªÉm tra text "Du er allerede logget ind" trong page source
        try:
            page_source = sb.get_page_source()
            if 'Du er allerede logget ind' in page_source:
                # T√¨m n√∫t logout trong page source
                if 'button.logout' in page_source or 'class="logout"' in page_source:
                    print("   ‚úÖ Already logged in (found login status message with logout button)")
                    # Ch·ª•p m√†n h√¨nh ƒë·ªÉ x√°c nh·∫≠n
                    screenshot_path = os.path.join(USER_DATA_DIR, 'login_check_logged_in.png')
                    sb.save_screenshot(screenshot_path)
                    print(f"   üì∏ Screenshot saved: {screenshot_path}")
                    return True
        except:
            pass
        
        # N·∫øu kh√¥ng t√¨m th·∫•y logout button ho·∫∑c message, ch∆∞a login
        print("   ‚ö†Ô∏è  Not logged in (no Log ud button found), attempting login...")
        needs_login = True
        
    except Exception as e:
        print(f"   ‚ö†Ô∏è  Error checking login status: {e}, attempting login...")
        needs_login = True
    
    if needs_login:
        try:
            # ƒê·∫£m b·∫£o ƒëang ·ªü trang login (c√≥ th·ªÉ ƒë√£ m·ªü ·ªü tr√™n)
            current_url = sb.get_current_url()
            if 'login' not in current_url.lower():
                print(f"   üîë Navigating to {LOGIN_URL}...")
                sb.open(LOGIN_URL)
                sb.sleep(3)  # Wait for page to load
            
            # X·ª≠ l√Ω cookie popup tr∆∞·ªõc khi login
            handle_cookie_popup(sb)
            
            # ƒê·∫£m b·∫£o ƒëang ·ªü trang login v√† ƒë·ª£i form load
            current_url = sb.get_current_url()
            if 'login' not in current_url.lower():
                print(f"   üîÑ Navigating to {LOGIN_URL}...")
                sb.open(LOGIN_URL)
                sb.sleep(3)
            
            # Ki·ªÉm tra l·∫°i xem c√≥ ƒë√£ login ch∆∞a (c√≥ th·ªÉ ƒë√£ login trong l√∫c ch·ªù)
            try:
                logout_button = sb.find_element('button.logout', timeout=2)
                if logout_button:
                    print("   ‚úÖ Already logged in (found Log ud button after navigation)")
                    # Ch·ª•p m√†n h√¨nh ƒë·ªÉ x√°c nh·∫≠n
                    screenshot_path = os.path.join(USER_DATA_DIR, 'login_check_after_nav.png')
                    sb.save_screenshot(screenshot_path)
                    print(f"   üì∏ Screenshot saved: {screenshot_path}")
                    return True
            except:
                pass
            
            print(f"   üîë Logging in...")
            
            # Ch·ª•p m√†n h√¨nh tr∆∞·ªõc khi login ƒë·ªÉ debug
            screenshot_path = os.path.join(USER_DATA_DIR, 'before_login.png')
            sb.save_screenshot(screenshot_path)
            print(f"   üì∏ Screenshot before login: {screenshot_path}")
            
            # Form login n·∫±m trong iframe 0, c·∫ßn switch v√†o iframe tr∆∞·ªõc
            print("   üîÑ Switching to iframe containing login form...")
            try:
                # ƒê·ª£i iframe load
                sb.sleep(2)
                # Switch v√†o iframe 0 (iframe ƒë·∫ßu ti√™n ch·ª©a form login)
                sb.switch_to_frame(0)
                sb.sleep(2)  # ƒê·ª£i iframe content load
                print("   ‚úÖ Switched to iframe")
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Could not switch to iframe: {e}")
                # Th·ª≠ reload v√† switch l·∫°i
                sb.switch_to_default_content()
                sb.open(LOGIN_URL)
                sb.sleep(3)
                handle_cookie_popup(sb)
                sb.sleep(2)
                sb.switch_to_frame(0)
                sb.sleep(2)
            
            # ƒê·ª£i form login xu·∫•t hi·ªán trong iframe
            try:
                sb.wait_for_element('#id_subscriber', timeout=10)
            except:
                print("   ‚ö†Ô∏è  Login form not found in iframe, trying to reload...")
                sb.switch_to_default_content()
                sb.open(LOGIN_URL)
                sb.sleep(3)
                handle_cookie_popup(sb)
                sb.sleep(2)
                sb.switch_to_frame(0)
                sb.sleep(2)
                sb.wait_for_element('#id_subscriber', timeout=10)
            
            # Fill in email/subscriber field
            subscriber_input = sb.find_element('#id_subscriber', timeout=10)
            if subscriber_input:
                sb.type('#id_subscriber', LOGIN_EMAIL)
                print(f"   ‚úÖ Filled subscriber field")
            else:
                print("   ‚ùå Could not find subscriber input field")
                return False
            
            # Fill in password field
            password_input = sb.find_element('#id_password', timeout=10)
            if password_input:
                sb.type('#id_password', LOGIN_PASSWORD)
                print(f"   ‚úÖ Filled password field")
            else:
                print("   ‚ùå Could not find password input field")
                return False
            
            # Click login button
            login_button = sb.find_element('button[type="submit"]', timeout=10)
            if login_button:
                sb.click('button[type="submit"]')
                print(f"   ‚úÖ Clicked login button")
                sb.sleep(5)  # Wait for login to complete
                
                # Switch v·ªÅ default content ƒë·ªÉ ki·ªÉm tra login status
                sb.switch_to_default_content()
                sb.sleep(2)
                
                # Ki·ªÉm tra xem ƒë√£ login th√†nh c√¥ng ch∆∞a b·∫±ng c√°ch t√¨m n√∫t "Log ud"
                try:
                    logout_button = sb.find_element('button.logout', timeout=3)
                    if logout_button:
                        print("   ‚úÖ Login successful! (found Log ud button)")
                        # Ch·ª•p m√†n h√¨nh ƒë·ªÉ x√°c nh·∫≠n
                        screenshot_path = os.path.join(USER_DATA_DIR, 'login_success.png')
                        sb.save_screenshot(screenshot_path)
                        print(f"   üì∏ Screenshot saved: {screenshot_path}")
                        return True
                except:
                    pass
                
                # Fallback: ki·ªÉm tra page source
                try:
                    page_source = sb.get_page_source()
                    if 'Du er allerede logget ind' in page_source or 'Log ud' in page_source:
                        print("   ‚úÖ Login successful! (found login status message)")
                        # Ch·ª•p m√†n h√¨nh ƒë·ªÉ x√°c nh·∫≠n
                        screenshot_path = os.path.join(USER_DATA_DIR, 'login_success.png')
                        sb.save_screenshot(screenshot_path)
                        print(f"   üì∏ Screenshot saved: {screenshot_path}")
                        return True
                except:
                    pass
                
                # Fallback: ki·ªÉm tra URL
                current_url = sb.get_current_url()
                if 'login' not in current_url.lower():
                    print("   ‚úÖ Login successful! (redirected away from login page)")
                    # Ch·ª•p m√†n h√¨nh ƒë·ªÉ x√°c nh·∫≠n
                    screenshot_path = os.path.join(USER_DATA_DIR, 'login_success_redirect.png')
                    sb.save_screenshot(screenshot_path)
                    print(f"   üì∏ Screenshot saved: {screenshot_path}")
                    return True
                else:
                    print("   ‚ùå Login failed - still on login page")
                    # Ch·ª•p m√†n h√¨nh ƒë·ªÉ debug
                    screenshot_path = os.path.join(USER_DATA_DIR, 'login_failed.png')
                    sb.save_screenshot(screenshot_path)
                    print(f"   üì∏ Screenshot saved: {screenshot_path}")
                    return False
            else:
                print("   ‚ùå Could not find login button")
                return False
                
        except Exception as e:
            print(f"   ‚ùå Error during login: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    return True


def crawl_article_detail(url: str, language: str = 'da', headless: bool = True):
    """
    Crawl article detail page v√† l∆∞u v√†o database
    
    Args:
        url: URL c·ªßa article detail page
        language: Language code ('da', 'kl', 'en')
        headless: Run browser in headless mode
    
    Returns:
        ArticleDetail object or None
    """
    print(f"üîç Crawling: {url[:70]}...")
    
    # T·∫°o user_data_dir n·∫øu ch∆∞a t·ªìn t·∫°i
    os.makedirs(USER_DATA_DIR, exist_ok=True)
    
    with SB(uc=True, headless=headless, user_data_dir=USER_DATA_DIR) as sb:
        # ƒê·∫£m b·∫£o ƒë√£ login tr∆∞·ªõc khi crawl (ch·ªâ login 1 l·∫ßn cho batch)
        # Note: Login s·∫Ω ƒë∆∞·ª£c th·ª±c hi·ªán ·ªü batch_crawl_articles, kh√¥ng c·∫ßn login l·∫°i m·ªói article
        try:
            # Navigate to URL
            sb.open(url)
            sb.sleep(2)  # Wait for page to load
            
            # Wait for bodytext content
            try:
                sb.wait_for_element('.bodytext', timeout=10)
            except:
                print(f"   ‚ö†Ô∏è  Timeout waiting for .bodytext")
                return None
            
            # Get article title
            title = None
            try:
                title_elem = sb.find_element('h1.headline.mainTitle', timeout=5)
                if title_elem:
                    title = title_elem.text
            except:
                pass
            
            # Get excerpt
            excerpt = None
            try:
                excerpt_elem = sb.find_element('h2.subtitle', timeout=5)
                if excerpt_elem:
                    excerpt = excerpt_elem.text
            except:
                pass
            
            # Get article meta (bylines and dates)
            meta_html = None
            try:
                article_header = sb.find_element('.articleHeader', timeout=5)
                if article_header:
                    meta_elem = article_header.find_element('.meta', timeout=3)
                    if meta_elem:
                        meta_html = meta_elem.get_attribute('outerHTML')
            except:
                # Fallback: parse from page source
                try:
                    from bs4 import BeautifulSoup
                    page_source = sb.get_page_source()
                    soup = BeautifulSoup(page_source, 'html.parser')
                    article_header = soup.find('div', class_='articleHeader')
                    if article_header:
                        meta_div = article_header.find('div', class_='meta')
                        if meta_div:
                            meta_html = str(meta_div)
                except:
                    pass
            
            # Get bodytext HTML
            bodytext_html = None
            try:
                page_source = sb.get_page_source()
                from bs4 import BeautifulSoup
                soup = BeautifulSoup(page_source, 'html.parser')
                
                bodytext_div = soup.find('div', class_='bodytext', attrs={'data-element-guid': True})
                if bodytext_div:
                    bodytext_html = str(bodytext_div)
                else:
                    # Fallback: try Selenium element
                    try:
                        bodytext_elem = sb.find_element('div.bodytext.large-12', timeout=5)
                        if bodytext_elem:
                            bodytext_html = bodytext_elem.get_attribute('outerHTML')
                    except:
                        pass
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Error parsing bodytext: {e}")
                return None
            
            # Get paywall offers section
            offers_html = None
            try:
                offers_elem = sb.find_element('.iteras-offers', timeout=5)
                if offers_elem:
                    offers_html = offers_elem.get_attribute('outerHTML')
            except:
                pass
            
            # Get article footer tags section
            footer_html = None
            try:
                footer_elem = sb.find_element('.articleFooter', timeout=5)
                if footer_elem:
                    footer_html = footer_elem.get_attribute('outerHTML')
            except:
                # Fallback: parse from page source
                try:
                    from bs4 import BeautifulSoup
                    page_source = sb.get_page_source()
                    soup = BeautifulSoup(page_source, 'html.parser')
                    footer_div = soup.find('div', class_='articleFooter')
                    if footer_div:
                        footer_html = str(footer_div)
                except:
                    pass
            
            # Combine HTML
            full_html = ''
            if meta_html:
                full_html = meta_html
            if bodytext_html:
                full_html += bodytext_html
            if offers_html:
                full_html += offers_html
            if footer_html:
                full_html += footer_html
            
            if not full_html:
                print(f"   ‚ùå Could not find content")
                return None
            
            # Get element_guid from bodytext
            element_guid = None
            if bodytext_html:
                from bs4 import BeautifulSoup
                soup = BeautifulSoup(bodytext_html, 'html.parser')
                bodytext_div = soup.find('div', class_='bodytext')
                if bodytext_div:
                    element_guid = bodytext_div.get('data-element-guid', '')
            
            # Parse and save to database
            article_detail = ArticleDetailParser.save_article_detail(
                published_url=url,
                html_content=full_html,
                language=language,
                element_guid=element_guid
            )
            
            print(f"   ‚úÖ Saved (Detail ID: {article_detail.id}, Blocks: {len(article_detail.content_blocks)})")
            
            # Update article if exists
            article = Article.query.filter_by(published_url=url).first()
            if article:
                if title and not article.title:
                    article.title = title
                if excerpt and not article.excerpt:
                    article.excerpt = excerpt
                db.session.commit()
            
            # N·∫øu l√† article_detail DA (kh√¥ng ph·∫£i kl.sermitsiaq.ag), t·ª± ƒë·ªông t·∫°o EN version
            if language == 'da' and 'kl.sermitsiaq.ag' not in url:
                try:
                    en_url = convert_da_url_to_en_url(url)
                    existing_en = ArticleDetail.query.filter_by(published_url=en_url, language='en').first()
                    if not existing_en:
                        print(f"   üåê Auto-creating EN version...")
                        create_en_article_detail_from_da(article_detail)
                except Exception as e:
                    print(f"   ‚ö†Ô∏è  Error creating EN version: {e}")
            
            return article_detail
                
        except Exception as e:
            print(f"   ‚ùå Error: {e}")
            import traceback
            traceback.print_exc()
            return None


def crawl_all(language=None, section=None, limit=None, headless=True, delay=2):
    """
    Crawl t·∫•t c·∫£ articles ch∆∞a c√≥ detail
    
    Args:
        language: Filter theo language
        section: Filter theo section
        limit: Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng
        headless: Run browser in headless mode
        delay: Delay gi·ªØa c√°c requests (seconds)
    """
    articles = get_articles_to_crawl(language=language, section=section, limit=limit)
    
    if not articles:
        print("\n‚úÖ Kh√¥ng c√≥ articles n√†o c·∫ßn crawl!")
        return
    
    print(f"\nüöÄ B·∫Øt ƒë·∫ßu crawl {len(articles)} articles...")
    if language:
        print(f"   Language: {language}")
    if section:
        print(f"   Section: {section}")
    if limit:
        print(f"   Limit: {limit}")
    print(f"   Headless: {headless}")
    print(f"   Delay: {delay}s gi·ªØa c√°c requests\n")
    
    # T·∫°o user_data_dir n·∫øu ch∆∞a t·ªìn t·∫°i
    os.makedirs(USER_DATA_DIR, exist_ok=True)
    
    # Login m·ªôt l·∫ßn tr∆∞·ªõc khi b·∫Øt ƒë·∫ßu crawl (s·ª≠ d·ª•ng user_data_dir ƒë·ªÉ l∆∞u session)
    print("üîê Initializing browser session with login...")
    with SB(uc=True, headless=headless, user_data_dir=USER_DATA_DIR) as sb:
        if not ensure_login(sb):
            print("‚ùå Failed to login, cannot proceed with crawling")
            return
    
    # Sau khi login xong, session ƒë√£ ƒë∆∞·ª£c l∆∞u trong user_data_dir
    # C√°c l·∫ßn crawl ti·∫øp theo s·∫Ω t·ª± ƒë·ªông s·ª≠ d·ª•ng session ƒë√£ l∆∞u
    print("‚úÖ Login session saved, starting to crawl articles...\n")
    
    success_count = 0
    fail_count = 0
    
    for i, article in enumerate(articles, 1):
        print(f"\n[{i}/{len(articles)}] Article ID: {article.id}")
        
        result = crawl_article_detail(
            url=article.published_url,
            language=article.language,
            headless=headless
        )
        
        if result:
            success_count += 1
        else:
            fail_count += 1
        
        # Delay gi·ªØa c√°c requests
        if i < len(articles):
            time.sleep(delay)
    
    print(f"\n{'='*60}")
    print(f"‚úÖ Ho√†n th√†nh!")
    print(f"   Success: {success_count}/{len(articles)}")
    print(f"   Failed: {fail_count}/{len(articles)}")
    print(f"{'='*60}\n")


def main():
    parser = argparse.ArgumentParser(
        description='Crawl article detail pages theo batch',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # List t·∫•t c·∫£ articles c·∫ßn crawl
  python scripts/crawl_article_details_batch.py --list
  
  # Crawl t·∫•t c·∫£ articles ch∆∞a c√≥ detail
  python scripts/crawl_article_details_batch.py --crawl-all
  
  # Crawl theo language
  python scripts/crawl_article_details_batch.py --crawl-all --language en
  
  # Crawl theo section
  python scripts/crawl_article_details_batch.py --crawl-all --section samfund
  
  # Crawl gi·ªõi h·∫°n s·ªë l∆∞·ª£ng
  python scripts/crawl_article_details_batch.py --crawl-all --limit 10
        """
    )
    
    parser.add_argument('--list', action='store_true',
                        help='List t·∫•t c·∫£ articles c·∫ßn crawl')
    parser.add_argument('--crawl-all', action='store_true',
                        help='Crawl t·∫•t c·∫£ articles ch∆∞a c√≥ detail')
    parser.add_argument('--language', '-l', choices=['da', 'kl', 'en'],
                        help='Filter theo language')
    parser.add_argument('--section', '-s',
                        help='Filter theo section (samfund, sport, kultur, etc.)')
    parser.add_argument('--limit', '-n', type=int,
                        help='Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng articles')
    parser.add_argument('--no-headless', action='store_true',
                        help='Run browser in no-headless mode (ƒë·ªÉ debug)')
    parser.add_argument('--delay', '-d', type=float, default=2.0,
                        help='Delay gi·ªØa c√°c requests (seconds, default: 2.0)')
    parser.add_argument('--translate-da-to-en', action='store_true',
                        help='D·ªãch article_detail t·ª´ DA sang EN sau khi crawl')
    parser.add_argument('--translate-only', action='store_true',
                        help='Ch·ªâ d·ªãch c√°c article_detail DA ƒë√£ c√≥, kh√¥ng crawl m·ªõi')
    parser.add_argument('--translate-limit', type=int,
                        help='Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng article_detail ƒë·ªÉ d·ªãch')
    
    args = parser.parse_args()
    
    if not args.list and not args.crawl_all:
        parser.print_help()
        return
    
    with app.app_context():
        if args.translate_only:
            # Ch·ªâ d·ªãch, kh√¥ng crawl
            translate_da_article_details_to_en(limit=args.translate_limit)
        elif args.list:
            list_articles_to_crawl(
                language=args.language,
                section=args.section,
                limit=args.limit
            )
        elif args.crawl_all:
            crawl_all(
                language=args.language,
                section=args.section,
                limit=args.limit,
                headless=not args.no_headless,
                delay=args.delay
            )
            
            # D·ªãch DA sang EN sau khi crawl n·∫øu ƒë∆∞·ª£c y√™u c·∫ßu
            if args.translate_da_to_en:
                print("\n" + "="*60)
                print("üåê Starting translation from DA to EN...")
                print("="*60 + "\n")
                translate_da_article_details_to_en(limit=args.translate_limit)


if __name__ == '__main__':
    main()

