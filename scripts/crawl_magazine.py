"""
Script ƒë·ªÉ crawl magazine t·ª´ e-pages.pub
- V√†o link magazine
- Click v√†o button Next ƒë·ªÉ navigate qua c√°c pages
- Thu th·∫≠p data-article v√† l∆∞u v√†o CSV

Usage:
    python scripts/crawl_magazine.py
    python scripts/crawl_magazine.py --no-headless  # ƒê·ªÉ xem browser
    python scripts/crawl_magazine.py --magazine-start 7 --magazine-end 164  # Ch·∫°y t·ª´ magazine 7 ƒë·∫øn 164
"""
import sys
import os
import argparse
import time
import csv
import re
from collections import OrderedDict

# Add parent directory to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from seleniumbase import SB

# User data directory ƒë·ªÉ l∆∞u session (d√πng chung v·ªõi crawl script)
USER_DATA_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'user_data')
LOGIN_EMAIL = "aluu@greenland.org"
LOGIN_PASSWORD = "LEn924924jfkjfk"

# Base URL template
BASE_URL_TEMPLATE = "https://sermitsiaq.e-pages.pub/titles/ag/13765/publications/{magazine_id}/pages/1"


def handle_login_if_needed(sb):
    """
    Ki·ªÉm tra v√† x·ª≠ l√Ω login n·∫øu page y√™u c·∫ßu ƒëƒÉng nh·∫≠p
    Form login n·∫±m trong shadow DOM c·ªßa element LOGIN-DIALOG
    
    Args:
        sb: SeleniumBase instance
    
    Returns:
        bool: True n·∫øu ƒë√£ login th√†nh c√¥ng ho·∫∑c kh√¥ng c·∫ßn login, False n·∫øu login th·∫•t b·∫°i
    """
    try:
        print("üîç Checking for login form...")
        sb.sleep(5)  # ƒê·ª£i JS render
        
        # Ki·ªÉm tra xem c√≥ form login kh√¥ng
        needs_login = False
        try:
            page_source = sb.get_page_source()
            if 'Er du allerede abonnent' in page_source:
                needs_login = True
                print("üîê Found login form in page source")
        except:
            pass
        
        if not needs_login:
            print("‚úÖ No login required")
            return True
        
        print("üîê Login form detected, attempting to login...")
        
        # T√¨m v√† ƒëi·ªÅn form trong shadow DOM c·ªßa LOGIN-DIALOG
        login_success = sb.execute_script("""
            var email = arguments[0];
            var password = arguments[1];
            
            // T√¨m LOGIN-DIALOG element
            var loginDialogs = document.querySelectorAll('login-dialog');
            for (var i = 0; i < loginDialogs.length; i++) {
                var loginDialog = loginDialogs[i];
                if (loginDialog.shadowRoot) {
                    var shadowRoot = loginDialog.shadowRoot;
                    
                    // T√¨m email input
                    var emailInput = shadowRoot.querySelector('input[type="email"], input[name="email"]');
                    if (emailInput && emailInput.offsetParent !== null) {
                        emailInput.value = email;
                        emailInput.dispatchEvent(new Event('input', { bubbles: true }));
                        emailInput.dispatchEvent(new Event('change', { bubbles: true }));
                        
                        // T√¨m password input
                        var passwordInput = shadowRoot.querySelector('input[type="password"], input[name="password"]');
                        if (passwordInput && passwordInput.offsetParent !== null) {
                            passwordInput.value = password;
                            passwordInput.dispatchEvent(new Event('input', { bubbles: true }));
                            passwordInput.dispatchEvent(new Event('change', { bubbles: true }));
                            
                            // T√¨m v√† click submit button
                            var submitButton = shadowRoot.querySelector('button.login-button, button[type="submit"]');
                            if (!submitButton) {
                                // Fallback: t√¨m button c√≥ text "Log p√•"
                                var buttons = shadowRoot.querySelectorAll('button');
                                for (var j = 0; j < buttons.length; j++) {
                                    var btn = buttons[j];
                                    var text = btn.textContent || btn.innerText || '';
                                    if (text.includes('Log p√•') && btn.offsetParent !== null) {
                                        submitButton = btn;
                                        break;
                                    }
                                }
                            }
                            
                            if (submitButton && submitButton.offsetParent !== null) {
                                submitButton.click();
                                return true;
                            }
                        }
                    }
                }
            }
            return false;
        """, LOGIN_EMAIL, LOGIN_PASSWORD)
        
        if login_success:
            print("‚úÖ Filled form and clicked submit")
        else:
            print("‚ö†Ô∏è  Could not fill form or click submit")
        
        # ƒê·ª£i sau khi submit
        sb.sleep(5)
        
        # Ki·ªÉm tra login th√†nh c√¥ng
        try:
            page_source = sb.get_page_source()
            if 'Du er allerede logget ind' in page_source or 'Log ud' in page_source:
                print("‚úÖ Login successful!")
                return True
        except:
            pass
        
        # Ki·ªÉm tra xem form login c√≤n kh√¥ng
        try:
            # Ki·ªÉm tra trong shadow DOM
            form_still_present = sb.execute_script("""
                var loginDialogs = document.querySelectorAll('login-dialog');
                for (var i = 0; i < loginDialogs.length; i++) {
                    if (loginDialogs[i].shadowRoot) {
                        var passwordInput = loginDialogs[i].shadowRoot.querySelector('input[type="password"]');
                        if (passwordInput && passwordInput.offsetParent !== null) {
                            return true;
                        }
                    }
                }
                return false;
            """)
            
            if form_still_present:
                print("‚ö†Ô∏è  Login form still present, login may have failed")
                return False
            else:
                print("‚úÖ Login successful (login form no longer present)")
                return True
        except:
            print("‚úÖ Login successful (could not verify)")
            return True
        
    except Exception as e:
        print(f"‚ö†Ô∏è  Error during login: {e}")
        import traceback
        traceback.print_exc()
        return True


def extract_magazine_id_from_url(url: str) -> int:
    """
    Extract magazine ID t·ª´ URL
    V√≠ d·ª•: https://sermitsiaq.e-pages.pub/titles/ag/13765/publications/7/pages/1 -> 7
    
    Args:
        url: Magazine URL
    
    Returns:
        int: Magazine ID ho·∫∑c None n·∫øu kh√¥ng t√¨m th·∫•y
    """
    match = re.search(r'/publications/(\d+)/', url)
    if match:
        return int(match.group(1))
    return None


def get_crawled_magazines(output_file: str = 'magazine_articles.csv') -> set:
    """
    ƒê·ªçc CSV file ƒë·ªÉ l·∫•y danh s√°ch magazine_id ƒë√£ crawl
    
    Args:
        output_file: T√™n file CSV
    
    Returns:
        set: Set c√°c magazine_id ƒë√£ crawl
    """
    crawled_magazines = set()
    
    if not os.path.exists(output_file):
        return crawled_magazines
    
    try:
        with open(output_file, 'r', encoding='utf-8') as csvfile:
            reader = csv.DictReader(csvfile)
            for row in reader:
                magazine_id = row.get('magazine_id', '').strip()
                if magazine_id and magazine_id.isdigit():
                    crawled_magazines.add(int(magazine_id))
    except Exception as e:
        print(f"‚ö†Ô∏è  Error reading {output_file}: {e}")
    
    return crawled_magazines


def save_articles_to_csv(articles: list, output_file: str = 'magazine_articles.csv'):
    """
    L∆∞u articles v√†o CSV file, drop duplicate d·ª±a tr√™n data_article
    
    Args:
        articles: List c√°c articles
        output_file: T√™n file CSV output
    """
    if not articles:
        print("‚ö†Ô∏è  No articles to save")
        return
    
    # Drop duplicate d·ª±a tr√™n data_article, gi·ªØ l·∫°i entry ƒë·∫ßu ti√™n
    seen = OrderedDict()
    for article in articles:
        data_article = article.get('data_article', '')
        if data_article and data_article not in seen:
            seen[data_article] = article
    
    unique_articles = list(seen.values())
    
    # Sort by magazine_id, then by data_article
    unique_articles.sort(key=lambda x: (x.get('magazine_id', 0), x.get('data_article', '')))
    
    # Write to CSV
    fieldnames = ['magazine_id', 'data_article', 'data_external_id', 'aria_label', 'lang', 
                  'tag_name', 'class_name', 'page_number', 'url']
    
    file_exists = os.path.exists(output_file)
    
    # ƒê·ªçc c√°c articles hi·ªán c√≥ ƒë·ªÉ tr√°nh duplicate khi append
    existing_articles = set()
    if file_exists:
        try:
            with open(output_file, 'r', encoding='utf-8') as csvfile:
                reader = csv.DictReader(csvfile)
                for row in reader:
                    data_article = row.get('data_article', '').strip()
                    magazine_id = row.get('magazine_id', '').strip()
                    if data_article and magazine_id:
                        existing_articles.add((magazine_id, data_article))
        except:
            pass
    
    # Filter out articles ƒë√£ t·ªìn t·∫°i
    new_articles = []
    for article in unique_articles:
        key = (str(article.get('magazine_id', '')), article.get('data_article', ''))
        if key not in existing_articles:
            new_articles.append(article)
    
    if not new_articles:
        print(f"‚ö†Ô∏è  All articles already exist in {output_file}")
        return
    
    with open(output_file, 'a', newline='', encoding='utf-8') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        
        if not file_exists:
            writer.writeheader()
        
        for article in new_articles:
            row = {field: article.get(field, '') for field in fieldnames}
            writer.writerow(row)
    
    print(f"‚úÖ Saved {len(new_articles)} new unique articles to {output_file}")
    print(f"   Total duplicates removed: {len(articles) - len(unique_articles)}")


def crawl_magazine(magazine_id: int, headless: bool = True, max_pages: int = None):
    """
    Crawl magazine page v√† click Next button, thu th·∫≠p data-article t·ª´ m·ªói page
    
    Args:
        magazine_id: Magazine ID (v√≠ d·ª•: 7, 162, ...)
        headless: Run browser in headless mode
        max_pages: S·ªë l∆∞·ª£ng pages t·ªëi ƒëa ƒë·ªÉ crawl (None = kh√¥ng gi·ªõi h·∫°n)
    
    Returns:
        list: Danh s√°ch c√°c articles v·ªõi data-article
    """
    magazine_url = BASE_URL_TEMPLATE.format(magazine_id=magazine_id)
    print(f"\n{'='*60}")
    print(f"üîç Opening magazine #{magazine_id}: {magazine_url}")
    print(f"{'='*60}")
    
    # List ƒë·ªÉ l∆∞u t·∫•t c·∫£ articles
    all_articles = []
    
    # T·∫°o user_data_dir n·∫øu ch∆∞a t·ªìn t·∫°i
    os.makedirs(USER_DATA_DIR, exist_ok=True)
    
    with SB(uc=True, headless=headless, user_data_dir=USER_DATA_DIR) as sb:
        try:
            # Navigate to magazine URL
            print(f"üìñ Navigating to: {magazine_url}")
            sb.open(magazine_url)
            sb.sleep(3)  # Wait for page to load
            
            # Ki·ªÉm tra v√† x·ª≠ l√Ω login n·∫øu c·∫ßn
            if not handle_login_if_needed(sb):
                print("‚ùå Login failed, cannot proceed")
                return
            
            print("‚úÖ Page loaded successfully")
            
            # T√¨m v√† click button "Next" (c√≥ th·ªÉ n·∫±m trong shadow DOM ho·∫∑c ph·∫ßn ·∫©n)
            # ƒê·ª£i ƒë·ªÉ c√°c custom elements ƒë∆∞·ª£c render
            sb.sleep(3)
            
            # Helper function ƒë·ªÉ extract articles t·ª´ page hi·ªán t·∫°i
            def extract_articles_from_current_page(page_num):
                """Extract articles t·ª´ page hi·ªán t·∫°i"""
                # T√¨m v√† hi·ªÉn th·ªã book-navigation trong shadow DOM c·ªßa view-publication
                print("üîç Activating book-navigation...")
                navigation_found = sb.execute_script("""
                // T√¨m book-navigation trong shadow DOM c·ªßa view-publication
                var viewPublications = document.querySelectorAll('view-publication');
                for (var i = 0; i < viewPublications.length; i++) {
                    if (viewPublications[i].shadowRoot) {
                        var bookNav = viewPublications[i].shadowRoot.querySelector('book-navigation');
                        if (bookNav) {
                            // Remove class "hideSectionNavigationSpread" ƒë·ªÉ hi·ªÉn th·ªã navigation
                            if (bookNav.classList.contains('hideSectionNavigationSpread')) {
                                bookNav.classList.remove('hideSectionNavigationSpread');
                            }
                            
                            // ƒê·∫£m b·∫£o c√≥ class "visible"
                            if (!bookNav.classList.contains('visible')) {
                                bookNav.classList.add('visible');
                            }
                            
                            // Hi·ªÉn th·ªã navbar trong shadow DOM c·ªßa book-navigation
                            if (bookNav.shadowRoot) {
                                var navbar = bookNav.shadowRoot.querySelector('.navbar');
                                if (navbar) {
                                    // Remove class "out" ƒë·ªÉ hi·ªÉn th·ªã navbar
                                    if (navbar.classList.contains('out')) {
                                        navbar.classList.remove('out');
                                    }
                                }
                            }
                            
                            return {found: true};
                        }
                    }
                }
                return {found: false};
            """)
                
                if navigation_found and navigation_found.get('found'):
                    print("   ‚úÖ Activated book-navigation")
                    sb.sleep(2)  # ƒê·ª£i navigation bar hi·ªÉn th·ªã
                else:
                    print("   ‚ö†Ô∏è  book-navigation not found")
                
                # Unhide book-page elements tr∆∞·ªõc khi t√¨m articles
                print("üîç Unhiding book-page elements...")
                sb.execute_script("""
                    // T√¨m book-page trong shadow DOM c·ªßa view-publication
                    var viewPublications = document.querySelectorAll('view-publication');
                    for (var i = 0; i < viewPublications.length; i++) {
                        if (viewPublications[i].shadowRoot) {
                            var shadowRoot = viewPublications[i].shadowRoot;
                            
                            // T√¨m t·∫•t c·∫£ book-page elements
                            var bookPages = shadowRoot.querySelectorAll('book-page');
                            for (var j = 0; j < bookPages.length; j++) {
                                var bookPage = bookPages[j];
                                
                                // Remove aria-hidden v√† inert ƒë·ªÉ hi·ªÉn th·ªã
                                if (bookPage.hasAttribute('aria-hidden')) {
                                    bookPage.removeAttribute('aria-hidden');
                                }
                                if (bookPage.hasAttribute('inert')) {
                                    bookPage.removeAttribute('inert');
                                }
                                
                                // ƒê·∫£m b·∫£o c√≥ class "current" ho·∫∑c "initialized"
                                if (!bookPage.classList.contains('initialized')) {
                                    bookPage.classList.add('initialized');
                                }
                                
                                // Set style ƒë·ªÉ ƒë·∫£m b·∫£o hi·ªÉn th·ªã
                                bookPage.style.display = '';
                                bookPage.style.visibility = '';
                                bookPage.style.opacity = '';
                                
                                // N·∫øu c√≥ shadowRoot, c≈©ng unhide c√°c elements b√™n trong
                                if (bookPage.shadowRoot) {
                                    var pageElements = bookPage.shadowRoot.querySelectorAll('*');
                                    for (var k = 0; k < pageElements.length; k++) {
                                        var el = pageElements[k];
                                        if (el.hasAttribute('aria-hidden')) {
                                            el.removeAttribute('aria-hidden');
                                        }
                                        if (el.hasAttribute('inert')) {
                                            el.removeAttribute('inert');
                                        }
                                    }
                                }
                            }
                        }
                    }
                """)
                sb.sleep(1)  # ƒê·ª£i elements hi·ªÉn th·ªã
                
                # T√¨m v√† l∆∞u c√°c elements c√≥ data-article t·ª´ page hi·ªán t·∫°i
                print("üîç Finding elements with data-article...")
                articles = sb.execute_script("""
                    var result = [];
                    
                    // T√¨m trong main document
                    var mainElements = document.querySelectorAll('[data-article]');
                    for (var i = 0; i < mainElements.length; i++) {
                        var el = mainElements[i];
                        var dataArticle = el.getAttribute('data-article') || '';
                        var dataExternalId = el.getAttribute('data-external_id') || '';
                        var ariaLabel = el.getAttribute('aria-label') || '';
                        var lang = el.getAttribute('lang') || '';
                        var tagName = el.tagName || '';
                        var className = el.className || '';
                        
                        result.push({
                            data_article: dataArticle,
                            data_external_id: dataExternalId,
                            aria_label: ariaLabel,
                            lang: lang,
                            tag_name: tagName,
                            class_name: className
                        });
                    }
                    
                    // T√¨m trong shadow DOM c·ªßa view-publication
                    var viewPublications = document.querySelectorAll('view-publication');
                    for (var i = 0; i < viewPublications.length; i++) {
                        if (viewPublications[i].shadowRoot) {
                            var shadowRoot = viewPublications[i].shadowRoot;
                            
                            // T√¨m trong book-page elements
                            var bookPages = shadowRoot.querySelectorAll('book-page');
                            for (var j = 0; j < bookPages.length; j++) {
                                var bookPage = bookPages[j];
                                
                                // T√¨m trong book-page (c√≥ th·ªÉ c√≥ shadowRoot ho·∫∑c kh√¥ng)
                                var pageElements = [];
                                if (bookPage.shadowRoot) {
                                    pageElements = bookPage.shadowRoot.querySelectorAll('[data-article]');
                                } else {
                                    // N·∫øu kh√¥ng c√≥ shadowRoot, t√¨m tr·ª±c ti·∫øp trong book-page
                                    pageElements = bookPage.querySelectorAll('[data-article]');
                                }
                                
                                for (var k = 0; k < pageElements.length; k++) {
                                    var el = pageElements[k];
                                    var dataArticle = el.getAttribute('data-article') || '';
                                    var dataExternalId = el.getAttribute('data-external_id') || '';
                                    var ariaLabel = el.getAttribute('aria-label') || '';
                                    var lang = el.getAttribute('lang') || '';
                                    var tagName = el.tagName || '';
                                    var className = el.className || '';
                                    
                                    result.push({
                                        data_article: dataArticle,
                                        data_external_id: dataExternalId,
                                        aria_label: ariaLabel,
                                        lang: lang,
                                        tag_name: tagName,
                                        class_name: className
                                    });
                                }
                            }
                            
                            // T√¨m tr·ª±c ti·∫øp trong shadowRoot c·ªßa view-publication
                            var shadowElements = shadowRoot.querySelectorAll('[data-article]');
                            for (var m = 0; m < shadowElements.length; m++) {
                                var el = shadowElements[m];
                                var dataArticle = el.getAttribute('data-article') || '';
                                var dataExternalId = el.getAttribute('data-external_id') || '';
                                var ariaLabel = el.getAttribute('aria-label') || '';
                                var lang = el.getAttribute('lang') || '';
                                var tagName = el.tagName || '';
                                var className = el.className || '';
                                
                                result.push({
                                    data_article: dataArticle,
                                    data_external_id: dataExternalId,
                                    aria_label: ariaLabel,
                                    lang: lang,
                                    tag_name: tagName,
                                    class_name: className
                                });
                            }
                        }
                    }
                    
                    return result;
                """)
                
                if articles:
                    print(f"   üìä Found {len(articles)} article(s) with data-article on page {page_num}:")
                    for idx, article in enumerate(articles):
                        print(f"      Article {idx + 1}:")
                        print(f"         - data-article: '{article.get('data_article', '')}'")
                        print(f"         - data-external_id: '{article.get('data_external_id', '')}'")
                        print(f"         - aria-label: '{article.get('aria_label', '')}'")
                        print(f"         - lang: '{article.get('lang', '')}'")
                        print(f"         - tag: '{article.get('tag_name', '')}'")
                    
                    # Th√™m page number, magazine_id v√† URL v√†o m·ªói article v√† l∆∞u v√†o list
                    for article in articles:
                        article['page_number'] = page_num
                        article['magazine_id'] = magazine_id
                        article['url'] = sb.get_current_url()
                    return articles
                else:
                    print("   ‚ö†Ô∏è  No articles found with data-article attribute")
                    return []
            
            # Page 1: Extract articles t·ª´ page ƒë·∫ßu ti√™n
            page_count = 1
            print(f"\nüìÑ Processing page {page_count}...")
            articles_page1 = extract_articles_from_current_page(page_count)
            if articles_page1:
                all_articles.extend(articles_page1)
            
            # Click Next button 1 l·∫ßn
            print("\nüîç Looking for 'Next' button...")
            next_clicked = sb.execute_script("""
                // T√¨m book-navigation trong shadow DOM c·ªßa view-publication
                var viewPublications = document.querySelectorAll('view-publication');
                for (var i = 0; i < viewPublications.length; i++) {
                    if (viewPublications[i].shadowRoot) {
                        var bookNav = viewPublications[i].shadowRoot.querySelector('book-navigation');
                        if (bookNav && bookNav.shadowRoot) {
                            var buttons = bookNav.shadowRoot.querySelectorAll('button, [role="button"]');
                            
                            for (var j = 0; j < buttons.length; j++) {
                                var btn = buttons[j];
                                var ariaLabel = (btn.getAttribute('aria-label') || '').trim();
                                var dataType = (btn.getAttribute('data-type') || '').trim();
                                var disabled = btn.hasAttribute('disabled');
                                
                                // T√¨m button "N√¶ste side" v·ªõi data-type="next" v√† kh√¥ng b·ªã disabled
                                if (dataType === 'next' && !disabled && ariaLabel.toLowerCase().includes('n√¶ste side')) {
                                    btn.click();
                                    return {success: true, method: 'book-navigation', ariaLabel: ariaLabel};
                                }
                            }
                        }
                    }
                }
                return {success: false, message: 'Could not find Next button'};
            """)
            
            if not next_clicked or not next_clicked.get('success'):
                print("‚ö†Ô∏è  Could not find 'Next' button - only page 1 will be processed")
            else:
                print(f"‚úÖ Clicked 'Next' button, waiting for page to load...")
                sb.sleep(3)  # Wait for navigation
                
                # Page 2: Extract articles t·ª´ page sau khi click Next
                page_count = 2
                print(f"\nüìÑ Processing page {page_count}...")
                articles_page2 = extract_articles_from_current_page(page_count)
                if articles_page2:
                    all_articles.extend(articles_page2)
                
                # Ki·ªÉm tra xem URL c√≥ thay ƒë·ªïi kh√¥ng (ƒë·ªÉ x√°c nh·∫≠n ƒë√£ chuy·ªÉn page)
                new_url = sb.get_current_url()
                print(f"   Current URL: {new_url}")
            
            # T·ªïng k·∫øt
            print(f"\n‚úÖ Crawling completed!")
            print(f"   Total pages processed: {page_count}")
            print(f"   Total articles found: {len(all_articles)}")
            
            return all_articles
            
        except Exception as e:
            print(f"‚ùå Error during crawling: {e}")
            import traceback
            traceback.print_exc()
            
            # In ra th√¥ng tin ƒë·ªÉ debug
            try:
                print(f"\nüìÑ Current URL: {sb.get_current_url()}")
                print(f"üìÑ Page title: {sb.get_title()}")
            except:
                pass


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Crawl magazine from e-pages.pub')
    parser.add_argument('--no-headless', action='store_true', help='Run browser in visible mode (for debugging)')
    parser.add_argument('--max-pages', type=int, default=None, help='Maximum number of pages to crawl per magazine (default: unlimited)')
    parser.add_argument('--magazine-start', type=int, default=7, help='Start magazine ID (default: 7)')
    parser.add_argument('--magazine-end', type=int, default=164, help='End magazine ID (default: 164)')
    parser.add_argument('--magazine-id', type=int, default=None, help='Crawl single magazine ID (overrides start/end)')
    parser.add_argument('--output', type=str, default='magazine_articles.csv', help='Output CSV file (default: magazine_articles.csv)')
    parser.add_argument('--no-resume', action='store_true', help='Disable resume mode (crawl all magazines even if already crawled)')
    parser.add_argument('--force', action='store_true', help='Force re-crawl all magazines (ignore existing data)')
    
    args = parser.parse_args()
    
    headless = not args.no_headless
    output_file = args.output
    
    # Resume mode l√† m·∫∑c ƒë·ªãnh (tr·ª´ khi c√≥ --no-resume ho·∫∑c --force)
    resume_mode = not args.no_resume and not args.force
    
    # Ki·ªÉm tra progress n·∫øu c√≥ resume mode (m·∫∑c ƒë·ªãnh l√† True)
    crawled_magazines = set()
    if resume_mode and os.path.exists(output_file):
        crawled_magazines = get_crawled_magazines(output_file)
        if crawled_magazines:
            print(f"üìä Found {len(crawled_magazines)} already crawled magazine(s): {sorted(crawled_magazines)}")
            print(f"   Will skip these magazines and continue from where we left off")
        else:
            print(f"üìä No previously crawled magazines found in {output_file}")
    elif args.force:
        print(f"üîÑ Force mode: Will re-crawl all magazines")
    elif args.no_resume:
        print(f"üîÑ Resume mode disabled: Will crawl all magazines")
    
    # Remove existing output file n·∫øu force mode
    if args.force and args.magazine_id is None:
        if os.path.exists(output_file):
            os.remove(output_file)
            print(f"üóëÔ∏è  Removed existing {output_file} (force mode)")
            crawled_magazines = set()
    
    all_collected_articles = []
    
    if args.magazine_id:
        # Crawl single magazine
        print(f"üìö Crawling single magazine: {args.magazine_id}")
        articles = crawl_magazine(args.magazine_id, headless=headless, max_pages=args.max_pages)
        if articles:
            all_collected_articles.extend(articles)
            save_articles_to_csv(articles, output_file)
    else:
        # Crawl multiple magazines
        magazine_start = args.magazine_start
        magazine_end = args.magazine_end
        
        print(f"üìö Crawling magazines from {magazine_start} to {magazine_end}")
        
        skipped_count = 0
        for magazine_id in range(magazine_start, magazine_end + 1):
            # Skip n·∫øu ƒë√£ crawl v√† c√≥ resume mode (m·∫∑c ƒë·ªãnh)
            if resume_mode and magazine_id in crawled_magazines:
                print(f"‚è≠Ô∏è  Skipping magazine #{magazine_id} (already crawled)")
                skipped_count += 1
                continue
            
            try:
                articles = crawl_magazine(magazine_id, headless=headless, max_pages=args.max_pages)
                if articles:
                    all_collected_articles.extend(articles)
                    # Save sau m·ªói magazine ƒë·ªÉ kh√¥ng m·∫•t d·ªØ li·ªáu n·∫øu c√≥ l·ªói
                    save_articles_to_csv(articles, output_file)
                else:
                    print(f"‚ö†Ô∏è  No articles found in magazine #{magazine_id}")
                
                # ƒê·ª£i m·ªôt ch√∫t gi·ªØa c√°c magazines
                time.sleep(2)
            except Exception as e:
                print(f"‚ùå Error crawling magazine #{magazine_id}: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        if skipped_count > 0:
            print(f"\n‚è≠Ô∏è  Skipped {skipped_count} already crawled magazine(s)")
    
    # T·ªïng k·∫øt cu·ªëi c√πng
    if all_collected_articles:
        # Drop duplicate m·ªôt l·∫ßn n·ªØa ƒë·ªÉ ƒë·∫£m b·∫£o
        seen = OrderedDict()
        for article in all_collected_articles:
            data_article = article.get('data_article', '')
            if data_article and data_article not in seen:
                seen[data_article] = article
        
        unique_count = len(seen)
        total_count = len(all_collected_articles)
        
        print(f"\n{'='*60}")
        print(f"üìã FINAL SUMMARY")
        print(f"{'='*60}")
        print(f"   Total articles collected: {total_count}")
        print(f"   Unique articles (after deduplication): {unique_count}")
        print(f"   Duplicates removed: {total_count - unique_count}")
        print(f"   Output file: {output_file}")
        print(f"{'='*60}")
    else:
        print("\n‚ö†Ô∏è  No articles collected")

