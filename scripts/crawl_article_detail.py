"""
Script ƒë·ªÉ crawl article detail page v√† l∆∞u v√†o article_details table
Usage: python scripts/crawl_article_detail.py <url>
"""
import sys
import os
import argparse
from datetime import datetime
from pathlib import Path

# Add parent directory to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app import app
from database import db, Article, ArticleDetail
from services.article_detail_parser import ArticleDetailParser
from seleniumbase import SB

# User data directory ƒë·ªÉ l∆∞u session login
USER_DATA_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'user_data')
LOGIN_URL = "https://www.sermitsiaq.ag/login"
LOGIN_EMAIL = "aluu@greenland.org"
LOGIN_PASSWORD = "LEn924924jfkjfk"


def handle_cookie_popup(sb):
    """
    X·ª≠ l√Ω cookie consent popup n·∫øu c√≥ xu·∫•t hi·ªán
    
    Args:
        sb: SeleniumBase instance
    """
    try:
        # Ki·ªÉm tra xem c√≥ popup kh√¥ng
        sb.sleep(1)  # ƒê·ª£i popup xu·∫•t hi·ªán
        
        # T√¨m v√† click button "ACCEPTER ALLE" (Accept All)
        # S·ª≠ d·ª•ng JavaScript ƒë·ªÉ t√¨m button c√≥ text ch·ª©a "ACCEPTER" ho·∫∑c "Accept"
        buttons = sb.execute_script("""
            var buttons = document.querySelectorAll('button');
            for (var i = 0; i < buttons.length; i++) {
                var text = buttons[i].textContent || buttons[i].innerText;
                if (text.includes('ACCEPTER') || text.includes('Accepter') || 
                    text.includes('ACCEPT') || text.includes('Accept')) {
                    return buttons[i];
                }
            }
            return null;
        """)
        
        if buttons:
            sb.execute_script("arguments[0].click();", buttons)
            print("   ‚úÖ Accepted cookie consent popup")
            sb.sleep(3)  # Wait for popup to close and page to stabilize
            
            # Ki·ªÉm tra xem popup ƒë√£ ƒë√≥ng ch∆∞a
            try:
                # ƒê·ª£i popup bi·∫øn m·∫•t
                sb.wait_for_element_not_visible('button:contains("ACCEPTER")', timeout=5)
            except:
                pass
            
            # ƒê·∫£m b·∫£o ƒëang ·ªü ƒë√∫ng trang login sau khi accept cookie
            current_url = sb.get_current_url()
            if 'login' not in current_url.lower():
                print("   üîÑ Reloading login page after cookie acceptance...")
                sb.open(LOGIN_URL)
                sb.sleep(3)  # Wait for page to load
            
            return True
        
        # Fallback: T√¨m b·∫±ng text content
        try:
            page_source = sb.get_page_source()
            if 'Du bestemmer over dine data' in page_source or 'cookie' in page_source.lower():
                # T√¨m button b·∫±ng xpath
                try:
                    accept_btn = sb.find_element('//button[contains(text(), "ACCEPTER") or contains(text(), "Accepter")]', timeout=3)
                    if accept_btn:
                        sb.click(accept_btn)
                        print("   ‚úÖ Accepted cookie consent popup (via XPath)")
                        sb.sleep(3)  # Wait for popup to close
                        
                        # ƒê·∫£m b·∫£o ƒëang ·ªü ƒë√∫ng trang login
                        current_url = sb.get_current_url()
                        if 'login' not in current_url.lower():
                            print("   üîÑ Reloading login page after cookie acceptance...")
                            sb.open(LOGIN_URL)
                            sb.sleep(3)
                        
                        return True
                except:
                    pass
        except:
            pass
        
        # N·∫øu kh√¥ng t√¨m th·∫•y popup, c√≥ th·ªÉ ƒë√£ ƒë∆∞·ª£c accept ho·∫∑c kh√¥ng c√≥
        return False
    except Exception as e:
        # Kh√¥ng c√≥ popup ho·∫∑c ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω
        return False


def ensure_login(sb):
    """
    ƒê·∫£m b·∫£o ƒë√£ login v√†o sermitsiaq.ag
    Ki·ªÉm tra b·∫±ng c√°ch m·ªü trang login v√† xem c√≥ n√∫t "Log ud" kh√¥ng
    N·∫øu ƒë√£ login, trang login s·∫Ω hi·ªÉn th·ªã "Du er allerede logget ind: Log ud"
    
    Args:
        sb: SeleniumBase instance
    """
    print("üîê Checking login status...")
    
    # M·ªü trang login ƒë·ªÉ ki·ªÉm tra
    try:
        sb.open(LOGIN_URL)
        sb.sleep(2)  # Wait for page to load
        
        # X·ª≠ l√Ω cookie popup tr∆∞·ªõc khi ki·ªÉm tra login
        handle_cookie_popup(sb)
        sb.sleep(1)  # Wait a bit after handling popup
        
        # Ki·ªÉm tra xem c√≥ n√∫t "Log ud" (Logout) ho·∫∑c text "Du er allerede logget ind" kh√¥ng
        # CH·ªà ki·ªÉm tra n·∫øu th·ª±c s·ª± ƒë√£ login (n√∫t logout ch·ªâ xu·∫•t hi·ªán khi ƒë√£ login)
        try:
            # T√¨m n√∫t logout
            logout_button = sb.find_element('button.logout', timeout=3)
            if logout_button:
                print("   ‚úÖ Already logged in (found Log ud button)")
                # Ch·ª•p m√†n h√¨nh ƒë·ªÉ x√°c nh·∫≠n
                screenshot_path = os.path.join(USER_DATA_DIR, 'login_check_logged_in.png')
                sb.save_screenshot(screenshot_path)
                print(f"   üì∏ Screenshot saved: {screenshot_path}")
                return True
        except:
            pass
        
        # Ki·ªÉm tra text "Du er allerede logget ind" trong page source
        try:
            page_source = sb.get_page_source()
            if 'Du er allerede logget ind' in page_source:
                # T√¨m n√∫t logout trong page source
                if 'button.logout' in page_source or 'class="logout"' in page_source:
                    print("   ‚úÖ Already logged in (found login status message with logout button)")
                    # Ch·ª•p m√†n h√¨nh ƒë·ªÉ x√°c nh·∫≠n
                    screenshot_path = os.path.join(USER_DATA_DIR, 'login_check_logged_in.png')
                    sb.save_screenshot(screenshot_path)
                    print(f"   üì∏ Screenshot saved: {screenshot_path}")
                    return True
        except:
            pass
        
        # N·∫øu kh√¥ng t√¨m th·∫•y logout button ho·∫∑c message, ch∆∞a login
        print("   ‚ö†Ô∏è  Not logged in (no Log ud button found), attempting login...")
        needs_login = True
        
    except Exception as e:
        print(f"   ‚ö†Ô∏è  Error checking login status: {e}, attempting login...")
        needs_login = True
    
    if needs_login:
        try:
            # ƒê·∫£m b·∫£o ƒëang ·ªü trang login (c√≥ th·ªÉ ƒë√£ m·ªü ·ªü tr√™n)
            current_url = sb.get_current_url()
            if 'login' not in current_url.lower():
                print(f"   üîë Navigating to {LOGIN_URL}...")
                sb.open(LOGIN_URL)
                sb.sleep(3)  # Wait for page to load
            
            # X·ª≠ l√Ω cookie popup tr∆∞·ªõc khi login
            handle_cookie_popup(sb)
            
            # ƒê·∫£m b·∫£o ƒëang ·ªü trang login v√† ƒë·ª£i form load
            current_url = sb.get_current_url()
            if 'login' not in current_url.lower():
                print(f"   üîÑ Navigating to {LOGIN_URL}...")
                sb.open(LOGIN_URL)
                sb.sleep(3)
            
            # Ki·ªÉm tra l·∫°i xem c√≥ ƒë√£ login ch∆∞a (c√≥ th·ªÉ ƒë√£ login trong l√∫c ch·ªù)
            try:
                logout_button = sb.find_element('button.logout', timeout=2)
                if logout_button:
                    print("   ‚úÖ Already logged in (found Log ud button after navigation)")
                    # Ch·ª•p m√†n h√¨nh ƒë·ªÉ x√°c nh·∫≠n
                    screenshot_path = os.path.join(USER_DATA_DIR, 'login_check_after_nav.png')
                    sb.save_screenshot(screenshot_path)
                    print(f"   üì∏ Screenshot saved: {screenshot_path}")
                    return True
            except:
                pass
            
            print(f"   üîë Logging in...")
            
            # Ch·ª•p m√†n h√¨nh tr∆∞·ªõc khi login ƒë·ªÉ debug
            screenshot_path = os.path.join(USER_DATA_DIR, 'before_login.png')
            sb.save_screenshot(screenshot_path)
            print(f"   üì∏ Screenshot before login: {screenshot_path}")
            
            # Form login n·∫±m trong iframe 0, c·∫ßn switch v√†o iframe tr∆∞·ªõc
            print("   üîÑ Switching to iframe containing login form...")
            try:
                # ƒê·ª£i iframe load
                sb.sleep(2)
                # Switch v√†o iframe 0 (iframe ƒë·∫ßu ti√™n ch·ª©a form login)
                sb.switch_to_frame(0)
                sb.sleep(2)  # ƒê·ª£i iframe content load
                print("   ‚úÖ Switched to iframe")
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Could not switch to iframe: {e}")
                # Th·ª≠ reload v√† switch l·∫°i
                sb.switch_to_default_content()
                sb.open(LOGIN_URL)
                sb.sleep(3)
                handle_cookie_popup(sb)
                sb.sleep(2)
                sb.switch_to_frame(0)
                sb.sleep(2)
            
            # ƒê·ª£i form login xu·∫•t hi·ªán trong iframe
            try:
                sb.wait_for_element('#id_subscriber', timeout=10)
            except:
                print("   ‚ö†Ô∏è  Login form not found in iframe, trying to reload...")
                sb.switch_to_default_content()
                sb.open(LOGIN_URL)
                sb.sleep(3)
                handle_cookie_popup(sb)
                sb.sleep(2)
                sb.switch_to_frame(0)
                sb.sleep(2)
                sb.wait_for_element('#id_subscriber', timeout=10)
            
            # Fill in email/subscriber field
            subscriber_input = sb.find_element('#id_subscriber', timeout=10)
            if subscriber_input:
                sb.type('#id_subscriber', LOGIN_EMAIL)
                print(f"   ‚úÖ Filled subscriber field")
            else:
                print("   ‚ùå Could not find subscriber input field")
                return False
            
            # Fill in password field
            password_input = sb.find_element('#id_password', timeout=10)
            if password_input:
                sb.type('#id_password', LOGIN_PASSWORD)
                print(f"   ‚úÖ Filled password field")
            else:
                print("   ‚ùå Could not find password input field")
                return False
            
            # Click login button
            login_button = sb.find_element('button[type="submit"]', timeout=10)
            if login_button:
                sb.click('button[type="submit"]')
                print(f"   ‚úÖ Clicked login button")
                sb.sleep(5)  # Wait for login to complete
                
                # Switch v·ªÅ default content ƒë·ªÉ ki·ªÉm tra login status
                sb.switch_to_default_content()
                sb.sleep(2)
                
                # Ki·ªÉm tra xem ƒë√£ login th√†nh c√¥ng ch∆∞a b·∫±ng c√°ch t√¨m n√∫t "Log ud"
                try:
                    logout_button = sb.find_element('button.logout', timeout=3)
                    if logout_button:
                        print("   ‚úÖ Login successful! (found Log ud button)")
                        # Ch·ª•p m√†n h√¨nh ƒë·ªÉ x√°c nh·∫≠n
                        screenshot_path = os.path.join(USER_DATA_DIR, 'login_success.png')
                        sb.save_screenshot(screenshot_path)
                        print(f"   üì∏ Screenshot saved: {screenshot_path}")
                        return True
                except:
                    pass
                
                # Fallback: ki·ªÉm tra page source
                try:
                    page_source = sb.get_page_source()
                    if 'Du er allerede logget ind' in page_source or 'Log ud' in page_source:
                        print("   ‚úÖ Login successful! (found login status message)")
                        # Ch·ª•p m√†n h√¨nh ƒë·ªÉ x√°c nh·∫≠n
                        screenshot_path = os.path.join(USER_DATA_DIR, 'login_success.png')
                        sb.save_screenshot(screenshot_path)
                        print(f"   üì∏ Screenshot saved: {screenshot_path}")
                        return True
                except:
                    pass
                
                # Fallback: ki·ªÉm tra URL
                current_url = sb.get_current_url()
                if 'login' not in current_url.lower():
                    print("   ‚úÖ Login successful! (redirected away from login page)")
                    # Ch·ª•p m√†n h√¨nh ƒë·ªÉ x√°c nh·∫≠n
                    screenshot_path = os.path.join(USER_DATA_DIR, 'login_success_redirect.png')
                    sb.save_screenshot(screenshot_path)
                    print(f"   üì∏ Screenshot saved: {screenshot_path}")
                    return True
                else:
                    print("   ‚ùå Login failed - still on login page")
                    # Ch·ª•p m√†n h√¨nh ƒë·ªÉ debug
                    screenshot_path = os.path.join(USER_DATA_DIR, 'login_failed.png')
                    sb.save_screenshot(screenshot_path)
                    print(f"   üì∏ Screenshot saved: {screenshot_path}")
                    return False
            else:
                print("   ‚ùå Could not find login button")
                return False
                
        except Exception as e:
            print(f"   ‚ùå Error during login: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    return True


def crawl_article_detail(url: str, language: str = 'da'):
    """
    Crawl article detail page v√† l∆∞u v√†o database
    
    Args:
        url: URL c·ªßa article detail page
        language: Language code ('da', 'kl', 'en')
    """
    print(f"üîç Crawling article detail: {url}")
    print(f"   Language: {language}")
    
    # T·∫°o user_data_dir n·∫øu ch∆∞a t·ªìn t·∫°i
    os.makedirs(USER_DATA_DIR, exist_ok=True)
    
    with SB(uc=True, headless=True, user_data_dir=USER_DATA_DIR) as sb:
        # ƒê·∫£m b·∫£o ƒë√£ login tr∆∞·ªõc khi crawl
        if not ensure_login(sb):
            print("‚ùå Failed to login, cannot proceed with crawling")
            return None
        try:
            # Navigate to URL
            sb.open(url)
            sb.sleep(2)  # Wait for page to load
            
            # Wait for bodytext content
            sb.wait_for_element('.bodytext', timeout=10)
            
            # Get article title
            title = None
            try:
                title_elem = sb.find_element('h1.headline.mainTitle', timeout=5)
                if title_elem:
                    title = title_elem.text
            except:
                pass
            
            # Get excerpt
            excerpt = None
            try:
                excerpt_elem = sb.find_element('h2.subtitle', timeout=5)
                if excerpt_elem:
                    excerpt = excerpt_elem.text
            except:
                pass
            
            # Get articleHeader HTML (ch·ª©a subtitle v√† meta) - ƒë·ªÉ parser c√≥ th·ªÉ parse subtitle
            article_header_html = None
            meta_html = None
            try:
                # Try to find articleHeader
                article_header = sb.find_element('.articleHeader', timeout=5)
                if article_header:
                    article_header_html = article_header.get_attribute('outerHTML')
                    print(f"   ‚úÖ Found articleHeader via Selenium ({len(article_header_html)} chars)")
                    
                    # Extract meta t·ª´ articleHeader
                    try:
                        meta_elem = article_header.find_element('.meta', timeout=3)
                        if meta_elem:
                            meta_html = meta_elem.get_attribute('outerHTML')
                            print(f"   ‚úÖ Found article meta via Selenium ({len(meta_html)} chars)")
                    except:
                        pass
            except:
                # Fallback: parse from page source
                try:
                    from bs4 import BeautifulSoup
                    page_source = sb.get_page_source()
                    soup = BeautifulSoup(page_source, 'html.parser')
                    article_header = soup.find('div', class_='articleHeader')
                    if article_header:
                        article_header_html = str(article_header)
                        print(f"   ‚úÖ Found articleHeader from page source ({len(article_header_html)} chars)")
                        
                        meta_div = article_header.find('div', class_='meta')
                        if meta_div:
                            meta_html = str(meta_div)
                            print(f"   ‚úÖ Found article meta from page source ({len(meta_html)} chars)")
                except Exception as e:
                    print(f"   ‚ö†Ô∏è  Could not find articleHeader: {e}")
            
            # Get bodytext HTML - l·∫•y to√†n b·ªô bodytext container bao g·ªìm c·∫£ intro v√† content-text
            # S·ª≠ d·ª•ng page source ƒë·ªÉ ƒë·∫£m b·∫£o l·∫•y ƒë·∫ßy ƒë·ªß HTML
            bodytext_html = None
            try:
                # Get full page source v√† parse b·∫±ng BeautifulSoup
                page_source = sb.get_page_source()
                from bs4 import BeautifulSoup
                soup = BeautifulSoup(page_source, 'html.parser')
                
                # T√¨m bodytext div ch√≠nh (c√≥ data-element-guid)
                bodytext_div = soup.find('div', class_='bodytext', attrs={'data-element-guid': True})
                if bodytext_div:
                    bodytext_html = str(bodytext_div)
                    print(f"   Found bodytext from page source ({len(bodytext_html)} chars)")
                    
                    # Ki·ªÉm tra xem c√≥ content-text kh√¥ng
                    if 'content-text' in bodytext_html:
                        print(f"   ‚úÖ Contains content-text section")
                    else:
                        print(f"   ‚ö†Ô∏è  No content-text section found")
                else:
                    print(f"   ‚ö†Ô∏è  Could not find bodytext div")
            except Exception as e:
                print(f"   ‚ùå Error parsing page source: {e}")
                # Fallback: try Selenium element
                try:
                    bodytext_elem = sb.find_element('div.bodytext.large-12', timeout=5)
                    if bodytext_elem:
                        bodytext_html = bodytext_elem.get_attribute('outerHTML')
                        print(f"   Found bodytext via Selenium ({len(bodytext_html)} chars)")
                except:
                    pass
            
            # Get paywall offers section
            offers_html = None
            try:
                offers_elem = sb.find_element('.iteras-offers', timeout=5)
                if offers_elem:
                    offers_html = offers_elem.get_attribute('outerHTML')
            except:
                pass
            
            # Get article footer tags section - t√¨m t·ª´ page source ƒë·ªÉ ƒë·∫£m b·∫£o
            footer_html = None
            try:
                # Try Selenium first
                footer_elem = sb.find_element('.articleFooter', timeout=5)
                if footer_elem:
                    footer_html = footer_elem.get_attribute('outerHTML')
                    print(f"   ‚úÖ Found articleFooter via Selenium ({len(footer_html)} chars)")
            except:
                # Fallback: parse from page source
                try:
                    from bs4 import BeautifulSoup
                    page_source = sb.get_page_source()
                    soup = BeautifulSoup(page_source, 'html.parser')
                    footer_div = soup.find('div', class_='articleFooter')
                    if footer_div:
                        footer_html = str(footer_div)
                        print(f"   ‚úÖ Found articleFooter from page source ({len(footer_html)} chars)")
                except Exception as e:
                    print(f"   ‚ö†Ô∏è  Could not find articleFooter: {e}")
            
            # Combine HTML - articleHeader (ch·ª©a subtitle) n√™n ƒë·∫∑t ƒë·∫ßu ti√™n, sau ƒë√≥ meta, r·ªìi bodytext
            full_html = ''
            if article_header_html:
                # S·ª≠ d·ª•ng articleHeader HTML ƒë·ªÉ parser c√≥ th·ªÉ parse subtitle
                full_html = article_header_html
                print(f"   ‚úÖ Added articleHeader to HTML ({len(article_header_html)} chars)")
            elif meta_html:
                # Fallback: ch·ªâ c√≥ meta n·∫øu kh√¥ng c√≥ articleHeader
                full_html = meta_html
                print(f"   ‚úÖ Added article meta to HTML ({len(meta_html)} chars)")
            if bodytext_html:
                full_html += bodytext_html
            if offers_html:
                full_html += offers_html
            if footer_html:
                full_html += footer_html
                print(f"   ‚úÖ Found articleFooter ({len(footer_html)} chars)")
            
            if not full_html:
                print("‚ùå Could not find bodytext content")
                return None
            
            print(f"‚úÖ Found content ({len(full_html)} chars)")
            
            # Get element_guid from bodytext
            element_guid = None
            if bodytext_html:
                from bs4 import BeautifulSoup
                soup = BeautifulSoup(bodytext_html, 'html.parser')
                bodytext_div = soup.find('div', class_='bodytext')
                if bodytext_div:
                    element_guid = bodytext_div.get('data-element-guid', '')
            
            # Parse and save to database
            with app.app_context():
                article_detail = ArticleDetailParser.save_article_detail(
                    published_url=url,
                    html_content=full_html,
                    language=language,
                    element_guid=element_guid
                )
                
                print(f"‚úÖ Saved article detail (ID: {article_detail.id})")
                print(f"   Blocks: {len(article_detail.content_blocks)}")
                
                # Also update article if exists
                article = Article.query.filter_by(published_url=url).first()
                if article:
                    print(f"‚úÖ Found matching article (ID: {article.id})")
                    if title and not article.title:
                        article.title = title
                    if excerpt and not article.excerpt:
                        article.excerpt = excerpt
                    db.session.commit()
                else:
                    print(f"‚ö†Ô∏è  No matching article found with published_url={url}")
                
                return article_detail
                
        except Exception as e:
            print(f"‚ùå Error crawling: {e}")
            import traceback
            traceback.print_exc()
            return None


def main():
    parser = argparse.ArgumentParser(description='Crawl article detail page')
    parser.add_argument('url', help='URL of article detail page')
    parser.add_argument('--language', '-l', default='da', choices=['da', 'kl', 'en'],
                        help='Language code (default: da)')
    
    args = parser.parse_args()
    
    with app.app_context():
        article_detail = crawl_article_detail(args.url, args.language)
        
        if article_detail:
            # Query again from database to avoid DetachedInstanceError
            saved_detail = ArticleDetail.query.filter_by(published_url=args.url).first()
            print(f"\n‚úÖ Success!")
            if saved_detail:
                print(f"   Article Detail ID: {saved_detail.id}")
                print(f"   Published URL: {saved_detail.published_url}")
                print(f"   Blocks: {len(saved_detail.content_blocks) if saved_detail.content_blocks else 0}")
            print(f"\nüìù Test URL: http://localhost:5000/article/detail/test?url={args.url}")
        else:
            print("\n‚ùå Failed to crawl article detail")


if __name__ == '__main__':
    main()

